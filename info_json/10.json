{
  "id": 10,
  "repo": "neon",
  "issue_url": "https://github.com/neondatabase/neon/issues/7984",
  "pr_url": "https://github.com/neondatabase/neon/pull/7989",
  "issue_description": "# Problem\r\n\r\nThe `test_vm_bit_clear_on_heap_lock` test occasionally fails with\r\n\r\n```\r\nassert vm_page_at_pageserver == vm_page_in_cache\r\nAssertionError: assert '550000000000\u2026\u20260000000000000'=='0000000.\u20260000000000000\r\n...\r\ntest_runner/regress/test_vm_bits.py:185: AssertionError\r\n```\r\n\r\n![image](https://github.com/neondatabase/neon/assets/956573/dc7f8d8d-bca4-49b2-81ff-2c2d93b0558c)\r\n\r\n\r\nExample Allure report that encountered the issue: https://neon-github-public-dev.s3.amazonaws.com/reports/pr-7979/9395161864/index.html#testresult/604af091fcc61d51/retries\r\n\r\n# Initial Analysis\r\n\r\n(by @hlinnaka )\r\n\r\nI think this is indeed an orthogonal issue. Hypothesis for how this happens:\r\n\r\n1. When we run the `VACUUM (FREEZE, DISABLE_PAGE_SKIPPING true) vmtest_lock` command, autovacuum/autoanalyze is running in background and has an active snapshot, which prevents VACUUM FREEZE from freezing the rows. That\u2019s how you sometimes get an all-zeros VM page.\r\n\r\n2. Between the calls to get the page from cache, and from the pageserver, autovacuum runs and has vacuumed just a few of the pages.\r\n\r\nIf that hypothesis holds, this is a case of an unstable test, not a bug as such. We could disable autovacuum for this first part of the test (the second part requires it to be on, though) to make it more stable.\r\n\r\nI think you can see this failure more often if you add a sleep between the `get_raw_page` and `get_raw_page_at_lsn` calls.\r\n \r\n\r\nI think we should make the test more robust so that it actually sets all the VM bits in step 1. That would make the hack to ignore the page header unnecessary, too. Maybe check that the VM bits are set and issue the VACUUM again. Or maybe disabling autovacuum is enough to stabilize it.\r\n\r\n\r\n# Distinction From #6967 \r\n\r\nThis flakiness was discovered while investigating #6967 but is believed to be orthogonal.\r\nSee https://www.notion.so/neondatabase/2024-06-05-vm-flakiness-investigation-ee3ed981aa974635bf475a7e5aebe19e?pvs=4#64d17080b88e4c3e8204f79dd337627f \r\n\r\n\r\n",
  "files_changed": [
    {
      "filename": "test_runner/regress/test_vm_bits.py",
      "status": "modified",
      "patch": "@@ -1,7 +1,9 @@\n import time\n+from contextlib import closing\n \n from fixtures.log_helper import log\n from fixtures.neon_fixtures import NeonEnv, NeonEnvBuilder, fork_at_current_lsn\n+from fixtures.utils import query_scalar\n \n \n #\n@@ -113,11 +115,88 @@ def test_vm_bit_clear(neon_simple_env: NeonEnv):\n     assert cur_new.fetchall() == []\n \n \n-#\n-# Test that the ALL_FROZEN VM bit is cleared correctly at a HEAP_LOCK\n-# record.\n-#\n-def test_vm_bit_clear_on_heap_lock(neon_env_builder: NeonEnvBuilder):\n+def test_vm_bit_clear_on_heap_lock_whitebox(neon_env_builder: NeonEnvBuilder):\n+    \"\"\"\n+    Test that the ALL_FROZEN VM bit is cleared correctly at a HEAP_LOCK record.\n+\n+    This is a repro for the bug fixed in commit 66fa176cc8.\n+    \"\"\"\n+    env = neon_env_builder.init_start()\n+    endpoint = env.endpoints.create_start(\n+        \"main\",\n+        config_lines=[\n+            # If auto-analyze runs at the same time that we run VACUUM FREEZE, it\n+            # can hold a snasphot that prevent the tuples from being frozen.\n+            \"autovacuum=off\",\n+            \"log_checkpoints=on\",\n+        ],\n+    )\n+\n+    # Run the tests in a dedicated database, because the activity monitor\n+    # periodically runs some queries on to the 'postgres' database. If that\n+    # happens at the same time that we're trying to freeze, the activity\n+    # monitor's queries can hold back the xmin horizon and prevent freezing.\n+    with closing(endpoint.connect()) as pg_conn:\n+        pg_conn.cursor().execute(\"CREATE DATABASE vmbitsdb\")\n+    pg_conn = endpoint.connect(dbname=\"vmbitsdb\")\n+    cur = pg_conn.cursor()\n+\n+    # Install extension containing function needed for test\n+    cur.execute(\"CREATE EXTENSION neon_test_utils\")\n+    cur.execute(\"CREATE EXTENSION pageinspect\")\n+\n+    # Create a test table and freeze it to set the all-frozen VM bit on all pages.\n+    cur.execute(\"CREATE TABLE vmtest_lock (id integer PRIMARY KEY)\")\n+    cur.execute(\"BEGIN\")\n+    cur.execute(\"INSERT INTO vmtest_lock SELECT g FROM generate_series(1, 50000) g\")\n+    xid = int(query_scalar(cur, \"SELECT txid_current()\"))\n+    cur.execute(\"COMMIT\")\n+    cur.execute(\"VACUUM (FREEZE, DISABLE_PAGE_SKIPPING true, VERBOSE) vmtest_lock\")\n+    for notice in pg_conn.notices:\n+        log.info(f\"{notice}\")\n+\n+    # This test has been flaky in the past, because background activity like\n+    # auto-analyze and compute_ctl's activity monitor queries have prevented the\n+    # tuples from being frozen. Check that they were frozen.\n+    relfrozenxid = int(\n+        query_scalar(cur, \"SELECT relfrozenxid FROM pg_class WHERE relname='vmtest_lock'\")\n+    )\n+    assert (\n+        relfrozenxid > xid\n+    ), f\"Inserted rows were not frozen. This can be caused by concurrent activity in the database. (XID {xid}, relfrozenxid {relfrozenxid}\"\n+\n+    # Lock a row. This clears the all-frozen VM bit for that page.\n+    cur.execute(\"BEGIN\")\n+    cur.execute(\"SELECT * FROM vmtest_lock WHERE id = 40000 FOR UPDATE\")\n+    cur.execute(\"COMMIT\")\n+\n+    # The VM page in shared buffer cache, and the same page as reconstructed by\n+    # the pageserver, should be equal. Except for the LSN: Clearing a bit in the\n+    # VM doesn't bump the LSN in PostgreSQL, but the pageserver updates the LSN\n+    # when it replays the VM-bit clearing record (since commit 387a36874c)\n+    #\n+    # This is a bit fragile, we've had lot of flakiness in this test before. For\n+    # example, because all the VM bits were not set because concurrent\n+    # autoanalyze prevented the VACUUM FREEZE from freezing the tuples. Or\n+    # because autoavacuum kicked in and re-froze the page between the\n+    # get_raw_page() and get_raw_page_at_lsn() calls. We disable autovacuum now,\n+    # which should make this deterministic.\n+    cur.execute(\"select get_raw_page( 'vmtest_lock', 'vm', 0 )\")\n+    vm_page_in_cache = (cur.fetchall()[0][0])[8:100].hex()\n+    cur.execute(\n+        \"select get_raw_page_at_lsn( 'vmtest_lock', 'vm', 0, pg_current_wal_insert_lsn(), NULL )\"\n+    )\n+    vm_page_at_pageserver = (cur.fetchall()[0][0])[8:100].hex()\n+\n+    assert vm_page_at_pageserver == vm_page_in_cache\n+\n+\n+def test_vm_bit_clear_on_heap_lock_blackbox(neon_env_builder: NeonEnvBuilder):\n+    \"\"\"\n+    The previous test is enough to verify the bug that was fixed in\n+    commit 66fa176cc8. But for good measure, we also reproduce the\n+    original problem that the missing VM page update caused.\n+    \"\"\"\n     tenant_conf = {\n         \"checkpoint_distance\": f\"{128 * 1024}\",\n         \"compaction_target_size\": f\"{128 * 1024}\",\n@@ -130,9 +209,9 @@ def test_vm_bit_clear_on_heap_lock(neon_env_builder: NeonEnvBuilder):\n     env = neon_env_builder.init_start(initial_tenant_conf=tenant_conf)\n \n     tenant_id = env.initial_tenant\n-    timeline_id = env.neon_cli.create_branch(\"test_vm_bit_clear_on_heap_lock\")\n+    timeline_id = env.initial_timeline\n     endpoint = env.endpoints.create_start(\n-        \"test_vm_bit_clear_on_heap_lock\",\n+        \"main\",\n         config_lines=[\n             \"log_autovacuum_min_duration = 0\",\n             # Perform anti-wraparound vacuuming aggressively\n@@ -146,12 +225,10 @@ def test_vm_bit_clear_on_heap_lock(neon_env_builder: NeonEnvBuilder):\n \n     # Install extension containing function needed for test\n     cur.execute(\"CREATE EXTENSION neon_test_utils\")\n-    cur.execute(\"CREATE EXTENSION pageinspect\")\n \n     # Create a test table and freeze it to set the all-frozen VM bit on all pages.\n     cur.execute(\"CREATE TABLE vmtest_lock (id integer PRIMARY KEY)\")\n     cur.execute(\"INSERT INTO vmtest_lock SELECT g FROM generate_series(1, 50000) g\")\n-\n     cur.execute(\"VACUUM (FREEZE, DISABLE_PAGE_SKIPPING true) vmtest_lock\")\n \n     # Lock a row. This clears the all-frozen VM bit for that page.\n@@ -165,27 +242,6 @@ def test_vm_bit_clear_on_heap_lock(neon_env_builder: NeonEnvBuilder):\n \n     cur.execute(\"COMMIT\")\n \n-    # The VM page in shared buffer cache, and the same page as reconstructed\n-    # by the pageserver, should be equal.\n-    #\n-    # Ignore page header (24 bytes) of visibility map.\n-    # If the dirty VM page is flushed from the cache for some reason,\n-    # it gets WAL-logged, which changes the LSN on the page.\n-    # Also in neon SMGR we can replace empty heap page with zero (uninitialized) heap page.\n-    cur.execute(\"select get_raw_page( 'vmtest_lock', 'vm', 0 )\")\n-    vm_page_in_cache = (cur.fetchall()[0][0])[24:100].hex()\n-    cur.execute(\n-        \"select get_raw_page_at_lsn( 'vmtest_lock', 'vm', 0, pg_current_wal_insert_lsn(), NULL )\"\n-    )\n-    vm_page_at_pageserver = (cur.fetchall()[0][0])[24:100].hex()\n-\n-    assert vm_page_at_pageserver == vm_page_in_cache\n-\n-    # The above assert is enough to verify the bug that was fixed in\n-    # commit 66fa176cc8. But for good measure, we also reproduce the\n-    # original problem that the missing VM page update caused. The\n-    # rest of the test does that.\n-\n     # Kill and restart postgres, to clear the buffer cache.\n     #\n     # NOTE: clear_buffer_cache() will not do, because it evicts the dirty pages"
    }
  ],
  "fix_category": NaN,
  "root_cause_category": NaN,
  "root_cause_subcategory": "!Rust (Python)"
}