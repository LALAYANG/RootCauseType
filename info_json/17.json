{
  "id": 17,
  "repo": "parsec-cloud",
  "issue_url": "https://github.com/Scille/parsec-cloud/issues/2864",
  "pr_url": "https://github.com/Scille/parsec-cloud/pull/2903",
  "issue_description": "The hypothesis test during the action [2887918745](https://github.com/Scille/parsec-cloud/actions/runs/2887918745) failed because of that.\r\n\r\nThe log trace is contain in [macos-hypothesis-test.zip](https://github.com/Scille/parsec-cloud/files/9380445/macos-hypothesis-test.zip)\r\n\r\n",
  "files_changed": [
    {
      "filename": ".cspell/custom-words.txt",
      "status": "modified",
      "patch": "@@ -61,6 +61,7 @@ Dbus\n DDTHH\n dearmor\n debouncer\n+deconnection\n Deeplink\n dehaze\n derandomize\n@@ -120,6 +121,7 @@ ifempty\n ifeq\n ifnewer\n ijkl\n+inversed\n ionicons\n iqmp\n itemsoverlay"
    },
    {
      "filename": ".cspell/names.txt",
      "status": "modified",
      "patch": "@@ -11,6 +11,7 @@ Dalvik\n dorny\n dtolnay\n expiredorg\n+farn\n fifi\n Fonzie\n foobarrrrr\n@@ -26,6 +27,7 @@ Reynholm\n riri\n Roboto\n samuelmeuli\n+Sanctorum\n Scille\n titeuf\n werkzeug"
    },
    {
      "filename": ".github/workflows/ci.yml",
      "status": "modified",
      "patch": "@@ -210,15 +210,23 @@ jobs:\n         if: >-\n           steps.python-changes.outputs.run == 'true'\n           && startsWith(matrix.os, 'ubuntu-')\n-        run: >\n-          sudo apt-get -y install\n-          libfuse2\n-          fuse\n-          desktop-file-utils\n-          postgresql-${{ env.postgresql-version }}\n-          libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0\n-          libxcb-keysyms1 libxcb-randr0 libxcb-render-util0\n-          libxcb-xinerama0 libxcb-xfixes0 x11-utils\n+        run: |\n+          # Retry the command until it succeed.\n+          # We retry because sometime the APT repo configured\n+          # by the runner seems drop the connection causing the command to fail.\n+          until sudo apt-get -y install ${{ env.PACKAGE_TO_INSTALL }}; do\n+            echo \"Fail to install APT package retrying ...\";\n+          done\n+        env:\n+          PACKAGE_TO_INSTALL: >-\n+            libfuse2\n+            fuse\n+            desktop-file-utils\n+            postgresql-${{ env.postgresql-version }}\n+            libxkbcommon-x11-0 libxcb-icccm4 libxcb-image0\n+            libxcb-keysyms1 libxcb-randr0 libxcb-render-util0\n+            libxcb-xinerama0 libxcb-xfixes0 x11-utils\n+        timeout-minutes: 5\n \n       - name: (\ud83c\udf4e macOS) Install macfuse\n         if: >-\n@@ -310,11 +318,13 @@ jobs:\n           && startsWith(matrix.os, 'ubuntu-')\n         run: poetry run pip install pytest-xvfb\n \n-      - name: (\ud83d\udc27\ud83c\udfc1 Not macOS) GUI tests\n+      - name: (\ud83d\udc27\ud83c\udfc1 Not MacOS) GUI tests\n         if: >-\n           steps.python-changes.outputs.run == 'true'\n           && startsWith(matrix.os, 'macos-') != true\n-        run: poetry run pytest ${{ env.pytest-base-args }} tests --runmountpoint --runslow --rungui -m gui\n+        # Note: using `--numprocesses 1` force pytest to print the test that it will be executing\n+        # This is usefull to pinpoint exactly which test in blocking.\n+        run: poetry run pytest ${{ env.pytest-base-args }} tests --runmountpoint --runslow --rungui -m gui --numprocesses 1\n         timeout-minutes: 10\n \n       - name: (\ud83d\udc27 Linux) PostgreSQL tests"
    },
    {
      "filename": ".pre-commit-config.yaml",
      "status": "modified",
      "patch": "@@ -28,11 +28,12 @@ repos:\n       #   require_serial: true\n       - id: license_headers\n         name: license_headers\n+        alias: headers\n         entry: python ./misc/license_headers.py add\n         language: python\n         language_version: python3\n         # see: https://github.com/pre-commit/identify/blob/master/identify/extensions.py\n-        types_or: [ python, sql, rust, vue, ts, tsx, javascript, jsx ]\n+        types_or: [ python, pyi, sql, rust, vue, ts, tsx, javascript, jsx ]\n \n   - repo: https://github.com/adrienverge/yamllint\n     rev: v1.28.0\n@@ -51,6 +52,7 @@ repos:\n           - --no-summary\n           - --gitignore\n         additional_dependencies:\n+          - \"@cspell/dict-en_us\"\n           - \"@cspell/dict-fr-fr\"\n           - \"@cspell/dict-fr-reforme\"\n           - \"@cspell/dict-bash\""
    },
    {
      "filename": "Cargo.lock",
      "status": "modified",
      "patch": "@@ -71,6 +71,17 @@ dependencies = [\n  \"password-hash\",\n ]\n \n+[[package]]\n+name = \"async-trait\"\n+version = \"0.1.64\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"1cd7fce9ba8c3c042128ce72d8b2ddbf3a05747efb67ea0313c635e10bda47a2\"\n+dependencies = [\n+ \"proc-macro2\",\n+ \"quote\",\n+ \"syn\",\n+]\n+\n [[package]]\n name = \"atty\"\n version = \"0.2.14\"\n@@ -449,7 +460,6 @@ checksum = \"68c186a7418a2aac330bb76cde82f16c36b03a66fb91db32d20214311f9f6545\"\n dependencies = [\n  \"diesel_derives\",\n  \"libsqlite3-sys\",\n- \"r2d2\",\n ]\n \n [[package]]\n@@ -1162,6 +1172,7 @@ dependencies = [\n  \"libparsec_crypto\",\n  \"libparsec_platform_async\",\n  \"libparsec_platform_device_loader\",\n+ \"libparsec_platform_local_db\",\n  \"libparsec_protocol\",\n  \"libparsec_testbed\",\n  \"libparsec_types\",\n@@ -1287,15 +1298,19 @@ dependencies = [\n name = \"libparsec_core_fs\"\n version = \"0.0.0\"\n dependencies = [\n+ \"async-trait\",\n  \"diesel\",\n+ \"lazy_static\",\n  \"libparsec_client_types\",\n  \"libparsec_crypto\",\n+ \"libparsec_platform_async\",\n+ \"libparsec_platform_local_db\",\n  \"libparsec_tests_fixtures\",\n  \"libparsec_types\",\n- \"libsqlite3-sys\",\n  \"regex\",\n  \"rstest\",\n  \"thiserror\",\n+ \"tokio\",\n  \"uuid\",\n ]\n \n@@ -1370,6 +1385,22 @@ dependencies = [\n  \"web-sys\",\n ]\n \n+[[package]]\n+name = \"libparsec_platform_local_db\"\n+version = \"0.0.0\"\n+dependencies = [\n+ \"async-trait\",\n+ \"diesel\",\n+ \"lazy_static\",\n+ \"libparsec_platform_async\",\n+ \"libparsec_tests_fixtures\",\n+ \"libsqlite3-sys\",\n+ \"pretty_assertions\",\n+ \"rstest\",\n+ \"thiserror\",\n+ \"tokio\",\n+]\n+\n [[package]]\n name = \"libparsec_protocol\"\n version = \"0.0.0\"\n@@ -1816,39 +1847,14 @@ version = \"2.0.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"427c3892f9e783d91cc128285287e70a59e206ca452770ece88a76f7a3eddd72\"\n \n-[[package]]\n-name = \"parking_lot\"\n-version = \"0.11.2\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"7d17b78036a60663b797adeaee46f5c9dfebb86948d1255007a1d6be0271ff99\"\n-dependencies = [\n- \"instant\",\n- \"lock_api\",\n- \"parking_lot_core 0.8.5\",\n-]\n-\n [[package]]\n name = \"parking_lot\"\n version = \"0.12.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"87f5ec2493a61ac0506c0f4199f99070cbe83857b0337006a30f3e6719b8ef58\"\n dependencies = [\n  \"lock_api\",\n- \"parking_lot_core 0.9.3\",\n-]\n-\n-[[package]]\n-name = \"parking_lot_core\"\n-version = \"0.8.5\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"d76e8e1493bcac0d2766c42737f34458f1c8c50c0d23bcb24ea953affb273216\"\n-dependencies = [\n- \"cfg-if\",\n- \"instant\",\n- \"libc\",\n- \"redox_syscall\",\n- \"smallvec\",\n- \"winapi\",\n+ \"parking_lot_core\",\n ]\n \n [[package]]\n@@ -2086,7 +2092,7 @@ dependencies = [\n  \"inventory\",\n  \"libc\",\n  \"memoffset\",\n- \"parking_lot 0.12.0\",\n+ \"parking_lot\",\n  \"pyo3-build-config\",\n  \"pyo3-ffi\",\n  \"pyo3-macros\",\n@@ -2160,17 +2166,6 @@ dependencies = [\n  \"proc-macro2\",\n ]\n \n-[[package]]\n-name = \"r2d2\"\n-version = \"0.8.9\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"545c5bc2b880973c9c10e4067418407a0ccaa3091781d1671d46eb35107cb26f\"\n-dependencies = [\n- \"log\",\n- \"parking_lot 0.11.2\",\n- \"scheduled-thread-pool\",\n-]\n-\n [[package]]\n name = \"rand\"\n version = \"0.7.3\"\n@@ -2451,15 +2446,6 @@ dependencies = [\n  \"windows-sys 0.36.1\",\n ]\n \n-[[package]]\n-name = \"scheduled-thread-pool\"\n-version = \"0.2.6\"\n-source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"977a7519bff143a44f842fd07e80ad1329295bd71686457f18e496736f4bf9bf\"\n-dependencies = [\n- \"parking_lot 0.12.0\",\n-]\n-\n [[package]]\n name = \"scoped-tls\"\n version = \"1.0.0\"\n@@ -2895,7 +2881,7 @@ dependencies = [\n  \"memchr\",\n  \"mio\",\n  \"num_cpus\",\n- \"parking_lot 0.12.0\",\n+ \"parking_lot\",\n  \"pin-project-lite\",\n  \"signal-hook-registry\",\n  \"socket2\","
    },
    {
      "filename": "oxidation/libparsec/Cargo.toml",
      "status": "modified",
      "patch": "@@ -8,13 +8,8 @@ rust-version = \"1.62.0\"\n \n [features]\n # Remember kid: RustCrypto is used if `use-sodiumoxide` is not set !\n-use-sodiumoxide = [ \"libparsec_crypto/use-sodiumoxide\" ]\n-test-utils = [\n-    \"libparsec_testbed\",\n-    \"libparsec_client_high_level_api/test-utils\",\n-    \"libparsec_platform_device_loader/test-with-testbed\",\n-    \"libparsec_types/test-mock-time\",\n-]\n+use-sodiumoxide = [\"libparsec_crypto/use-sodiumoxide\"]\n+test-utils = [\"dep:libparsec_testbed\", \"libparsec_client_high_level_api/test-utils\", \"libparsec_platform_device_loader/test-with-testbed\", \"libparsec_types/test-mock-time\", \"libparsec_core_fs/test-utils\", \"dep:libparsec_platform_local_db\"]\n \n [dependencies]\n libparsec_client_high_level_api = { path = \"crates/client_high_level_api\" }\n@@ -30,3 +25,4 @@ libparsec_testbed = { path = \"crates/testbed\", optional = true }\n libparsec_protocol = { path = \"crates/protocol\" }\n libparsec_core_fs = { path = \"crates/core_fs\" }\n libparsec_types = { path = \"crates/types\" }\n+libparsec_platform_local_db = { path = \"crates/platform_local_db\", features = [\"test-utils\"], optional = true }"
    },
    {
      "filename": "oxidation/libparsec/crates/client_types/src/local_manifest.rs",
      "status": "modified",
      "patch": "@@ -1062,3 +1062,27 @@ impl LocalManifest {\n         rmp_serde::from_slice(&serialized).map_err(|_| Box::new(DataError::Serialization))\n     }\n }\n+\n+impl From<LocalFileManifest> for LocalManifest {\n+    fn from(value: LocalFileManifest) -> Self {\n+        Self::File(value)\n+    }\n+}\n+\n+impl From<LocalUserManifest> for LocalManifest {\n+    fn from(value: LocalUserManifest) -> Self {\n+        Self::User(value)\n+    }\n+}\n+\n+impl From<LocalFolderManifest> for LocalManifest {\n+    fn from(value: LocalFolderManifest) -> Self {\n+        Self::Folder(value)\n+    }\n+}\n+\n+impl From<LocalWorkspaceManifest> for LocalManifest {\n+    fn from(value: LocalWorkspaceManifest) -> Self {\n+        Self::Workspace(value)\n+    }\n+}"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/Cargo.toml",
      "status": "modified",
      "patch": "@@ -7,6 +7,9 @@ autotests = false\n \n # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n \n+[features]\n+test-utils = []\n+\n [dependencies]\n libparsec_crypto = { path = \"../crypto\" }\n libparsec_types = { path = \"../types\" }\n@@ -15,13 +18,17 @@ libparsec_client_types = { path = \"../client_types\" }\n # Disable Diesel's default feature to remove `32-column-tables` which slowdown compilation\n # quiet a lot (Diesel crate compilation on my machine: 19s with vs 8s without).\n # Note this is fine as long as we use tables with at most 16 columns.\n-diesel = { version = \"2.0.2\", features = [\"sqlite\", \"r2d2\", \"with-deprecated\"], default-features = false }\n-libsqlite3-sys = { version = \"0.25.2\", features = [\"bundled\"] }\n+diesel = { version = \"2.0.2\", default-features = false }\n regex = \"1.6.0\"\n thiserror = \"1.0.37\"\n uuid = { version = \"1.2.2\", features = [\"v4\", \"fast-rng\"] }\n+lazy_static = \"1.4.0\"\n+libparsec_platform_local_db = { path = \"../platform_local_db\" }\n+async-trait = \"0.1.61\"\n+libparsec_platform_async = { path = \"../platform_async\" }\n \n [dev-dependencies]\n libparsec_tests_fixtures = { path = \"../tests_fixtures\" }\n \n rstest = \"0.16.0\"\n+tokio = { version = \"1.24.1\", features = [\"macros\"]}"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/conftest.rs",
      "status": "modified",
      "patch": "@@ -2,7 +2,10 @@\n \n use rstest::fixture;\n \n-use crate::{WorkspaceStorage, DEFAULT_WORKSPACE_STORAGE_CACHE_SIZE};\n+use crate::{\n+    WorkspaceStorage, DEFAULT_CHUNK_VACUUM_THRESHOLD, DEFAULT_WORKSPACE_STORAGE_CACHE_SIZE,\n+    FAILSAFE_PATTERN_FILTER,\n+};\n use libparsec_tests_fixtures::{alice, tmp_path, Device, TmpPath};\n use libparsec_types::EntryID;\n \n@@ -26,15 +29,18 @@ impl std::ops::DerefMut for TmpWorkspaceStorage {\n }\n \n #[fixture]\n-pub fn alice_workspace_storage(alice: &Device, tmp_path: TmpPath) -> TmpWorkspaceStorage {\n+pub async fn alice_workspace_storage(alice: &Device, tmp_path: TmpPath) -> TmpWorkspaceStorage {\n     let db_path = tmp_path.join(\"workspace_storage.sqlite\");\n \n     let ws = WorkspaceStorage::new(\n         db_path,\n         alice.local_device(),\n         EntryID::default(),\n+        FAILSAFE_PATTERN_FILTER.clone(),\n+        DEFAULT_CHUNK_VACUUM_THRESHOLD,\n         DEFAULT_WORKSPACE_STORAGE_CACHE_SIZE,\n     )\n+    .await\n     .unwrap();\n \n     TmpWorkspaceStorage {"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/error.rs",
      "status": "modified",
      "patch": "@@ -4,12 +4,13 @@ use thiserror::Error;\n use uuid::Uuid;\n \n use libparsec_crypto::CryptoError;\n+use libparsec_platform_local_db::DatabaseError;\n use libparsec_types::{EntryID, FileDescriptor};\n \n #[derive(Error, Debug, PartialEq, Eq)]\n pub enum FSError {\n-    #[error(\"ConfigurationError\")]\n-    Configuration,\n+    #[error(\"ConfigurationError: {0}\")]\n+    Configuration(String),\n \n     #[error(\"ConnectionError: {0}\")]\n     Connection(String),\n@@ -23,14 +24,14 @@ pub enum FSError {\n     #[error(\"CryptoError: {0}\")]\n     Crypto(CryptoError),\n \n-    #[error(\"DeleteTableError: {0}\")]\n-    DeleteTable(String),\n+    #[error(\"Database query error: {0}\")]\n+    DatabaseQueryError(String),\n \n-    #[error(\"DropTableError: {0}\")]\n-    DropTable(String),\n+    #[error(\"Database is closed: {0}\")]\n+    DatabaseClosed(String),\n \n-    #[error(\"InsertTableError: {0}\")]\n-    InsertTable(String),\n+    #[error(\"Database operational error: {0}\")]\n+    DatabaseOperationalError(String),\n \n     #[error(\"Invalid FileDescriptor {0:?}\")]\n     InvalidFileDescriptor(FileDescriptor),\n@@ -54,7 +55,7 @@ pub enum FSError {\n     #[error(\"PoolError\")]\n     Pool,\n \n-    #[error(\"Entry `{0}` modified without beeing locked\")]\n+    #[error(\"Entry `{0}` modified without being locked\")]\n     Runtime(EntryID),\n \n     #[error(\"UpdateTableError: {0}\")]\n@@ -65,6 +66,11 @@ pub enum FSError {\n \n     #[error(\"VacuumError: {0}\")]\n     Vacuum(String),\n+\n+    /// Error returned by [crate::storage::WorkspaceStorageTimestamped]\n+    /// when requiring more features than it's able to provide.\n+    #[error(\"Not implemented: WorkspaceStorage is timestamped\")]\n+    WorkspaceStorageTimestamped,\n }\n \n pub type FSResult<T> = Result<T, FSError>;\n@@ -74,3 +80,52 @@ impl From<CryptoError> for FSError {\n         Self::Crypto(e)\n     }\n }\n+\n+impl From<diesel::result::Error> for FSError {\n+    fn from(e: diesel::result::Error) -> Self {\n+        use diesel::result::{DatabaseErrorKind, Error};\n+\n+        match e {\n+            Error::DatabaseError(DatabaseErrorKind::ClosedConnection, e) => {\n+                Self::DatabaseClosed(e.message().to_string())\n+            }\n+            Error::DatabaseError(_kind, msg) => {\n+                Self::DatabaseOperationalError(msg.message().to_string())\n+            }\n+            _ => Self::DatabaseQueryError(e.to_string()),\n+        }\n+    }\n+}\n+\n+impl From<diesel::result::ConnectionError> for FSError {\n+    fn from(e: diesel::result::ConnectionError) -> Self {\n+        match e {\n+            diesel::ConnectionError::InvalidCString(e) => {\n+                Self::Configuration(format!(\"Invalid c string: {e}\"))\n+            }\n+            diesel::ConnectionError::BadConnection(e) => {\n+                Self::Configuration(format!(\"Bad connection: {e}\"))\n+            }\n+            diesel::ConnectionError::InvalidConnectionUrl(e) => {\n+                Self::Configuration(format!(\"Invalid connection url: {e}\"))\n+            }\n+            diesel::ConnectionError::CouldntSetupConfiguration(e) => {\n+                Self::Configuration(format!(\"Couldnt setup configuration: {e}\"))\n+            }\n+            _ => Self::Configuration(\"Unknown error\".to_string()),\n+        }\n+    }\n+}\n+\n+impl From<DatabaseError> for FSError {\n+    fn from(e: DatabaseError) -> Self {\n+        match e {\n+            DatabaseError::Closed => Self::DatabaseClosed(\"Database is closed\".to_string()),\n+            DatabaseError::DieselDatabaseError(kind, err) => {\n+                Self::from(diesel::result::Error::DatabaseError(kind, err))\n+            }\n+            DatabaseError::Diesel(e) => Self::from(e),\n+            DatabaseError::DieselConnectionError(e) => Self::from(e),\n+        }\n+    }\n+}"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/extensions.rs",
      "status": "modified",
      "patch": "@@ -2,9 +2,9 @@\n \n use diesel::backend::Backend;\n use diesel::expression::ValidGrouping;\n-use diesel::query_builder::{AstPass, QueryFragment};\n-use diesel::sql_types::BigInt;\n-use diesel::{impl_selectable_expression, DieselNumericOps, Expression, QueryId, QueryResult};\n+use diesel::query_builder::{AstPass, QueryFragment, QueryId};\n+use diesel::sql_types::{BigInt, DieselNumericOps};\n+use diesel::{impl_selectable_expression, Expression, QueryResult};\n \n #[derive(Default, Debug, Clone, Copy, QueryId, DieselNumericOps, ValidGrouping)]\n pub struct CoalesceTotalSize;"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/chunk_storage.rs",
      "status": "modified",
      "patch": "@@ -1,15 +1,24 @@\n // Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n \n+use std::{\n+    collections::HashSet,\n+    sync::{\n+        atomic::{AtomicBool, Ordering},\n+        Arc,\n+    },\n+};\n+\n use diesel::{\n-    dsl::count_star, sql_query, table, AsChangeset, ExpressionMethods, Insertable, QueryDsl,\n-    RunQueryDsl,\n+    dsl::{count_star, not},\n+    sql_query, table, AsChangeset, BoolExpressionMethods, ExpressionMethods, Insertable,\n+    OptionalExtension, QueryDsl, RunQueryDsl,\n };\n-use std::sync::Mutex;\n \n use libparsec_crypto::SecretKey;\n-use libparsec_types::{ChunkID, TimeProvider, DEFAULT_BLOCK_SIZE};\n+use libparsec_platform_async::future;\n+use libparsec_platform_local_db::{LocalDatabase, LOCAL_DATABASE_MAX_VARIABLE_NUMBER};\n+use libparsec_types::{BlockID, ChunkID, TimeProvider, DEFAULT_BLOCK_SIZE};\n \n-use super::local_database::{SqliteConn, SQLITE_MAX_VARIABLE_NUMBER};\n use crate::{\n     error::{FSError, FSResult},\n     extensions::CoalesceTotalSize,\n@@ -25,6 +34,13 @@ table! {\n     }\n }\n \n+table! {\n+    remanence(_id) {\n+        _id -> BigInt,\n+        block_remanent -> Bool,\n+    }\n+}\n+\n #[derive(Insertable, AsChangeset)]\n #[diesel(table_name = chunks)]\n struct NewChunk<'a> {\n@@ -35,73 +51,87 @@ struct NewChunk<'a> {\n     pub data: &'a [u8],\n }\n \n+#[async_trait::async_trait]\n pub(crate) trait ChunkStorageTrait {\n-    fn conn(&self) -> &Mutex<SqliteConn>;\n+    fn conn(&self) -> &LocalDatabase;\n     fn local_symkey(&self) -> &SecretKey;\n     fn time_provider(&self) -> &TimeProvider;\n \n-    fn create_db(&self) -> FSResult<()> {\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        sql_query(\n-            \"CREATE TABLE IF NOT EXISTS chunks (\n-                chunk_id BLOB PRIMARY KEY NOT NULL, -- UUID\n-                size INTEGER NOT NULL,\n-                offline BOOLEAN NOT NULL,\n-                accessed_on REAL, -- Timestamp\n-                data BLOB NOT NULL\n-            );\",\n-        )\n-        .execute(conn)\n-        .map_err(|e| FSError::CreateTable(format!(\"chunks {e}\")))?;\n+    async fn create_db(&self) -> FSResult<()> {\n+        self.conn()\n+            .exec(|conn| {\n+                conn.exclusive_transaction(|conn| {\n+                    sql_query(std::include_str!(\"sql/create-chunks-table.sql\")).execute(conn)?;\n+                    sql_query(std::include_str!(\"sql/create-remanence-table.sql\")).execute(conn)\n+                })\n+            })\n+            .await?;\n         Ok(())\n     }\n \n+    fn close_connection(&self) {\n+        self.conn().close();\n+    }\n+\n     #[cfg(test)]\n-    fn drop_db(&self) -> FSResult<()> {\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        sql_query(\"DROP TABLE IF EXISTS chunks;\")\n-            .execute(conn)\n-            .map_err(|e| FSError::DropTable(format!(\"chunks {e}\")))?;\n+    async fn drop_db(&self) -> FSResult<()> {\n+        self.conn()\n+            .exec(|conn| {\n+                sql_query(\"DROP TABLE IF EXISTS chunks;\").execute(conn)?;\n+                sql_query(\"DROP TABLE IF EXISTS remanence;\").execute(conn)\n+            })\n+            .await?;\n         Ok(())\n     }\n \n     // Size and chunks\n \n-    fn get_nb_blocks(&self) -> FSResult<i64> {\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        chunks::table\n-            .select(count_star())\n-            .first(conn)\n-            .map_err(|e| FSError::QueryTable(format!(\"chunks: get_nb_blocks {e}\")))\n+    async fn get_nb_blocks(&self) -> FSResult<i64> {\n+        self.conn()\n+            .exec_with_error_handler(\n+                |conn| chunks::table.select(count_star()).first(conn),\n+                |e| FSError::QueryTable(format!(\"chunks: get_nb_blocks {e}\")),\n+            )\n+            .await\n     }\n \n-    fn get_total_size(&self) -> FSResult<i64> {\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        chunks::table\n-            .select(CoalesceTotalSize::default())\n-            .first(conn)\n-            .map_err(|e| FSError::QueryTable(format!(\"chunks: get_total_size {e}\")))\n+    async fn get_total_size(&self) -> FSResult<i64> {\n+        let conn = self.conn();\n+        conn.exec_with_error_handler(\n+            |conn| {\n+                chunks::table\n+                    .select(CoalesceTotalSize::default())\n+                    .first(conn)\n+            },\n+            |e| FSError::QueryTable(format!(\"chunks: get_total_size {e}\")),\n+        )\n+        .await\n     }\n \n-    fn is_chunk(&self, chunk_id: ChunkID) -> FSResult<bool> {\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        chunks::table\n-            .select(count_star())\n-            .filter(chunks::chunk_id.eq((*chunk_id).as_ref()))\n-            .first::<i64>(conn)\n-            .map_err(|e| FSError::QueryTable(format!(\"chunks: is_chunk {e}\")))\n-            .map(|res| res > 0)\n+    async fn is_chunk(&self, chunk_id: ChunkID) -> FSResult<bool> {\n+        self.conn()\n+            .exec_with_error_handler(\n+                move |conn| {\n+                    chunks::table\n+                        .select(count_star())\n+                        .filter(chunks::chunk_id.eq((*chunk_id).as_ref()))\n+                        .first::<i64>(conn)\n+                        .map(|res| res > 0)\n+                },\n+                |e| FSError::QueryTable(format!(\"chunks: is_chunk {e}\")),\n+            )\n+            .await\n     }\n \n-    fn get_local_chunk_ids(&self, chunk_ids: &[ChunkID]) -> FSResult<Vec<ChunkID>> {\n+    async fn get_local_chunk_ids(&self, chunk_ids: &[ChunkID]) -> FSResult<Vec<ChunkID>> {\n         let bytes_id_list = chunk_ids\n             .iter()\n-            .map(|chunk_id| (**chunk_id).as_ref())\n+            .map(|chunk_id| (**chunk_id).as_ref().to_vec())\n             .collect::<Vec<_>>();\n \n         let mut res = Vec::with_capacity(chunk_ids.len());\n \n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n+        let conn = self.conn();\n         // The number of loop iteration is expected to be rather low:\n         // typically considering 4ko per chunk (i.e. the size of the buffer the\n         // Linux Kernel send us through Fuse), each query could perform ~4mo of data.\n@@ -111,83 +141,113 @@ pub(crate) trait ChunkStorageTrait {\n         // means those correspond to local modification and hence can be filtered out.\n         // With this we would only provide the chunks corresponding to a synced block\n         // which are 512ko big, so each query performs up to 512Mo !\n-        for bytes_id_list_chunk in bytes_id_list.chunks(SQLITE_MAX_VARIABLE_NUMBER) {\n-            res.append(\n-                &mut chunks::table\n+        let futures = bytes_id_list\n+            .chunks(LOCAL_DATABASE_MAX_VARIABLE_NUMBER)\n+            .map(|bytes_id_list_chunk| {\n+                let query = chunks::table\n                     .select(chunks::chunk_id)\n-                    .filter(chunks::chunk_id.eq_any(bytes_id_list_chunk))\n-                    .load::<Vec<u8>>(conn)\n-                    .map_err(|e| FSError::QueryTable(format!(\"chunks: get_local_chunk_ids {e}\")))?\n+                    .filter(chunks::chunk_id.eq_any(bytes_id_list_chunk.to_vec()));\n+\n+                conn.exec_with_error_handler(\n+                    move |conn| query.load::<Vec<u8>>(conn),\n+                    |e| FSError::QueryTable(format!(\"chunks: get_local_chunk_ids {e}\")),\n+                )\n+            });\n+        for chunks in future::join_all(futures).await {\n+            let mut chunks = chunks.and_then(|chunk| {\n+                chunk\n                     .into_iter()\n                     .map(|chunk_id| {\n-                        <[u8; 16]>::try_from(&chunk_id[..]).map_err(|_| {\n+                        ChunkID::try_from(chunk_id.as_slice()).map_err(|_| {\n                             FSError::QueryTable(format!(\"chunks: corrupted chunk_id {chunk_id:?}\"))\n                         })\n                     })\n-                    .collect::<Result<Vec<_>, _>>()?\n-                    .into_iter()\n-                    .map(ChunkID::from)\n-                    .collect(),\n-            )\n+                    .collect::<Result<Vec<_>, _>>()\n+            })?;\n+\n+            res.append(&mut chunks)\n         }\n \n         Ok(res)\n     }\n \n-    fn get_chunk(&self, chunk_id: ChunkID) -> FSResult<Vec<u8>> {\n+    async fn get_chunk(&self, chunk_id: ChunkID) -> FSResult<Vec<u8>> {\n         let accessed_on = self.time_provider().now().get_f64_with_us_precision();\n \n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        let changes =\n-            diesel::update(chunks::table.filter(chunks::chunk_id.eq((*chunk_id).as_ref())))\n-                .set(chunks::accessed_on.eq(accessed_on))\n-                .execute(conn)\n-                .map_err(|e| FSError::UpdateTable(format!(\"chunks: get_chunk {e}\")))?\n-                > 0;\n+        let conn = self.conn();\n+\n+        let changes = conn\n+            .exec_with_error_handler(\n+                move |conn| {\n+                    conn.exclusive_transaction(|conn| {\n+                        diesel::update(\n+                            chunks::table.filter(chunks::chunk_id.eq((*chunk_id).as_ref())),\n+                        )\n+                        .set(chunks::accessed_on.eq(accessed_on))\n+                        .execute(conn)\n+                    })\n+                },\n+                |e| FSError::UpdateTable(format!(\"chunks: get_chunk {e}\")),\n+            )\n+            .await?\n+            > 0;\n \n         if !changes {\n             return Err(FSError::LocalMiss(*chunk_id));\n         }\n \n-        let ciphered = chunks::table\n-            .select(chunks::data)\n-            .filter(chunks::chunk_id.eq((*chunk_id).as_ref()))\n-            .first::<Vec<u8>>(conn)\n-            .map_err(|e| FSError::QueryTable(format!(\"chunks: get_chunk {e}\")))?;\n+        let ciphered = conn\n+            .exec_with_error_handler(\n+                move |conn| {\n+                    chunks::table\n+                        .select(chunks::data)\n+                        .filter(chunks::chunk_id.eq((*chunk_id).as_ref()))\n+                        .first::<Vec<u8>>(conn)\n+                },\n+                |e| FSError::QueryTable(format!(\"chunks: get_chunk {e}\")),\n+            )\n+            .await?;\n \n         Ok(self.local_symkey().decrypt(&ciphered)?)\n     }\n \n-    fn set_chunk(&self, chunk_id: ChunkID, raw: &[u8]) -> FSResult<()> {\n+    async fn set_chunk(&self, chunk_id: ChunkID, raw: &[u8]) -> FSResult<()> {\n         let ciphered = self.local_symkey().encrypt(raw);\n         let accessed_on = self.time_provider().now();\n \n-        let new_chunk = NewChunk {\n-            chunk_id: (*chunk_id).as_ref(),\n-            size: ciphered.len() as i64,\n-            offline: false,\n-            accessed_on: Some(accessed_on.into()),\n-            data: &ciphered,\n-        };\n-\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        diesel::insert_into(chunks::table)\n-            .values(&new_chunk)\n-            .on_conflict(chunks::chunk_id)\n-            .do_update()\n-            .set(&new_chunk)\n-            .execute(conn)\n-            .map_err(|e| FSError::InsertTable(format!(\"chunks: set_chunk {e}\")))?;\n+        self.conn()\n+            .exec(move |conn| {\n+                let new_chunk = NewChunk {\n+                    chunk_id: (*chunk_id).as_ref(),\n+                    size: ciphered.len() as i64,\n+                    offline: false,\n+                    accessed_on: Some(accessed_on.into()),\n+                    data: ciphered.as_ref(),\n+                };\n+                conn.exclusive_transaction(|conn| {\n+                    diesel::insert_into(chunks::table)\n+                        .values(&new_chunk)\n+                        .on_conflict(chunks::chunk_id)\n+                        .do_update()\n+                        .set(&new_chunk)\n+                        .execute(conn)\n+                })\n+            })\n+            .await?;\n         Ok(())\n     }\n \n-    fn clear_chunk(&self, chunk_id: ChunkID) -> FSResult<()> {\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        let changes =\n-            diesel::delete(chunks::table.filter(chunks::chunk_id.eq((*chunk_id).as_ref())))\n-                .execute(conn)\n-                .map_err(|e| FSError::DeleteTable(format!(\"chunks: clear_chunk {e}\")))?\n-                > 0;\n+    async fn clear_chunk(&self, chunk_id: ChunkID) -> FSResult<()> {\n+        let changes = self\n+            .conn()\n+            .exec(move |conn| {\n+                conn.exclusive_transaction(|conn| {\n+                    diesel::delete(chunks::table.filter(chunks::chunk_id.eq((*chunk_id).as_ref())))\n+                        .execute(conn)\n+                })\n+            })\n+            .await?\n+            > 0;\n \n         if !changes {\n             return Err(FSError::LocalMiss(*chunk_id));\n@@ -196,16 +256,40 @@ pub(crate) trait ChunkStorageTrait {\n         Ok(())\n     }\n \n-    fn run_vacuum(&self) -> FSResult<()> {\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        sql_query(\"VACUUM;\")\n-            .execute(conn)\n-            .map_err(|e| FSError::Vacuum(e.to_string()))?;\n+    /// Clear the chunks identified by the chunk ids list `chunk_ids`.\n+    async fn clear_chunks(&self, chunk_ids: &[ChunkID]) -> FSResult<()> {\n+        let chunk_ids = chunk_ids\n+            .iter()\n+            .map(|id| id.as_bytes().to_vec())\n+            .collect::<Vec<_>>();\n+        self.conn()\n+            .exec(move |conn| {\n+                conn.exclusive_transaction(|conn| {\n+                    chunk_ids\n+                        .chunks(LOCAL_DATABASE_MAX_VARIABLE_NUMBER)\n+                        .try_for_each(|chunked_ids| {\n+                            diesel::delete(\n+                                chunks::table.filter(chunks::chunk_id.eq_any(chunked_ids.iter())),\n+                            )\n+                            .execute(conn)\n+                            .map(|_| ())\n+                        })\n+                })\n+            })\n+            .await?;\n         Ok(())\n     }\n+\n+    async fn run_vacuum(&self) -> FSResult<()> {\n+        self.conn()\n+            .vacuum()\n+            .await\n+            .map_err(|e| FSError::Vacuum(e.to_string()))\n+    }\n }\n \n-pub(crate) trait BlockStorageTrait: ChunkStorageTrait {\n+#[async_trait::async_trait]\n+pub(crate) trait BlockStorageTrait: ChunkStorageTrait + Remanence {\n     fn cache_size(&self) -> u64;\n \n     // Garbage collection\n@@ -214,56 +298,67 @@ pub(crate) trait BlockStorageTrait: ChunkStorageTrait {\n         self.cache_size() / *DEFAULT_BLOCK_SIZE\n     }\n \n-    fn clear_all_blocks(&self) -> FSResult<()> {\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        diesel::delete(chunks::table)\n-            .execute(conn)\n-            .map_err(|e| FSError::DeleteTable(format!(\"chunks: clear_all_blocks {e}\")))?;\n+    async fn clear_all_blocks(&self) -> FSResult<()> {\n+        self.conn()\n+            .exec(|conn| diesel::delete(chunks::table).execute(conn))\n+            .await?;\n         Ok(())\n     }\n \n     // Upgraded set method\n \n-    fn set_chunk_upgraded(&self, chunk_id: ChunkID, raw: &[u8]) -> FSResult<()> {\n+    async fn set_clean_block(&self, block_id: BlockID, raw: &[u8]) -> FSResult<HashSet<BlockID>> {\n         let ciphered = self.local_symkey().encrypt(raw);\n         let accessed_on = self.time_provider().now();\n \n-        let new_chunk = NewChunk {\n-            chunk_id: (*chunk_id).as_ref(),\n-            size: ciphered.len() as i64,\n-            offline: false,\n-            accessed_on: Some(accessed_on.into()),\n-            data: &ciphered,\n-        };\n-\n         // Insert the chunk\n-        diesel::insert_into(chunks::table)\n-            .values(&new_chunk)\n-            .on_conflict(chunks::chunk_id)\n-            .do_update()\n-            .set(&new_chunk)\n-            .execute(&mut *self.conn().lock().expect(\"Mutex is poisoned\"))\n-            .map_err(|e| FSError::InsertTable(format!(\"chunks: set_chunk {e}\")))?;\n+        self.conn()\n+            .exec(move |conn| {\n+                let new_chunk = NewChunk {\n+                    chunk_id: (*block_id).as_ref(),\n+                    size: ciphered.len() as i64,\n+                    offline: false,\n+                    accessed_on: Some(accessed_on.into()),\n+                    data: ciphered.as_ref(),\n+                };\n+\n+                conn.exclusive_transaction(|conn| {\n+                    diesel::insert_into(chunks::table)\n+                        .values(&new_chunk)\n+                        .on_conflict(chunks::chunk_id)\n+                        .do_update()\n+                        .set(&new_chunk)\n+                        .execute(conn)\n+                })\n+            })\n+            .await?;\n \n         // Perform cleanup if necessary\n-        self.cleanup()\n+        self.cleanup().await\n     }\n \n-    fn cleanup(&self) -> FSResult<()> {\n-        // Count the chunks\n-        let conn = &mut *self.conn().lock().expect(\"Mutex is poisoned\");\n-        let nb_blocks = chunks::table\n-            .select(count_star())\n-            .first::<i64>(conn)\n-            .map_err(|e| FSError::QueryTable(format!(\"chunks: cleanup {e}\")))?;\n+    async fn cleanup(&self) -> FSResult<HashSet<BlockID>> {\n+        // No cleanup for remanent storage.\n+        if self.is_block_remanent() {\n+            return Ok(HashSet::default());\n+        }\n+\n+        // Count the chunks.\n+        let conn = self.conn();\n+        let nb_blocks = conn\n+            .exec_with_error_handler(\n+                |conn| chunks::table.select(count_star()).first::<i64>(conn),\n+                |e| FSError::QueryTable(format!(\"chunks: cleanup {e}\")),\n+            )\n+            .await?;\n \n         let block_limit = self.block_limit() as i64;\n \n         let extra_blocks = nb_blocks - block_limit;\n \n         // No cleanup is needed\n         if extra_blocks <= 0 {\n-            return Ok(());\n+            return Ok(HashSet::new());\n         }\n \n         // Remove the extra block plus 10% of the cache size, i.e 100 blocks\n@@ -275,23 +370,38 @@ pub(crate) trait BlockStorageTrait: ChunkStorageTrait {\n             .limit(limit)\n             .into_boxed();\n \n-        diesel::delete(chunks::table.filter(chunks::chunk_id.eq_any(sub_query)))\n-            .execute(conn)\n-            .map_err(|e| FSError::DeleteTable(format!(\"chunks: cleanup {e}\")))?;\n-\n-        Ok(())\n+        conn.exec(|conn| {\n+            conn.exclusive_transaction(|conn| {\n+                diesel::delete(chunks::table.filter(chunks::chunk_id.eq_any(sub_query)))\n+                    .returning(chunks::chunk_id)\n+                    .get_results(conn)\n+            })\n+        })\n+        .await\n+        .map_err(FSError::from)\n+        .and_then(|res: Vec<Vec<u8>>| {\n+            res.into_iter()\n+                .map(|raw_chunk| {\n+                    BlockID::try_from(&raw_chunk[..]).map_err(|_| {\n+                        FSError::QueryTable(format!(\"Chunks: corrupted block_id {raw_chunk:?}\"))\n+                    })\n+                })\n+                .collect::<FSResult<HashSet<_>>>()\n+        })\n     }\n }\n \n-// Interface to access the local chunks of data\n+/// Interface to access the local chunks of data\n+// TODO: To improve perf, we would want to have the operations made in chunkstorage only commited when needed.\n+// i.e. We want to only commit chunks that are referenced by a manifest, so until they're referenced they may be not needed or changed.\n pub(crate) struct ChunkStorage {\n-    conn: Mutex<SqliteConn>,\n+    conn: Arc<LocalDatabase>,\n     local_symkey: SecretKey,\n     time_provider: TimeProvider,\n }\n \n impl ChunkStorageTrait for ChunkStorage {\n-    fn conn(&self) -> &Mutex<SqliteConn> {\n+    fn conn(&self) -> &LocalDatabase {\n         &self.conn\n     }\n     fn local_symkey(&self) -> &SecretKey {\n@@ -303,31 +413,33 @@ impl ChunkStorageTrait for ChunkStorage {\n }\n \n impl ChunkStorage {\n-    pub fn new(\n+    pub async fn new(\n         local_symkey: SecretKey,\n-        conn: Mutex<SqliteConn>,\n+        conn: Arc<LocalDatabase>,\n         time_provider: TimeProvider,\n     ) -> FSResult<Self> {\n         let instance = Self {\n             conn,\n             local_symkey,\n             time_provider,\n         };\n-        instance.create_db()?;\n+        instance.create_db().await?;\n         Ok(instance)\n     }\n }\n \n // Interface for caching the data blocks.\n pub(crate) struct BlockStorage {\n-    conn: Mutex<SqliteConn>,\n+    conn: LocalDatabase,\n     local_symkey: SecretKey,\n     cache_size: u64,\n     time_provider: TimeProvider,\n+    /// Flag that enable/disable the block remanence.\n+    block_remanent_enabled: AtomicBool,\n }\n \n impl ChunkStorageTrait for BlockStorage {\n-    fn conn(&self) -> &Mutex<SqliteConn> {\n+    fn conn(&self) -> &LocalDatabase {\n         &self.conn\n     }\n     fn local_symkey(&self) -> &SecretKey {\n@@ -345,9 +457,9 @@ impl BlockStorageTrait for BlockStorage {\n }\n \n impl BlockStorage {\n-    pub fn new(\n+    pub async fn new(\n         local_symkey: SecretKey,\n-        conn: Mutex<SqliteConn>,\n+        conn: LocalDatabase,\n         cache_size: u64,\n         time_provider: TimeProvider,\n     ) -> FSResult<Self> {\n@@ -356,96 +468,370 @@ impl BlockStorage {\n             local_symkey,\n             cache_size,\n             time_provider,\n+            block_remanent_enabled: AtomicBool::new(false),\n         };\n-        instance.create_db()?;\n+        instance.create_db().await?;\n+        instance.load_remanence_flag().await?;\n         Ok(instance)\n     }\n }\n \n+// TODO: Remove it !\n+// We would need to refactor the storage later.\n+// We would combine the `manifest + chunk` storage that both use the `data` database and let the `block storage` on it's own.\n+// At that moment we could more easily merge the trait directly into `block` storage.\n+#[async_trait::async_trait]\n+pub trait Remanence {\n+    /// Is block remanence enabled ?\n+    fn is_block_remanent(&self) -> bool;\n+    /// Enable block remanence.\n+    /// Return `true` if the value has changed from the previous state.\n+    async fn enable_block_remanence(&self) -> FSResult<bool>;\n+    /// Disable block remanence.\n+    /// If the block remanence was disabled by this call,\n+    /// It will return the list of block that where clean-up.\n+    async fn disable_block_remanence(&self) -> FSResult<Option<HashSet<BlockID>>>;\n+    /// Remove from the database, the chunk whose id is in `chunk_ids` and it's access time older than `not_accessed_after`.\n+    async fn clear_unreferenced_chunks(\n+        &self,\n+        chunk_ids: &[ChunkID],\n+        not_accessed_after: libparsec_types::DateTime,\n+    ) -> FSResult<()>;\n+}\n+\n+impl BlockStorage {\n+    async fn load_remanence_flag(&self) -> FSResult<()> {\n+        let remanence_flag = self\n+            .conn\n+            .exec(move |conn| {\n+                remanence::table\n+                    .select(remanence::block_remanent)\n+                    .first(conn)\n+                    .optional()\n+            })\n+            .await?;\n+\n+        self.block_remanent_enabled\n+            .store(remanence_flag.unwrap_or(false), Ordering::SeqCst);\n+        Ok(())\n+    }\n+\n+    async fn set_block_remanence(&self, toggle: bool) -> Result<(), FSError> {\n+        self.conn\n+            .exec(move |conn| {\n+                conn.exclusive_transaction(|conn| {\n+                    diesel::sql_query(\n+                        \"INSERT OR REPLACE INTO remanence(_id, block_remanent) VALUES (0, ?)\",\n+                    )\n+                    .bind::<diesel::sql_types::Bool, _>(toggle)\n+                    .execute(conn)\n+                })\n+            })\n+            .await?;\n+        self.block_remanent_enabled.store(toggle, Ordering::SeqCst);\n+        Ok(())\n+    }\n+}\n+\n+#[async_trait::async_trait]\n+impl Remanence for BlockStorage {\n+    fn is_block_remanent(&self) -> bool {\n+        self.block_remanent_enabled.load(Ordering::SeqCst)\n+    }\n+\n+    async fn enable_block_remanence(&self) -> FSResult<bool> {\n+        if self.is_block_remanent() {\n+            return Ok(false);\n+        }\n+        self.set_block_remanence(true).await?;\n+        Ok(true)\n+    }\n+\n+    async fn disable_block_remanence(&self) -> FSResult<Option<HashSet<BlockID>>> {\n+        if !self.is_block_remanent() {\n+            return Ok(None);\n+        }\n+\n+        self.set_block_remanence(false).await?;\n+\n+        self.cleanup().await.map(Some)\n+    }\n+\n+    /// Remove chunks that aren't in `chunk_ids` and are older than `not_accessed_after`.\n+    async fn clear_unreferenced_chunks(\n+        &self,\n+        chunk_ids: &[ChunkID],\n+        not_accessed_after: libparsec_types::DateTime,\n+    ) -> FSResult<()> {\n+        table! {\n+            unreferenced_chunks(chunk_id) {\n+                chunk_id -> Binary,\n+            }\n+        }\n+\n+        if chunk_ids.is_empty() {\n+            return Ok(());\n+        }\n+\n+        let not_accessed_after = super::sql_types::DateTime::from(not_accessed_after);\n+\n+        let raw_chunk_ids = chunk_ids.to_vec();\n+        self.conn\n+            .exec(move |conn| {\n+                conn.exclusive_transaction(|conn| {\n+                    // We have an exclusive access to the sqlite database, so nobody else is using/modifing the table `unreferenced_chunks`.\n+                    // We drop the table to prevent using an different table scheme or having garbage data from a previous call that failed to cleanup the table.\n+                    sql_query(\"DROP TABLE IF EXISTS unreferenced_chunks\").execute(conn)?;\n+                    // This table only existe in this transaction.\n+                    // it will be drop at the end so we wont have collision problem.\n+                    // Even in the case of a failure since the transaction will rollback the change from before the creation of the table.\n+                    // Even tho the rollback fail, later call would be safe with the `Drop table ..` above.\n+                    sql_query(std::include_str!(\n+                        \"sql/create-temp-unreferenced-chunks-table.sql\"\n+                    ))\n+                    .execute(conn)?;\n+\n+                    // We register `chunks` that need to be safe from the deletion at the later step.\n+                    raw_chunk_ids\n+                        .chunks(LOCAL_DATABASE_MAX_VARIABLE_NUMBER)\n+                        .try_for_each(|chunked| {\n+                            let entries = chunked\n+                                .iter()\n+                                .map(|v| unreferenced_chunks::chunk_id.eq(v.as_bytes().as_slice()))\n+                                .collect::<Vec<_>>();\n+\n+                            diesel::insert_or_ignore_into(unreferenced_chunks::table)\n+                                .values(&entries)\n+                                .execute(conn)\n+                                .and(Ok(()))\n+                        })?;\n+\n+                    // Sub query to select all chunk_id associated with the current transaction.\n+                    let sub_query = unreferenced_chunks::table\n+                        .select(unreferenced_chunks::chunk_id)\n+                        .into_boxed();\n+\n+                    // Remove unreferenced chunks from `chunks` table.\n+                    diesel::delete(\n+                        chunks::table.filter(\n+                            not(chunks::chunk_id.eq_any(sub_query))\n+                                .and(chunks::accessed_on.lt(not_accessed_after)),\n+                        ),\n+                    )\n+                    .execute(conn)?;\n+\n+                    sql_query(\"DROP TABLE unreferenced_chunks\").execute(conn)\n+                })\n+                .and(Ok(()))\n+            })\n+            .await\n+            .map_err(FSError::from)\n+    }\n+}\n+\n #[cfg(test)]\n mod tests {\n-    use std::collections::hash_map::RandomState;\n-    use std::collections::HashSet;\n+    use libparsec_platform_local_db::VacuumMode;\n+    use std::{collections::HashSet, sync::Arc};\n     use uuid::Uuid;\n \n     use libparsec_tests_fixtures::{tmp_path, TmpPath};\n     use rstest::rstest;\n \n     use super::*;\n-    use crate::storage::local_database::SqlitePool;\n \n     #[rstest]\n-    fn chunk_storage(tmp_path: TmpPath) {\n-        let db_path = tmp_path.join(\"chunk_storage.sqlite\");\n-        let pool = SqlitePool::new(db_path.to_str().unwrap()).unwrap();\n-        let conn = Mutex::new(pool.conn().unwrap());\n+    #[tokio::test]\n+    async fn chunk_storage(tmp_path: TmpPath) {\n+        let chunk_db_path = tmp_path.join(\"chunk_storage.sqlite\");\n+        let chunk_conn =\n+            LocalDatabase::from_path(chunk_db_path.to_str().unwrap(), VacuumMode::default())\n+                .await\n+                .unwrap();\n+\n+        let chunk_conn = Arc::new(chunk_conn);\n         let local_symkey = SecretKey::generate();\n \n-        let chunk_storage = ChunkStorage::new(local_symkey, conn, TimeProvider::default()).unwrap();\n+        let chunk_storage = ChunkStorage::new(local_symkey, chunk_conn, TimeProvider::default())\n+            .await\n+            .unwrap();\n \n         // Initialization\n-        chunk_storage.drop_db().unwrap();\n-        chunk_storage.create_db().unwrap();\n+        chunk_storage.drop_db().await.unwrap();\n+        chunk_storage.create_db().await.unwrap();\n+\n+        const CHUNK_TO_INSERT: usize = 2000;\n+        test_chunk_interface::<ChunkStorage, CHUNK_TO_INSERT>(&chunk_storage).await;\n+    }\n \n-        assert_eq!(chunk_storage.get_nb_blocks().unwrap(), 0);\n-        assert_eq!(chunk_storage.get_total_size().unwrap(), 0);\n+    async fn test_chunk_interface<\n+        S: ChunkStorageTrait + Send + Sync,\n+        const CHUNK_TO_INSERT: usize,\n+    >(\n+        storage: &S,\n+    ) {\n+        assert_eq!(storage.get_nb_blocks().await.unwrap(), 0);\n+        assert_eq!(storage.get_total_size().await.unwrap(), 0);\n \n         // Generate chunks\n-        let chunk_id = ChunkID::from(Uuid::new_v4());\n-        chunk_storage.set_chunk(chunk_id, &[1, 2, 3, 4]).unwrap();\n+        const RAW_CHUNK_DATA: [u8; 4] = [1, 2, 3, 4];\n+\n+        let chunk_ids =\n+            insert_random_chunk::<S, CHUNK_TO_INSERT>(storage, RAW_CHUNK_DATA.as_slice()).await;\n+\n+        assert_eq!(\n+            storage.get_nb_blocks().await.unwrap(),\n+            CHUNK_TO_INSERT as i64\n+        );\n+        assert_eq!(\n+            storage.get_total_size().await.unwrap(),\n+            CHUNK_TO_INSERT as i64 * 44\n+        );\n \n-        const N: usize = 2000;\n+        // Retrieve chunks\n+        let random_index = 42;\n+        assert!(random_index < CHUNK_TO_INSERT);\n+        let chunk_id = chunk_ids[random_index];\n+\n+        assert!(storage.is_chunk(chunk_id).await.unwrap());\n+        assert_eq!(storage.get_chunk(chunk_id).await.unwrap(), &RAW_CHUNK_DATA);\n+\n+        // Retrieve missing chunks.\n+        let random_chunk_id = ChunkID::from(Uuid::new_v4());\n+        assert!(\n+            !chunk_ids.iter().any(|id| id == &random_chunk_id),\n+            \"The random generated chunk_id MUST not be already inserted in the storage by the previous step\"\n+        );\n+        operation_on_missing_chunk(storage, random_chunk_id).await;\n+\n+        let local_chunk_ids = storage.get_local_chunk_ids(&chunk_ids).await.unwrap();\n+        let set0 = local_chunk_ids.iter().collect::<HashSet<_>>();\n+        let set1 = chunk_ids.iter().collect::<HashSet<_>>();\n+\n+        assert_eq!(set0.len(), CHUNK_TO_INSERT);\n+        assert_eq!(set1.len(), CHUNK_TO_INSERT);\n+\n+        assert_eq!(set0.intersection(&set1).count(), CHUNK_TO_INSERT);\n+\n+        remove_chunk_from_storage(storage, chunk_id).await;\n+\n+        assert_eq!(\n+            storage.get_nb_blocks().await.unwrap(),\n+            CHUNK_TO_INSERT as i64 - 1\n+        );\n+        assert_eq!(\n+            storage.get_total_size().await.unwrap(),\n+            (CHUNK_TO_INSERT as i64 - 1) * 44\n+        );\n+    }\n \n-        let mut chunk_ids = Vec::with_capacity(N);\n+    async fn insert_random_chunk<\n+        S: ChunkStorageTrait + Send + Sync,\n+        const CHUNK_TO_INSERT: usize,\n+    >(\n+        storage: &S,\n+        chunk_data: &[u8],\n+    ) -> Vec<ChunkID> {\n+        let mut chunk_ids = Vec::with_capacity(CHUNK_TO_INSERT);\n \n-        for _ in 0..N {\n+        for _ in 0..CHUNK_TO_INSERT {\n             let chunk_id = ChunkID::from(Uuid::new_v4());\n             chunk_ids.push(chunk_id);\n \n-            chunk_storage.set_chunk(chunk_id, &[1, 2, 3, 4]).unwrap();\n+            storage.set_chunk(chunk_id, chunk_data).await.unwrap();\n         }\n \n-        assert_eq!(chunk_storage.get_nb_blocks().unwrap(), N as i64 + 1);\n-        assert_eq!(chunk_storage.get_total_size().unwrap(), (N as i64 + 1) * 44);\n-\n-        // Retrieve chunks\n-        assert!(chunk_storage.is_chunk(chunk_id).unwrap());\n-        assert_eq!(&chunk_storage.get_chunk(chunk_id).unwrap(), &[1, 2, 3, 4]);\n+        chunk_ids\n+    }\n \n-        let local_chunk_ids = chunk_storage.get_local_chunk_ids(&chunk_ids).unwrap();\n-        let set0: HashSet<_, RandomState> = HashSet::from_iter(local_chunk_ids.iter());\n-        let set1: HashSet<_, RandomState> = HashSet::from_iter(chunk_ids.iter());\n-        assert_eq!(set0.len(), N);\n-        assert_eq!(set1.len(), N);\n-        assert_eq!(set0.intersection(&set1).count(), N);\n+    async fn remove_chunk_from_storage<S: ChunkStorageTrait + Send + Sync>(\n+        storage: &S,\n+        chunk_id: ChunkID,\n+    ) {\n+        storage.clear_chunk(chunk_id).await.unwrap();\n+        operation_on_missing_chunk(storage, chunk_id).await\n+    }\n \n-        chunk_storage.clear_chunk(chunk_id).unwrap();\n+    async fn operation_on_missing_chunk<S: ChunkStorageTrait + Send + Sync>(\n+        storage: &S,\n+        chunk_id: ChunkID,\n+    ) {\n+        assert!(!storage.is_chunk(chunk_id).await.unwrap());\n+        assert_eq!(\n+            storage.get_chunk(chunk_id).await,\n+            Err(FSError::LocalMiss(*chunk_id))\n+        );\n+\n+        assert_eq!(\n+            storage.clear_chunk(chunk_id).await,\n+            Err(FSError::LocalMiss(*chunk_id))\n+        );\n+    }\n \n-        assert_eq!(chunk_storage.get_nb_blocks().unwrap(), N as i64);\n-        assert_eq!(chunk_storage.get_total_size().unwrap(), N as i64 * 44);\n+    #[rstest]\n+    #[tokio::test]\n+    async fn test_block_storage(tmp_path: TmpPath) {\n+        let block_db_path = tmp_path.join(\"block_storage.sqlite\");\n+        let block_conn =\n+            LocalDatabase::from_path(block_db_path.to_str().unwrap(), VacuumMode::default())\n+                .await\n+                .unwrap();\n \n-        let new_conn = Mutex::new(pool.conn().unwrap());\n         let local_symkey = SecretKey::generate();\n-        let cache_size = *DEFAULT_BLOCK_SIZE * 1024;\n+        const CACHE_SIZE: u64 = DEFAULT_BLOCK_SIZE.inner() * 1024;\n+\n+        let block_storage = BlockStorage::new(\n+            local_symkey,\n+            block_conn,\n+            CACHE_SIZE,\n+            TimeProvider::default(),\n+        )\n+        .await\n+        .unwrap();\n \n-        let block_storage =\n-            BlockStorage::new(local_symkey, new_conn, cache_size, TimeProvider::default()).unwrap();\n+        block_storage.drop_db().await.unwrap();\n+        block_storage.create_db().await.unwrap();\n \n-        assert_eq!(block_storage.block_limit(), 1024);\n+        const CHUNK_TO_INSERT: usize = 2000;\n \n-        // Generate chunks\n-        let chunk_id = ChunkID::from(Uuid::new_v4());\n-        block_storage\n-            .set_chunk_upgraded(chunk_id, &[1, 2, 3, 4])\n+        test_block_interface::<BlockStorage, CACHE_SIZE, CHUNK_TO_INSERT>(block_storage).await;\n+    }\n+\n+    async fn test_block_interface<\n+        S: BlockStorageTrait + Send + Sync,\n+        const CACHE_SIZE: u64,\n+        const CHUNK_TO_INSERT: usize,\n+    >(\n+        storage: S,\n+    ) {\n+        test_chunk_interface::<S, CHUNK_TO_INSERT>(&storage).await;\n+        let block_limit = storage.block_limit();\n+        assert_eq!(block_limit, (CACHE_SIZE / *DEFAULT_BLOCK_SIZE));\n+\n+        let block_id = BlockID::from(Uuid::new_v4());\n+        const RAW_BLOCK_DATA: [u8; 4] = [5, 6, 7, 8];\n+        storage\n+            .set_clean_block(block_id, &RAW_BLOCK_DATA)\n+            .await\n             .unwrap();\n \n-        let blocks_left = std::cmp::min(N as i64 + 1, 1024 - 1024 / 10);\n-        assert_eq!(chunk_storage.get_nb_blocks().unwrap(), blocks_left);\n-        assert_eq!(chunk_storage.get_total_size().unwrap(), blocks_left * 44);\n+        let blocks_left = std::cmp::min(\n+            CHUNK_TO_INSERT as i64 + 1,\n+            block_limit as i64 - block_limit as i64 / 10,\n+        );\n+\n+        assert_eq!(storage.get_nb_blocks().await.unwrap(), blocks_left);\n+        /// `test_chunk_interface` & `test_block_interface` both use DATA that is 4 bytes wide.\n+        /// The value below correspond to the size of that data but after encryption.\n+        const CHUNK_SIZE: usize = 44;\n+        assert_eq!(\n+            storage.get_total_size().await.unwrap(),\n+            blocks_left * CHUNK_SIZE as i64\n+        );\n \n-        block_storage.clear_all_blocks().unwrap();\n+        storage.clear_all_blocks().await.unwrap();\n \n-        assert_eq!(chunk_storage.get_nb_blocks().unwrap(), 0);\n-        assert_eq!(chunk_storage.get_total_size().unwrap(), 0);\n+        assert_eq!(storage.get_nb_blocks().await.unwrap(), 0);\n+        assert_eq!(storage.get_total_size().await.unwrap(), 0);\n     }\n }"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/local_database.rs",
      "status": "removed",
      "patch": "@@ -1,48 +0,0 @@\n-// Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n-\n-use diesel::connection::SimpleConnection;\n-use diesel::r2d2::{ConnectionManager, Pool, PooledConnection};\n-use diesel::SqliteConnection;\n-use std::path::PathBuf;\n-\n-use crate::error::{FSError, FSResult};\n-\n-// https://www.sqlite.org/limits.html\n-// #9 Maximum Number Of Host Parameters In A Single SQL Statement\n-pub const SQLITE_MAX_VARIABLE_NUMBER: usize = 999;\n-pub struct SqlitePool(Pool<ConnectionManager<SqliteConnection>>);\n-pub type SqliteConn = PooledConnection<ConnectionManager<SqliteConnection>>;\n-\n-impl SqlitePool {\n-    pub fn new<P: Into<String>>(path: P) -> FSResult<SqlitePool> {\n-        let path = path.into();\n-        if let Some(prefix) = PathBuf::from(&path).parent() {\n-            std::fs::create_dir_all(prefix).map_err(|_| FSError::CreateDir)?;\n-        }\n-        let manager = ConnectionManager::<SqliteConnection>::new(path);\n-        Pool::builder()\n-            .build(manager)\n-            .map_err(|_| FSError::Pool)\n-            .map(Self)\n-    }\n-\n-    pub fn conn(&self) -> FSResult<SqliteConn> {\n-        let mut conn = self\n-            .0\n-            .get()\n-            .map_err(|e| FSError::Connection(e.to_string()))?;\n-        // The combination of WAL journal mode and NORMAL synchronous mode\n-        // is a great combination: it allows for fast commits (~10 us compare\n-        // to 15 ms the default mode) but still protects the database against\n-        // corruption in the case of OS crash or power failure.\n-        conn.batch_execute(\n-            \"\n-            PRAGMA journal_mode = WAL; -- better write-concurrency\n-            PRAGMA synchronous = NORMAL; -- fsync only in critical moments\n-        \",\n-        )\n-        .map_err(|_| FSError::Configuration)?;\n-\n-        Ok(conn)\n-    }\n-}"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/manifest_storage.rs",
      "status": "modified",
      "patch": "@@ -4,21 +4,21 @@ use diesel::{\n     sql_query, table, AsChangeset, BoolExpressionMethods, ExpressionMethods, Insertable, QueryDsl,\n     RunQueryDsl,\n };\n-use regex::Regex;\n use std::{\n     collections::{hash_map::RandomState, HashMap, HashSet},\n-    sync::Mutex,\n+    sync::{Arc, Mutex},\n };\n \n use libparsec_client_types::LocalManifest;\n use libparsec_crypto::{CryptoError, SecretKey};\n-use libparsec_types::{BlockID, ChunkID, EntryID};\n+use libparsec_platform_async::future::TryFutureExt;\n+use libparsec_types::{BlockID, ChunkID, EntryID, Regex};\n \n-use super::local_database::{SqliteConn, SQLITE_MAX_VARIABLE_NUMBER};\n use crate::{\n     error::{FSError, FSResult},\n     storage::chunk_storage::chunks,\n };\n+use libparsec_platform_local_db::{LocalDatabase, LOCAL_DATABASE_MAX_VARIABLE_NUMBER};\n \n table! {\n     prevent_sync_pattern (_id) {\n@@ -89,142 +89,171 @@ struct NewPreventSyncPattern<'a> {\n \n pub struct ManifestStorage {\n     local_symkey: SecretKey,\n-    conn: Mutex<SqliteConn>,\n+    conn: Arc<LocalDatabase>,\n     pub realm_id: EntryID,\n     /// This cache contains all the manifests that have been set or accessed\n     /// since the last call to `clear_memory_cache`\n-    pub(crate) cache: Mutex<HashMap<EntryID, LocalManifest>>,\n-    /// This dictionary keeps track of all the entry ids of the manifests\n-    /// that have been added to the cache but still needs to be written to\n-    /// the conn. The corresponding value is a set with the ids of all\n-    /// the chunks that needs to be removed from the conn after the\n-    /// manifest is written. Note: this set might be empty but the manifest\n-    /// still requires to be flushed.\n-    cache_ahead_of_localdb: Mutex<HashMap<EntryID, HashSet<ChunkOrBlockID>>>,\n+    caches: Mutex<HashMap<EntryID, Arc<Mutex<CacheEntry>>>>,\n }\n \n-impl Drop for ManifestStorage {\n-    fn drop(&mut self) {\n-        self.flush_cache_ahead_of_persistance()\n-            .expect(\"Cannot flush cache when ManifestStorage dropped\");\n+/// A cache entry that may contain a manifest and its pending chunk.\n+#[derive(Default)]\n+struct CacheEntry {\n+    /// The current manifest for the cache.\n+    // TODO: manifest are immutable, so have an `Arc` here would simplify thing\n+    manifest: Option<LocalManifest>,\n+    /// The set of entries ids that are pending to be written.\n+    ///\n+    /// > Note: this set might be empty but the manifest\n+    /// > still requires to be flushed.\n+    pending_chunk_ids: Option<HashSet<ChunkOrBlockID>>,\n+}\n+\n+impl CacheEntry {\n+    /// Return `true` if we have chunk that need to be push to the local database.\n+    pub fn has_chunk_to_be_flush(&self) -> bool {\n+        self.pending_chunk_ids.is_some()\n     }\n }\n \n impl ManifestStorage {\n-    pub fn new(\n+    pub async fn new(\n         local_symkey: SecretKey,\n-        conn: Mutex<SqliteConn>,\n         realm_id: EntryID,\n+        conn: Arc<LocalDatabase>,\n     ) -> FSResult<Self> {\n         let instance = Self {\n             local_symkey,\n             conn,\n             realm_id,\n-            cache: Mutex::new(HashMap::new()),\n-            cache_ahead_of_localdb: Mutex::new(HashMap::new()),\n+            caches: Mutex::new(HashMap::default()),\n         };\n-        instance.create_db()?;\n+        instance.create_db().await?;\n         Ok(instance)\n     }\n \n+    /// Close the connection to the database.\n+    /// Before that it will try to flush data the are stored in cache\n+    // TODO: Rework the close to take ownership over the `ManifestStorage`\n+    // Currently this is only used in Python's `WorkspaceStorage.run` teardown\n+    // routine that has been carefully crafted to ensure `clear_memory_cache`\n+    // and `close_connection` are done without concurrency call of other manifest\n+    // storage methods.\n+    pub fn close_connection(&self) {\n+        let cache_guard = self.caches.lock().expect(\"Mutex is poisoned\");\n+        let res = cache_guard.iter().try_for_each(|(id, lock)| {\n+            let id = *id;\n+            let has_pending_chunks = {\n+                lock.lock()\n+                    .expect(\"Mutex is poisoned\")\n+                    .pending_chunk_ids\n+                    .is_some()\n+            };\n+            if has_pending_chunks {\n+                Err(format!(\"Manifest {id} has pending chunk not saved\"))\n+            } else {\n+                Ok(())\n+            }\n+        });\n+        if let Err(e) = res {\n+            // TODO: replace that by an error log\n+            eprintln!(\"Invalid state when closing the connection {e}\")\n+        }\n+        self.conn.close();\n+    }\n+\n     /// Database initialization\n-    pub fn create_db(&self) -> FSResult<()> {\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        sql_query(\n-            \"CREATE TABLE IF NOT EXISTS vlobs (\n-              vlob_id BLOB PRIMARY KEY NOT NULL, -- UUID\n-              base_version INTEGER NOT NULL,\n-              remote_version INTEGER NOT NULL,\n-              need_sync BOOLEAN NOT NULL,\n-              blob BLOB NOT NULL\n-            );\",\n-        )\n-        .execute(conn)\n-        .map_err(|e| FSError::CreateTable(format!(\"vlobs {e}\")))?;\n-        // Singleton storing the checkpoint\n-        sql_query(\n-            \"CREATE TABLE IF NOT EXISTS realm_checkpoint (\n-              _id INTEGER PRIMARY KEY NOT NULL,\n-              checkpoint INTEGER NOT NULL\n-            );\",\n-        )\n-        .execute(conn)\n-        .map_err(|e| FSError::CreateTable(format!(\"realm_checkpoint {e}\")))?;\n-        // Singleton storing the prevent_sync_pattern\n-        sql_query(\n-            \"CREATE TABLE IF NOT EXISTS prevent_sync_pattern (\n-              _id INTEGER PRIMARY KEY NOT NULL,\n-              pattern TEXT NOT NULL,\n-              fully_applied BOOLEAN NOT NULL\n-            );\",\n+    pub async fn create_db(&self) -> FSResult<()> {\n+        let conn = &self.conn;\n+\n+        conn.exec_with_error_handler(\n+            |conn| {\n+                conn.exclusive_transaction(|conn| {\n+                    sql_query(std::include_str!(\"sql/create-vlobs-table.sql\")).execute(conn)?;\n+                    sql_query(std::include_str!(\"sql/create-realm-checkpoint-table.sql\"))\n+                        .execute(conn)?;\n+                    sql_query(std::include_str!(\n+                        \"sql/create-prevent-sync-pattern-table.sql\"\n+                    ))\n+                    .execute(conn)\n+                })\n+            },\n+            |e| {\n+                FSError::CreateTable(format!(\n+                    \"Failed to create tables required for ManifesStorage {e}\"\n+                ))\n+            },\n         )\n-        .execute(conn)\n-        .map_err(|e| FSError::CreateTable(format!(\"prevent_sync_pattern {e}\")))?;\n+        .await?;\n \n         let pattern = NewPreventSyncPattern {\n             _id: 0,\n             pattern: EMPTY_PATTERN,\n             fully_applied: false,\n         };\n         // Set the default \"prevent sync\" pattern if it doesn't exist\n-        diesel::insert_or_ignore_into(prevent_sync_pattern::table)\n-            .values(pattern)\n-            .execute(conn)\n-            .map_err(|e| FSError::InsertTable(format!(\"prevent_sync_pattern: create_db {e}\")))?;\n+        conn.exec(|conn| {\n+            conn.exclusive_transaction(|conn| {\n+                diesel::insert_or_ignore_into(prevent_sync_pattern::table)\n+                    .values(pattern)\n+                    .execute(conn)\n+            })\n+        })\n+        .await?;\n \n         Ok(())\n     }\n \n     #[cfg(test)]\n-    fn drop_db(&self) -> FSResult<()> {\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        sql_query(\"DROP TABLE IF EXISTS vlobs;\")\n-            .execute(conn)\n-            .map_err(|e| FSError::DropTable(format!(\"vlobs {e}\")))?;\n-\n-        sql_query(\"DROP TABLE IF EXISTS realm_checkpoints;\")\n-            .execute(conn)\n-            .map_err(|e| FSError::DropTable(format!(\"realm_checkpoints {e}\")))?;\n-\n-        sql_query(\"DROP TABLE IF EXISTS prevent_sync_pattern;\")\n-            .execute(conn)\n-            .map_err(|e| FSError::DropTable(format!(\"prevent_sync_pattern {e}\")))?;\n+    async fn drop_db(&self) -> FSResult<()> {\n+        self.conn\n+            .exec(|conn| {\n+                sql_query(\"DROP TABLE IF EXISTS vlobs;\").execute(conn)?;\n+                sql_query(\"DROP TABLE IF EXISTS realm_checkpoints;\").execute(conn)?;\n+                sql_query(\"DROP TABLE IF EXISTS prevent_sync_pattern;\").execute(conn)\n+            })\n+            .await?;\n \n         Ok(())\n     }\n \n-    pub fn clear_memory_cache(&self, flush: bool) -> FSResult<()> {\n-        if flush {\n-            self.flush_cache_ahead_of_persistance()?;\n+    // TODO: `clear_memory_cache` should always flush it's local cache into the sqlite one.\n+    // We use `flush=false` only during test.\n+    // Can rename to `flush_and_clear_memory_cache` & `drop_memory_cache`\n+    // - flush_and_clear_memory_cache: `and_clear` part only useful for tests, but doesn't\n+    //   hurt prod code (given it is only used during teardown)\n+    // - drop_memory_cache: only available with test-utils feature\n+    pub async fn clear_memory_cache(&self, flush: bool) -> FSResult<()> {\n+        let drained_items = {\n+            let mut lock = self.caches.lock().expect(\"Mutex is poisoned\");\n+            let drained = lock.drain();\n+\n+            if flush {\n+                Some(drained.collect::<Vec<(EntryID, Arc<Mutex<CacheEntry>>)>>())\n+            } else {\n+                None\n+            }\n+        };\n+        if let Some(drained_items) = drained_items {\n+            for (entry_id, lock) in drained_items {\n+                self.ensure_manifest_persistent_lock(entry_id, lock).await?;\n+            }\n         }\n-        self.cache_ahead_of_localdb\n-            .lock()\n-            .expect(\"Mutex is poisoned\")\n-            .clear();\n-        self.cache.lock().expect(\"Mutex is poisoned\").clear();\n-\n         Ok(())\n     }\n \n     // \"Prevent sync\" pattern operations\n \n-    pub fn get_prevent_sync_pattern(&self) -> FSResult<(Regex, bool)> {\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        let (re, fully_applied) = prevent_sync_pattern::table\n-            .select((\n-                prevent_sync_pattern::pattern,\n-                prevent_sync_pattern::fully_applied,\n-            ))\n-            .filter(prevent_sync_pattern::_id.eq(0))\n-            .first::<(String, _)>(conn)\n-            .map_err(|e| {\n-                FSError::QueryTable(format!(\n-                    \"prevent_sync_pattern: get_prevent_sync_pattern {e}\"\n-                ))\n-            })?;\n+    /// Prevent sync pattern is a global configuration, hence at each startup `WorkspaceStorage`\n+    /// has to make sure this configuration is consistent with what is stored in the DB (see\n+    /// `set_prevent_sync_pattern`). Long story short, we never need to retrieve the pattern\n+    /// from the DB when in production. However this is still convenient for testing ;-)\n+    #[cfg(test)]\n+    pub async fn get_prevent_sync_pattern(&self) -> FSResult<(Regex, bool)> {\n+        let (re, fully_applied) = get_prevent_sync_pattern_raw(&self.conn).await?;\n \n-        let re = Regex::new(&re).map_err(|e| {\n-            FSError::QueryTable(format!(\"prevent_sync_pattern: corrupted pattern {e}\"))\n+        let re = Regex::from_regex_str(&re).map_err(|e| {\n+            FSError::QueryTable(format!(\"prevent_sync_pattern: corrupted pattern: {e}\"))\n         })?;\n \n         Ok((re, fully_applied))\n@@ -233,67 +262,87 @@ impl ManifestStorage {\n     /// Set the \"prevent sync\" pattern for the corresponding workspace\n     /// This operation is idempotent,\n     /// i.e it does not reset the `fully_applied` flag if the pattern hasn't changed.\n-    pub fn set_prevent_sync_pattern(&self, pattern: &Regex) -> FSResult<()> {\n-        let pattern = pattern.as_str();\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        diesel::update(\n-            prevent_sync_pattern::table.filter(\n-                prevent_sync_pattern::_id\n-                    .eq(0)\n-                    .and(prevent_sync_pattern::pattern.ne(pattern)),\n-            ),\n-        )\n-        .set((\n-            prevent_sync_pattern::pattern.eq(pattern),\n-            prevent_sync_pattern::fully_applied.eq(false),\n-        ))\n-        .execute(conn)\n-        .map_err(|e| {\n-            FSError::UpdateTable(format!(\n-                \"prevent_sync_pattern: set_prevent_sync_pattern {e}\"\n-            ))\n-        })?;\n-\n-        Ok(())\n+    /// Return whether the pattern is fully applied\n+    pub async fn set_prevent_sync_pattern(&self, pattern: &Regex) -> FSResult<bool> {\n+        let pattern = pattern.to_string();\n+\n+        self.conn\n+            .exec_with_error_handler(\n+                move |conn| {\n+                    conn.exclusive_transaction(|conn| {\n+                        // TODO: use returning close to remove the need for `get_prevent_sync_pattern_raw` ?\n+                        diesel::update(\n+                            prevent_sync_pattern::table.filter(\n+                                prevent_sync_pattern::_id\n+                                    .eq(0)\n+                                    .and(prevent_sync_pattern::pattern.ne(&pattern)),\n+                            ),\n+                        )\n+                        .set((\n+                            prevent_sync_pattern::pattern.eq(&pattern),\n+                            prevent_sync_pattern::fully_applied.eq(false),\n+                        ))\n+                        .execute(conn)\n+                    })\n+                },\n+                |e| {\n+                    FSError::UpdateTable(format!(\n+                        \"prevent_sync_pattern: set_prevent_sync_pattern {e}\"\n+                    ))\n+                },\n+            )\n+            .and_then(|_| async { Ok(get_prevent_sync_pattern_raw(&self.conn).await?.1) })\n+            .await\n     }\n \n     /// Mark the provided pattern as fully applied.\n     /// This is meant to be called after one made sure that all the manifests in the\n     /// workspace are compliant with the new pattern. The applied pattern is provided\n     /// as an argument in order to avoid concurrency issues.\n-    pub fn mark_prevent_sync_pattern_fully_applied(&self, pattern: &Regex) -> FSResult<()> {\n-        let pattern = pattern.as_str();\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        diesel::update(\n-            prevent_sync_pattern::table.filter(\n-                prevent_sync_pattern::_id\n-                    .eq(0)\n-                    .and(prevent_sync_pattern::pattern.eq(pattern)),\n-            ),\n-        )\n-        .set(prevent_sync_pattern::fully_applied.eq(true))\n-        .execute(conn)\n-        .map_err(|e| {\n-            FSError::UpdateTable(format!(\n-                \"prevent_sync_pattern: mark_prevent_sync_pattern_fully_applied {e}\"\n-            ))\n-        })?;\n-\n-        Ok(())\n+    /// Return whether the pattern is fully applied\n+    pub async fn mark_prevent_sync_pattern_fully_applied(&self, pattern: &Regex) -> FSResult<bool> {\n+        let pattern = pattern.to_string();\n+\n+        self.conn\n+            .exec_with_error_handler(\n+                move |conn| {\n+                    conn.exclusive_transaction(|conn| {\n+                        diesel::update(\n+                            prevent_sync_pattern::table.filter(\n+                                prevent_sync_pattern::_id\n+                                    .eq(0)\n+                                    .and(prevent_sync_pattern::pattern.eq(pattern)),\n+                            ),\n+                        )\n+                        .set(prevent_sync_pattern::fully_applied.eq(true))\n+                        .execute(conn)\n+                    })\n+                },\n+                |e| {\n+                    FSError::UpdateTable(format!(\n+                        \"prevent_sync_pattern: mark_prevent_sync_pattern_fully_applied {e}\"\n+                    ))\n+                },\n+            )\n+            .and_then(|_| async { Ok(get_prevent_sync_pattern_raw(&self.conn).await?.1) })\n+            .await\n     }\n \n     // Checkpoint operations\n \n-    pub fn get_realm_checkpoint(&self) -> i64 {\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        realm_checkpoint::table\n-            .select(realm_checkpoint::checkpoint)\n-            .filter(realm_checkpoint::_id.eq(0))\n-            .first(conn)\n+    pub async fn get_realm_checkpoint(&self) -> i64 {\n+        self.conn\n+            .exec(|conn| {\n+                realm_checkpoint::table\n+                    .select(realm_checkpoint::checkpoint)\n+                    .filter(realm_checkpoint::_id.eq(0))\n+                    .first(conn)\n+            })\n+            .await\n             .unwrap_or(0)\n     }\n \n-    pub fn update_realm_checkpoint(\n+    pub async fn update_realm_checkpoint(\n         &self,\n         new_checkpoint: i64,\n         changed_vlobs: &[(EntryID, i64)],\n@@ -307,64 +356,86 @@ impl ManifestStorage {\n         // TODO: How to improve ?\n         // It is difficult to build a raw sql query with bind in a for loop\n         // Another solution is to query all data then insert\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        for (id, version) in changed_vlobs {\n-            sql_query(\"UPDATE vlobs SET remote_version = ? WHERE vlob_id = ?;\")\n-                .bind::<diesel::sql_types::BigInt, _>(version)\n-                .bind::<diesel::sql_types::Binary, _>((**id).as_ref())\n-                .execute(conn)\n-                .map_err(|e| FSError::UpdateTable(format!(\"vlobs: update_realm_checkpoint {e}\")))?;\n-        }\n \n-        diesel::insert_into(realm_checkpoint::table)\n-            .values(&new_realm_checkpoint)\n-            .on_conflict(realm_checkpoint::_id)\n-            .do_update()\n-            .set(&new_realm_checkpoint)\n-            .execute(conn)\n-            .map_err(|e| {\n-                FSError::InsertTable(format!(\"realm_checkpoint: update_realm_checkpoint {e}\"))\n-            })?;\n+        let changed_vlobs = changed_vlobs.to_vec();\n+        self.conn\n+            .exec_with_error_handler(\n+                move |conn| {\n+                    conn.immediate_transaction(|conn| {\n+                        // Update version on vlobs.\n+                        // TODO: Can update the vlobs in batch => `UPDATE vlobs SET remote_version = :version WHERE vlob_id in [changed_vlobs[].id];`\n+                        changed_vlobs\n+                            .into_iter()\n+                            .map(|(id, version)| {\n+                                let id = id.as_bytes();\n+                                diesel::update(\n+                                    vlobs::table.filter(vlobs::vlob_id.eq(id.as_slice())),\n+                                )\n+                                .set(vlobs::remote_version.eq(version))\n+                                .execute(conn)\n+                            })\n+                            .collect::<diesel::result::QueryResult<Vec<usize>>>()?;\n+                        // Update realm checkpoint value.\n+                        diesel::insert_into(realm_checkpoint::table)\n+                            .values(&new_realm_checkpoint)\n+                            .on_conflict(realm_checkpoint::_id)\n+                            .do_update()\n+                            .set(&new_realm_checkpoint)\n+                            .execute(conn)\n+                            .and(Ok(()))\n+                    })\n+                },\n+                |e| FSError::UpdateTable(format!(\"Failed to udpate vlobs table: {e}\")),\n+            )\n+            .await?;\n \n         Ok(())\n     }\n \n-    pub fn get_need_sync_entries(&self) -> FSResult<(HashSet<EntryID>, HashSet<EntryID>)> {\n+    pub async fn get_need_sync_entries(&self) -> FSResult<(HashSet<EntryID>, HashSet<EntryID>)> {\n         let mut remote_changes = HashSet::new();\n-        let mut local_changes = HashSet::<_, RandomState>::from_iter(\n-            self.cache\n+        let caches = self.caches.lock().expect(\"Mutex is poisoned\").clone();\n+        let iter = caches.iter().filter_map(|(id, cache)| {\n+            if cache\n                 .lock()\n                 .expect(\"Mutex is poisoned\")\n-                .iter()\n-                .filter_map(|(id, manifest)| {\n-                    if manifest.need_sync() {\n-                        Some(*id)\n-                    } else {\n-                        None\n-                    }\n-                }),\n-        );\n-\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        for (manifest_id, need_sync, bv, rv) in vlobs::table\n-            .select((\n-                vlobs::vlob_id,\n-                vlobs::need_sync,\n-                vlobs::base_version,\n-                vlobs::remote_version,\n-            ))\n-            .filter(\n-                vlobs::need_sync\n-                    .eq(true)\n-                    .or(vlobs::base_version.ne(vlobs::remote_version)),\n+                .manifest\n+                .as_ref()\n+                .map(|manifest| manifest.need_sync())\n+                .unwrap_or_default()\n+            {\n+                Some(*id)\n+            } else {\n+                None\n+            }\n+        });\n+        let mut local_changes = HashSet::<_, RandomState>::from_iter(iter);\n+\n+        let vlobs_that_need_sync = self\n+            .conn\n+            .exec_with_error_handler(\n+                |conn| {\n+                    vlobs::table\n+                        .select((\n+                            vlobs::vlob_id,\n+                            vlobs::need_sync,\n+                            vlobs::base_version,\n+                            vlobs::remote_version,\n+                        ))\n+                        .filter(\n+                            vlobs::need_sync\n+                                .eq(true)\n+                                .or(vlobs::base_version.ne(vlobs::remote_version)),\n+                        )\n+                        .load::<(Vec<u8>, bool, i64, i64)>(conn)\n+                },\n+                |e| FSError::QueryTable(format!(\"vlobs: get_need_sync_entries {e}\")),\n             )\n-            .load::<(Vec<u8>, bool, i64, i64)>(conn)\n-            .map_err(|e| FSError::QueryTable(format!(\"vlobs: get_need_sync_entries {e}\")))?\n-        {\n-            let manifest_id =\n-                EntryID::from(<[u8; 16]>::try_from(&manifest_id[..]).map_err(|e| {\n-                    FSError::QueryTable(format!(\"vlobs: corrupted manifest_id {e}\"))\n-                })?);\n+            .await?;\n+\n+        for (manifest_id, need_sync, bv, rv) in vlobs_that_need_sync {\n+            let manifest_id = EntryID::try_from(manifest_id.as_slice())\n+                .map_err(|e| FSError::QueryTable(format!(\"vlobs: corrupted manifest_id {e}\")))?;\n \n             if need_sync {\n                 local_changes.insert(manifest_id);\n@@ -378,252 +449,391 @@ impl ManifestStorage {\n         Ok((local_changes, remote_changes))\n     }\n \n+    /// Get a cache entry identified by `entry_id`.\n+    /// If the cache is not present it will insert it then return it (see `or_default`).\n+    fn get_or_insert_default_cache_entry(&self, entry_id: EntryID) -> Arc<Mutex<CacheEntry>> {\n+        self.caches\n+            .lock()\n+            .expect(\"Mutex is poisoned\")\n+            .entry(entry_id)\n+            .or_default()\n+            .clone()\n+    }\n     // Manifest operations\n \n-    pub fn get_manifest(&self, entry_id: EntryID) -> FSResult<LocalManifest> {\n+    pub fn get_manifest_in_cache(&self, entry_id: &EntryID) -> Option<LocalManifest> {\n+        let caches = self.caches.lock().expect(\"Mutex is poisoned\");\n+\n+        caches\n+            .get(entry_id)\n+            .and_then(|entry| entry.lock().expect(\"Mutex is poisoned\").manifest.clone())\n+    }\n+\n+    pub async fn get_manifest(&self, entry_id: EntryID) -> FSResult<LocalManifest> {\n         // Look in cache first\n-        if let Some(manifest) = self.cache.lock().expect(\"Mutex is poisoned\").get(&entry_id) {\n-            return Ok(manifest.clone());\n+        let cache_entry = self.get_or_insert_default_cache_entry(entry_id);\n+        if let Some(manifest) = {\n+            cache_entry\n+                .lock()\n+                .expect(\"Mutex is poisoned\")\n+                .manifest\n+                .clone()\n+        } {\n+            return Ok(manifest);\n         }\n \n         // Look into the database\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        let manifest = vlobs::table\n-            .select(vlobs::blob)\n-            .filter(vlobs::vlob_id.eq((*entry_id).as_ref()))\n-            .first::<Vec<u8>>(conn)\n+        let manifest = self\n+            .conn\n+            .exec(move |conn| {\n+                vlobs::table\n+                    .select(vlobs::blob)\n+                    .filter(vlobs::vlob_id.eq((*entry_id).as_ref()))\n+                    .first::<Vec<u8>>(conn)\n+            })\n+            .await\n             .map_err(|_| FSError::LocalMiss(*entry_id))?;\n \n         let manifest = LocalManifest::decrypt_and_load(&manifest, &self.local_symkey)\n             .map_err(|_| FSError::Crypto(CryptoError::Decryption))?;\n \n-        // Fill the cache\n-        self.cache\n-            .lock()\n-            .expect(\"Mutex is poisoned\")\n-            .insert(entry_id, manifest.clone());\n+        cache_entry.lock().expect(\"Mutex is poisoned\").manifest = Some(manifest.clone());\n \n         Ok(manifest)\n     }\n \n-    pub fn set_manifest(\n+    pub fn set_manifest_cache_only(\n         &self,\n         entry_id: EntryID,\n         manifest: LocalManifest,\n-        cache_only: bool,\n         removed_ids: Option<HashSet<ChunkOrBlockID>>,\n-    ) -> FSResult<()> {\n-        // Set the cache first\n-        self.cache\n-            .lock()\n-            .expect(\"Mutex is poisoned\")\n-            .insert(entry_id, manifest);\n-        // Tag the entry as ahead of localdb\n-        if self\n-            .cache_ahead_of_localdb\n-            .lock()\n-            .expect(\"Mutex is poisoned\")\n-            .get(&entry_id)\n-            .is_none()\n-        {\n-            self.cache_ahead_of_localdb\n-                .lock()\n-                .expect(\"Mutex is poisoned\")\n-                .insert(entry_id, HashSet::new());\n-        }\n+    ) {\n+        let cache_entry = self.get_or_insert_default_cache_entry(entry_id);\n \n-        // Cleanup\n-        if let Some(removed_ids) = &removed_ids {\n-            let value = self\n-                .cache_ahead_of_localdb\n-                .lock()\n-                .expect(\"Mutex is poisoned\")\n-                .get(&entry_id)\n-                .unwrap()\n-                | removed_ids;\n+        let mut cache_unlock = cache_entry.lock().expect(\"Mutex is poisoned\");\n \n-            self.cache_ahead_of_localdb\n-                .lock()\n-                .expect(\"Mutex is poisoned\")\n-                .insert(entry_id, value);\n+        cache_unlock.manifest = Some(manifest);\n+        let pending_chunk_ids = cache_unlock\n+            .pending_chunk_ids\n+            .get_or_insert(HashSet::default());\n+        if let Some(removed_ids) = &removed_ids {\n+            *pending_chunk_ids = (pending_chunk_ids as &HashSet<ChunkOrBlockID>) | removed_ids;\n         }\n+    }\n+\n+    pub async fn set_manifest(\n+        &self,\n+        entry_id: EntryID,\n+        manifest: LocalManifest,\n+        cache_only: bool,\n+        removed_ids: Option<HashSet<ChunkOrBlockID>>,\n+    ) -> FSResult<()> {\n+        self.set_manifest_cache_only(entry_id, manifest, removed_ids);\n \n         // Flush the cached value to the localdb\n         if !cache_only {\n-            self.ensure_manifest_persistent(entry_id)?;\n+            self.ensure_manifest_persistent(entry_id).await?;\n         }\n+\n         Ok(())\n     }\n \n-    pub fn ensure_manifest_persistent(&self, entry_id: EntryID) -> FSResult<()> {\n-        match (\n-            self.cache_ahead_of_localdb\n-                .lock()\n-                .expect(\"Mutex is poisoned\")\n-                .get(&entry_id),\n-            self.cache.lock().expect(\"Mutex is poisoned\").get(&entry_id),\n-        ) {\n-            (Some(pending_chunk_ids), Some(manifest)) => {\n-                let ciphered = manifest.dump_and_encrypt(&self.local_symkey);\n-                let pending_chunk_ids = pending_chunk_ids\n-                    .iter()\n-                    .map(|chunk_id| chunk_id.as_ref())\n-                    .collect::<Vec<_>>();\n-\n-                let vlob_id = (*entry_id).as_ref();\n-\n-                let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-                sql_query(\"INSERT OR REPLACE INTO vlobs (vlob_id, blob, need_sync, base_version, remote_version)\n-                    VALUES (\n-                    ?, ?, ?, ?,\n-                        max(\n-                            ?,\n-                            IFNULL((SELECT remote_version FROM vlobs WHERE vlob_id=?), 0)\n-                        )\n-                    )\")\n-                    .bind::<diesel::sql_types::Binary, _>(vlob_id)\n-                    .bind::<diesel::sql_types::Binary, _>(ciphered)\n-                    .bind::<diesel::sql_types::Bool, _>(manifest.need_sync())\n-                    .bind::<diesel::sql_types::BigInt, _>(manifest.base_version() as i64)\n-                    .bind::<diesel::sql_types::BigInt, _>(manifest.base_version() as i64)\n-                    .bind::<diesel::sql_types::Binary, _>(vlob_id)\n-                    .execute(conn)\n-                    .map_err(|e| FSError::InsertTable(format!(\"vlobs: ensure_manifest_persistent {e}\")))?;\n+    pub async fn ensure_manifest_persistent(&self, entry_id: EntryID) -> FSResult<()> {\n+        let cache_entry = {\n+            let cache_guard = self.caches.lock().expect(\"Mutex is poisoned\");\n \n-                for pending_chunk_ids_chunk in pending_chunk_ids.chunks(SQLITE_MAX_VARIABLE_NUMBER)\n-                {\n-                    diesel::delete(\n-                        chunks::table.filter(chunks::chunk_id.eq_any(pending_chunk_ids_chunk)),\n-                    )\n-                    .execute(conn)\n-                    .map_err(|e| FSError::DeleteTable(format!(\"chunks: clear_manifest {e}\")))?;\n-                }\n+            cache_guard.get(&entry_id).cloned()\n+        };\n \n-                Ok(())\n-            }\n-            _ => Ok(()),\n+        if let Some(cache_entry) = cache_entry {\n+            self.ensure_manifest_persistent_lock(entry_id, cache_entry)\n+                .await\n+        } else {\n+            Ok(())\n         }\n     }\n \n-    pub fn flush_cache_ahead_of_persistance(&self) -> FSResult<()> {\n-        let keys = self\n-            .cache_ahead_of_localdb\n-            .lock()\n-            .expect(\"Mutex is poisoned\")\n-            .keys()\n-            .copied()\n-            .collect::<Vec<_>>();\n-        for entry_id in keys {\n-            self.ensure_manifest_persistent(entry_id)?;\n-        }\n+    async fn ensure_manifest_persistent_lock(\n+        &self,\n+        entry_id: EntryID,\n+        cache_lock: Arc<Mutex<CacheEntry>>,\n+    ) -> FSResult<()> {\n+        // We retrieve some data that are required for the next step.\n+        let stuff = {\n+            let cache = cache_lock.lock().unwrap();\n+\n+            cache\n+                .manifest\n+                .as_ref()\n+                .and_then(|manifest| {\n+                    cache\n+                        .pending_chunk_ids\n+                        .clone()\n+                        .map(|chunk_ids| (manifest, chunk_ids))\n+                })\n+                .map(|(manifest, chunk_ids)| {\n+                    (\n+                        manifest.dump_and_encrypt(&self.local_symkey),\n+                        manifest.need_sync(),\n+                        manifest.base_version() as i64,\n+                        Vec::from_iter(chunk_ids.into_iter()),\n+                    )\n+                })\n+        };\n \n+        if let Some((ciphered, need_sync, base_version, chunk_ids)) = stuff {\n+            self.conn.exec(move |conn| {\n+                conn.exclusive_transaction(|conn| {\n+                    let vlob_id = (*entry_id).as_ref();\n+\n+                    // TODO: replace with [replace_into](https://docs.diesel.rs/2.0.x/diesel/fn.replace_into.html)\n+                    sql_query(\"INSERT OR REPLACE INTO vlobs (vlob_id, blob, need_sync, base_version, remote_version)\n+                            VALUES (\n+                            ?, ?, ?, ?,\n+                                max(\n+                                    ?,\n+                                    IFNULL((SELECT remote_version FROM vlobs WHERE vlob_id=?), 0)\n+                                )\n+                            )\")\n+                            .bind::<diesel::sql_types::Binary, _>(vlob_id)\n+                            .bind::<diesel::sql_types::Binary, _>(ciphered)\n+                            .bind::<diesel::sql_types::Bool, _>(need_sync)\n+                            .bind::<diesel::sql_types::BigInt, _>(base_version)\n+                            .bind::<diesel::sql_types::BigInt, _>(base_version)\n+                            .bind::<diesel::sql_types::Binary, _>(vlob_id)\n+                            .execute(conn)?;\n+                    chunk_ids\n+                        .chunks(LOCAL_DATABASE_MAX_VARIABLE_NUMBER)\n+                        .try_for_each(|chunked| {\n+                            let ids = chunked.iter().map(ChunkOrBlockID::as_ref);\n+                            let query = chunks::table.filter(chunks::chunk_id.eq_any(ids));\n+                            diesel::delete(query).execute(conn).and(Ok(()))\n+                        })\n+                })\n+            }).await?;\n+\n+            cache_lock\n+                .lock()\n+                .expect(\"Mutex is poisoned\")\n+                .pending_chunk_ids = None;\n+        }\n         Ok(())\n     }\n \n-    // This method is not used in the code base but it is still tested\n-    // as it might come handy in a cleanup routine later\n-\n-    #[deprecated]\n-    pub fn clear_manifest(&self, entry_id: EntryID) -> FSResult<()> {\n+    #[cfg(any(test, feature = \"test-utils\"))]\n+    pub async fn clear_manifest(&self, entry_id: EntryID) -> FSResult<()> {\n         // Remove from cache\n-        let in_cache = self\n-            .cache\n-            .lock()\n-            .expect(\"Mutex is poisoned\")\n-            .remove(&entry_id)\n-            .is_some();\n-\n-        // Remove from local database\n-        let conn = &mut *self.conn.lock().expect(\"Mutex is poisoned\");\n-        let deleted = diesel::delete(vlobs::table.filter(vlobs::vlob_id.eq((*entry_id).as_ref())))\n-            .execute(conn)\n-            .map_err(|e| FSError::DeleteTable(format!(\"vlobs: clear_manifest {e}\")))?\n-            > 0;\n-\n-        if let Some(pending_chunk_ids) = self\n-            .cache_ahead_of_localdb\n-            .lock()\n-            .expect(\"Mutex is poisoned\")\n-            .remove(&entry_id)\n-        {\n-            let pending_chunk_ids = pending_chunk_ids\n-                .iter()\n-                .map(ChunkOrBlockID::as_ref)\n-                .collect::<Vec<_>>();\n-\n-            for pending_chunk_ids_chunk in pending_chunk_ids.chunks(SQLITE_MAX_VARIABLE_NUMBER) {\n-                diesel::delete(\n-                    chunks::table.filter(chunks::chunk_id.eq_any(pending_chunk_ids_chunk)),\n-                )\n-                .execute(conn)\n-                .map_err(|e| FSError::DeleteTable(format!(\"chunks: clear_manifest {e}\")))?;\n-            }\n-        }\n+        let cache_entry = self.get_or_insert_default_cache_entry(entry_id);\n+        let (previous_manifest, previous_pending_chunk_ids) = {\n+            let mut cache_unlock = cache_entry.lock().expect(\"Mutex is poisoned\");\n+\n+            (\n+                cache_unlock.manifest.take(),\n+                cache_unlock.pending_chunk_ids.take(),\n+            )\n+        };\n+\n+        let res = self\n+            .conn\n+            .exec(move |conn| {\n+                conn.exclusive_transaction(|conn| {\n+                    let vlob_id = entry_id.as_bytes().as_slice();\n+                    let deleted = diesel::delete(vlobs::table.filter(vlobs::vlob_id.eq(vlob_id)))\n+                        .execute(conn)?\n+                        > 0;\n+\n+                    if let Some(pending_chunk_ids) = previous_pending_chunk_ids {\n+                        let pending_chunk_ids = pending_chunk_ids.into_iter().collect::<Vec<_>>();\n+\n+                        pending_chunk_ids\n+                            .chunks(LOCAL_DATABASE_MAX_VARIABLE_NUMBER)\n+                            .try_for_each(|chunked| {\n+                                let ids = chunked.iter().map(ChunkOrBlockID::as_ref);\n+                                let query = chunks::table.filter(chunks::chunk_id.eq_any(ids));\n+                                diesel::delete(query).execute(conn).and(Ok(()))\n+                            })?;\n+                    }\n \n-        if !deleted && !in_cache {\n+                    Ok(deleted)\n+                })\n+            })\n+            .await;\n+        // Something unexpected happened with the database, given we have modified the cache it is\n+        // safer to panic instead of returning an error that could be seen as recoverable from the\n+        // caller point of view\n+        let deleted =\n+            res.expect(\"Failed to remove the manifest & its associated data in the local database\");\n+\n+        let is_manifest_cached = previous_manifest.is_some();\n+\n+        if !deleted && !is_manifest_cached {\n             return Err(FSError::LocalMiss(*entry_id));\n         }\n-\n         Ok(())\n     }\n+\n+    /// Return `true` is the given manifest identified by `entry_id` is present in the cache\n+    /// waiting to be written onto the database.\n+    ///\n+    /// For more information see [ManifestStorage::cache_ahead_of_localdb].\n+    pub fn is_manifest_cache_ahead_of_persistance(&self, entry_id: &EntryID) -> bool {\n+        let cache_entry = {\n+            let caches_guard = self.caches.lock().expect(\"Mutex is poisoned\");\n+            caches_guard.get(entry_id).cloned()\n+        };\n+        cache_entry\n+            .as_ref()\n+            .map(|entry| {\n+                entry\n+                    .lock()\n+                    .expect(\"Mutex is poisoned\")\n+                    .has_chunk_to_be_flush()\n+            })\n+            .unwrap_or(false)\n+    }\n+}\n+\n+async fn get_prevent_sync_pattern_raw(\n+    connection: &LocalDatabase,\n+) -> Result<(String, bool), FSError> {\n+    connection\n+        .exec_with_error_handler(\n+            |conn| {\n+                prevent_sync_pattern::table\n+                    .select((\n+                        prevent_sync_pattern::pattern,\n+                        prevent_sync_pattern::fully_applied,\n+                    ))\n+                    .filter(prevent_sync_pattern::_id.eq(0))\n+                    .first::<(String, _)>(conn)\n+            },\n+            |e| {\n+                FSError::QueryTable(format!(\n+                    \"prevent_sync_pattern: get_prevent_sync_pattern {e}\"\n+                ))\n+            },\n+        )\n+        .await\n }\n \n #[cfg(test)]\n mod tests {\n+    use std::{ops::Deref, sync::Arc};\n+\n     use libparsec_client_types::{Chunk, LocalFileManifest};\n     use libparsec_crypto::{prelude::*, HashDigest};\n-    use libparsec_types::{BlockAccess, Blocksize, DateTime, DeviceID, FileManifest};\n+    use libparsec_platform_local_db::VacuumMode;\n+    use libparsec_types::{BlockAccess, Blocksize, DateTime, DeviceID, FileManifest, Regex};\n \n     use libparsec_tests_fixtures::{timestamp, tmp_path, TmpPath};\n-    use rstest::rstest;\n+    use rstest::{fixture, rstest};\n \n     use super::*;\n-    use crate::storage::local_database::SqlitePool;\n \n-    #[rstest]\n-    fn manifest_storage(tmp_path: TmpPath, timestamp: DateTime) {\n-        let t1 = timestamp;\n-        let t2 = t1.add_us(1);\n+    struct ManifestStorageFixture {\n+        storage: ManifestStorage,\n+        _tmp_path: TmpPath,\n+    }\n+\n+    impl Deref for ManifestStorageFixture {\n+        type Target = ManifestStorage;\n+\n+        fn deref(&self) -> &Self::Target {\n+            &self.storage\n+        }\n+    }\n+\n+    #[fixture]\n+    async fn manifest_storage(tmp_path: TmpPath) -> ManifestStorageFixture {\n         let db_path = tmp_path.join(\"manifest_storage.sqlite\");\n-        let pool = SqlitePool::new(db_path.to_str().unwrap()).unwrap();\n-        let conn = Mutex::new(pool.conn().unwrap());\n+        let conn = LocalDatabase::from_path(db_path.to_str().unwrap(), VacuumMode::default())\n+            .await\n+            .unwrap();\n+        let conn = Arc::new(conn);\n         let local_symkey = SecretKey::generate();\n         let realm_id = EntryID::default();\n \n-        let manifest_storage = ManifestStorage::new(local_symkey, conn, realm_id).unwrap();\n-        manifest_storage.drop_db().unwrap();\n-        manifest_storage.create_db().unwrap();\n+        let manifest_storage = ManifestStorage::new(local_symkey, realm_id, conn)\n+            .await\n+            .unwrap();\n+        manifest_storage.drop_db().await.unwrap();\n+        manifest_storage.create_db().await.unwrap();\n+\n+        ManifestStorageFixture {\n+            storage: manifest_storage,\n+            _tmp_path: tmp_path,\n+        }\n+    }\n+\n+    #[rstest]\n+    #[tokio::test]\n+    async fn prevent_sync_pattern(#[future] manifest_storage: ManifestStorageFixture) {\n+        let manifest_storage = manifest_storage.await;\n \n-        let (re, fully_applied) = manifest_storage.get_prevent_sync_pattern().unwrap();\n+        let (re, fully_applied) = manifest_storage.get_prevent_sync_pattern().await.unwrap();\n \n-        assert_eq!(re.as_str(), EMPTY_PATTERN);\n+        assert_eq!(re.to_string(), EMPTY_PATTERN);\n         assert!(!fully_applied);\n \n+        let regex = Regex::from_regex_str(r\"\\z\").unwrap();\n         manifest_storage\n-            .set_prevent_sync_pattern(&Regex::new(r\"\\z\").unwrap())\n+            .set_prevent_sync_pattern(&regex)\n+            .await\n             .unwrap();\n \n-        let (re, fully_applied) = manifest_storage.get_prevent_sync_pattern().unwrap();\n+        let (re, fully_applied) = manifest_storage.get_prevent_sync_pattern().await.unwrap();\n \n-        assert_eq!(re.as_str(), r\"\\z\");\n+        assert_eq!(re.to_string(), r\"\\z\");\n         assert!(!fully_applied);\n \n+        // Passing fully applied on a random pattern is a noop...\n+\n         manifest_storage\n-            .mark_prevent_sync_pattern_fully_applied(&Regex::new(EMPTY_PATTERN).unwrap())\n+            .mark_prevent_sync_pattern_fully_applied(&Regex::from_regex_str(EMPTY_PATTERN).unwrap())\n+            .await\n             .unwrap();\n \n-        let (re, fully_applied) = manifest_storage.get_prevent_sync_pattern().unwrap();\n+        let (re, fully_applied) = manifest_storage.get_prevent_sync_pattern().await.unwrap();\n \n-        assert_eq!(re.as_str(), r\"\\z\");\n+        assert_eq!(re.to_string(), r\"\\z\");\n         assert!(!fully_applied);\n \n+        // ...unlike passing fully applied on the currently registered pattern\n+\n+        manifest_storage\n+            .mark_prevent_sync_pattern_fully_applied(&regex)\n+            .await\n+            .unwrap();\n+\n+        let (re, fully_applied) = manifest_storage.get_prevent_sync_pattern().await.unwrap();\n+\n+        assert_eq!(re.to_string(), r\"\\z\");\n+        assert!(fully_applied);\n+    }\n+\n+    #[rstest]\n+    #[tokio::test]\n+    async fn realm_checkpoint(#[future] manifest_storage: ManifestStorageFixture) {\n+        let manifest_storage = manifest_storage.await;\n+\n         let entry_id = EntryID::default();\n \n         manifest_storage\n             .update_realm_checkpoint(64, &[(entry_id, 2)])\n+            .await\n             .unwrap();\n \n-        assert_eq!(manifest_storage.get_realm_checkpoint(), 64);\n+        assert_eq!(manifest_storage.get_realm_checkpoint().await, 64);\n+    }\n+\n+    #[rstest]\n+    #[tokio::test]\n+    async fn set_manifest(#[future] manifest_storage: ManifestStorageFixture, timestamp: DateTime) {\n+        let manifest_storage = manifest_storage.await;\n+        let t1 = timestamp;\n+        let t2 = t1.add_us(1);\n+\n+        let entry_id = EntryID::default();\n \n         let local_file_manifest = LocalManifest::File(LocalFileManifest {\n             base: FileManifest {\n@@ -658,16 +868,24 @@ mod tests {\n             }]],\n         });\n \n+        assert!(manifest_storage.get_manifest_in_cache(&entry_id).is_none());\n+        assert_eq!(\n+            manifest_storage.get_manifest(entry_id).await,\n+            Err(FSError::LocalMiss(*entry_id))\n+        );\n+\n         manifest_storage\n             .set_manifest(entry_id, local_file_manifest.clone(), false, None)\n+            .await\n             .unwrap();\n \n         assert_eq!(\n-            manifest_storage.get_manifest(entry_id).unwrap(),\n+            manifest_storage.get_manifest(entry_id).await.unwrap(),\n             local_file_manifest\n         );\n \n-        let (local_changes, remote_changes) = manifest_storage.get_need_sync_entries().unwrap();\n+        let (local_changes, remote_changes) =\n+            manifest_storage.get_need_sync_entries().await.unwrap();\n \n         assert_eq!(local_changes, HashSet::new());\n         assert_eq!(remote_changes, HashSet::new());"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/mod.rs",
      "status": "modified",
      "patch": "@@ -1,14 +1,13 @@\n // Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n \n mod chunk_storage;\n-mod local_database;\n mod manifest_storage;\n mod sql_types;\n-// Not used for the moment\n-#[allow(dead_code)]\n mod user_storage;\n mod version;\n mod workspace_storage;\n \n+pub use chunk_storage::Remanence;\n pub use manifest_storage::ChunkOrBlockID;\n+pub use user_storage::{user_storage_non_speculative_init, UserStorage};\n pub use workspace_storage::*;"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/sql/create-chunks-table.sql",
      "status": "added",
      "patch": "@@ -0,0 +1,9 @@\n+-- Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+\n+CREATE TABLE IF NOT EXISTS chunks (\n+    chunk_id BLOB PRIMARY KEY NOT NULL, -- UUID\n+    size INTEGER NOT NULL,\n+    offline BOOLEAN NOT NULL,\n+    accessed_on REAL, -- Timestamp\n+    data BLOB NOT NULL\n+)"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/sql/create-prevent-sync-pattern-table.sql",
      "status": "added",
      "patch": "@@ -0,0 +1,7 @@\n+-- Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+\n+CREATE TABLE IF NOT EXISTS prevent_sync_pattern (\n+    _id INTEGER PRIMARY KEY NOT NULL,\n+    pattern TEXT NOT NULL,\n+    fully_applied BOOLEAN NOT NULL\n+);"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/sql/create-realm-checkpoint-table.sql",
      "status": "added",
      "patch": "@@ -0,0 +1,6 @@\n+-- Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+\n+CREATE TABLE IF NOT EXISTS realm_checkpoint (\n+    _id INTEGER PRIMARY KEY NOT NULL,\n+    checkpoint INTEGER NOT NULL\n+);"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/sql/create-remanence-table.sql",
      "status": "added",
      "patch": "@@ -0,0 +1,6 @@\n+-- Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+\n+CREATE TABLE IF NOT EXISTS remanence (\n+    _id INTEGER PRIMARY KEY NOT NULL,\n+    block_remanent BOOL NOT NULL\n+)"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/sql/create-temp-unreferenced-chunks-table.sql",
      "status": "added",
      "patch": "@@ -0,0 +1,5 @@\n+-- Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+\n+CREATE TEMP TABLE unreferenced_chunks(\n+    chunk_id BLOB PRIMARY KEY -- UUID\n+);"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/sql/create-vlobs-table.sql",
      "status": "added",
      "patch": "@@ -0,0 +1,9 @@\n+-- Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+\n+CREATE TABLE IF NOT EXISTS vlobs (\n+    vlob_id BLOB PRIMARY KEY NOT NULL, -- UUID\n+    base_version INTEGER NOT NULL,\n+    remote_version INTEGER NOT NULL,\n+    need_sync BOOLEAN NOT NULL,\n+    blob BLOB NOT NULL\n+);"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/sql_types.rs",
      "status": "modified",
      "patch": "@@ -1,13 +1,13 @@\n // Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n \n use diesel::{\n-    deserialize::FromSql,\n+    deserialize::{FromSql, FromSqlRow},\n+    expression::AsExpression,\n     serialize::{Output, ToSql},\n     sqlite::{Sqlite, SqliteValue},\n-    AsExpression, FromSqlRow,\n };\n \n-#[derive(Debug, AsExpression, FromSqlRow)]\n+#[derive(Debug, AsExpression, FromSqlRow, Clone, Copy)]\n #[diesel(sql_type = diesel::sql_types::Double)]\n pub struct DateTime(pub f64);\n "
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/user_storage.rs",
      "status": "modified",
      "patch": "@@ -1,128 +1,222 @@\n // Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n \n-use std::collections::HashSet;\n+use std::{\n+    collections::HashSet,\n+    path::Path,\n+    sync::{Arc, Mutex},\n+};\n \n use libparsec_client_types::{LocalDevice, LocalManifest, LocalUserManifest};\n+use libparsec_platform_local_db::{LocalDatabase, VacuumMode};\n use libparsec_types::EntryID;\n \n-use super::manifest_storage::ManifestStorage;\n-use crate::error::{FSError, FSResult};\n+use super::{manifest_storage::ManifestStorage, version::get_user_data_storage_db_path};\n+use crate::error::FSResult;\n+use libparsec_platform_async::Mutex as AsyncMutex;\n \n pub struct UserStorage {\n-    device: LocalDevice,\n-    user_manifest_id: EntryID,\n+    pub device: LocalDevice,\n+    pub user_manifest_id: EntryID,\n     manifest_storage: ManifestStorage,\n+    /// A lock that will be used to prevent concurrent update in [UserStorage::set_user_manifest].\n+    /// When updating the user manifest.\n+    lock_update_manifest: AsyncMutex<()>,\n+    /// Keep a copy of the user manifest to have it available at all time.\n+    /// (We don't rely on [ManifestStorage]'s cache since it can be cleared).\n+    user_manifest_copy: Mutex<LocalUserManifest>,\n }\n \n impl UserStorage {\n-    pub fn new(\n+    pub async fn from_db_dir(\n         device: LocalDevice,\n         user_manifest_id: EntryID,\n-        manifest_storage: ManifestStorage,\n-    ) -> Self {\n-        Self {\n+        data_base_dir: &Path,\n+    ) -> FSResult<Self> {\n+        let data_path = get_user_data_storage_db_path(data_base_dir, &device);\n+        let conn = LocalDatabase::from_path(\n+            data_path\n+                .to_str()\n+                .expect(\"Non-Utf-8 character found in data_path\"),\n+            VacuumMode::default(),\n+        )\n+        .await?;\n+        let conn = Arc::new(conn);\n+        let manifest_storage =\n+            ManifestStorage::new(device.local_symkey.clone(), user_manifest_id, conn).await?;\n+        let user_manifest =\n+            UserStorage::load_user_manifest(&manifest_storage, user_manifest_id, &device).await?;\n+        let user_storage = Self {\n             device,\n             user_manifest_id,\n             manifest_storage,\n-        }\n+            lock_update_manifest: AsyncMutex::new(()),\n+            user_manifest_copy: Mutex::new(user_manifest),\n+        };\n+        Ok(user_storage)\n+    }\n+\n+    /// Close the connections to the databases.\n+    /// Provide a way to manually close those connections.\n+    /// In theory this is not needed given we always ask the manifest storage\n+    /// to flush manifests on disk (i.e. we never rely on cache-ahead-of-db feature).\n+    /// So it should be a noop compared to database close without cache flush that\n+    /// is done when [UserStorage] is dropped.\n+    pub fn close_connections(&self) {\n+        self.manifest_storage.close_connection()\n     }\n \n     // Checkpoint Interface\n \n-    pub fn get_realm_checkpoint(&self) -> i64 {\n-        self.manifest_storage.get_realm_checkpoint()\n+    pub async fn get_realm_checkpoint(&self) -> i64 {\n+        self.manifest_storage.get_realm_checkpoint().await\n     }\n \n-    pub fn update_realm_checkpoint(\n+    pub async fn update_realm_checkpoint(\n         &self,\n         new_checkpoint: i64,\n         changed_vlobs: &[(EntryID, i64)],\n     ) -> FSResult<()> {\n         self.manifest_storage\n             .update_realm_checkpoint(new_checkpoint, changed_vlobs)\n+            .await\n     }\n \n-    pub fn get_need_sync_entries(&self) -> FSResult<(HashSet<EntryID>, HashSet<EntryID>)> {\n-        self.manifest_storage.get_need_sync_entries()\n+    pub async fn get_need_sync_entries(&self) -> FSResult<(HashSet<EntryID>, HashSet<EntryID>)> {\n+        self.manifest_storage.get_need_sync_entries().await\n     }\n \n     // User manifest\n \n-    pub fn get_user_manifest(&self) -> FSResult<LocalUserManifest> {\n-        let cache = self\n-            .manifest_storage\n-            .cache\n+    pub fn get_user_manifest(&self) -> LocalUserManifest {\n+        self.user_manifest_copy\n             .lock()\n-            .expect(\"Mutex is poisoned\");\n-        match cache.get(&self.user_manifest_id) {\n-            Some(LocalManifest::User(manifest)) => Ok(manifest.clone()),\n-            _ => Err(FSError::UserManifestMissing),\n+            .expect(\"Mutex is poisoned\")\n+            .clone()\n+    }\n+\n+    async fn load_user_manifest(\n+        manifest_storage: &ManifestStorage,\n+        user_manifest_id: EntryID,\n+        device: &LocalDevice,\n+    ) -> FSResult<LocalUserManifest> {\n+        match manifest_storage.get_manifest(user_manifest_id).await {\n+            Ok(LocalManifest::User(manifest)) => Ok(manifest),\n+            Ok(_) => panic!(\"User manifest id is used for something other than a user manifest\"),\n+            // It is possible to lack the user manifest in local if our\n+            // device hasn't tried to access it yet (and we are not the\n+            // initial device of our user, in which case the user local db is\n+            // initialized with a non-speculative local manifest placeholder).\n+            // In such case it is easy to fall back on an empty manifest\n+            // which is a good enough approximation of the very first version\n+            // of the manifest (field `created` is invalid, but it will be\n+            // correction by the merge during sync).\n+            Err(_) => {\n+                let timestamp = device.now();\n+                let manifest = LocalUserManifest::new(\n+                    device.device_id.clone(),\n+                    timestamp,\n+                    Some(device.user_manifest_id),\n+                    true,\n+                );\n+                manifest_storage\n+                    .set_manifest(\n+                        user_manifest_id,\n+                        LocalManifest::User(manifest.clone()),\n+                        false,\n+                        None,\n+                    )\n+                    .await?;\n+                Ok(manifest)\n+            }\n         }\n     }\n \n-    fn load_user_manifest(&self) -> FSResult<()> {\n-        if self\n-            .manifest_storage\n-            .get_manifest(self.user_manifest_id)\n-            .is_err()\n-        {\n-            let timestamp = self.device.now();\n-            let manifest = LocalUserManifest::new(\n-                self.device.device_id.clone(),\n-                timestamp,\n-                Some(self.device.user_manifest_id),\n-                true,\n-            );\n-            self.manifest_storage.set_manifest(\n+    pub async fn set_user_manifest(&self, user_manifest: LocalUserManifest) -> FSResult<()> {\n+        assert_eq!(\n+            self.user_manifest_id, user_manifest.base.id,\n+            \"UserManifest should have the same EntryID as registered in UserStorage\"\n+        );\n+\n+        // We must make sure `manifest_storage` and `user_manifest_copy` are modified\n+        // atomically (given the copy is a basically a convenient shortcut on `manifest_storage`).\n+        let update_guard = self.lock_update_manifest.lock().await;\n+\n+        self.manifest_storage\n+            .set_manifest(\n                 self.user_manifest_id,\n-                LocalManifest::User(manifest),\n+                LocalManifest::User(user_manifest.clone()),\n                 false,\n                 None,\n-            )?;\n-        }\n+            )\n+            .await?;\n+        *self.user_manifest_copy.lock().expect(\"Mutex is poisoned\") = user_manifest;\n+\n+        drop(update_guard);\n         Ok(())\n     }\n+}\n \n-    pub fn set_user_manifest(&self, user_manifest: LocalUserManifest) -> FSResult<()> {\n-        self.manifest_storage.set_manifest(\n-            self.user_manifest_id,\n-            LocalManifest::User(user_manifest),\n+pub async fn user_storage_non_speculative_init(\n+    data_base_dir: &Path,\n+    device: LocalDevice,\n+) -> FSResult<()> {\n+    let data_path = get_user_data_storage_db_path(data_base_dir, &device);\n+    let conn = LocalDatabase::from_path(\n+        data_path\n+            .to_str()\n+            .expect(\"Non Utf-8 character found in data_path\"),\n+        VacuumMode::default(),\n+    )\n+    .await?;\n+    let conn = Arc::new(conn);\n+    let manifest_storage =\n+        ManifestStorage::new(device.local_symkey.clone(), device.user_manifest_id, conn).await?;\n+\n+    let timestamp = device.now();\n+    let manifest = LocalUserManifest::new(\n+        device.device_id,\n+        timestamp,\n+        Some(device.user_manifest_id),\n+        false,\n+    );\n+\n+    manifest_storage\n+        .set_manifest(\n+            device.user_manifest_id,\n+            LocalManifest::User(manifest),\n             false,\n             None,\n         )\n-    }\n+        .await?;\n+    manifest_storage.clear_memory_cache(true).await?;\n+    manifest_storage.close_connection();\n+\n+    Ok(())\n }\n \n #[cfg(test)]\n mod tests {\n-    use std::sync::Mutex;\n-\n-    use libparsec_crypto::SecretKey;\n     use libparsec_types::{DateTime, UserManifest};\n \n     use libparsec_tests_fixtures::{alice, timestamp, tmp_path, Device, TmpPath};\n     use rstest::rstest;\n \n-    use super::super::local_database::SqlitePool;\n     use super::*;\n \n     #[rstest]\n-    fn user_storage(alice: &Device, timestamp: DateTime, tmp_path: TmpPath) {\n+    #[tokio::test]\n+    async fn user_storage(alice: &Device, timestamp: DateTime, tmp_path: TmpPath) {\n         let db_path = tmp_path.join(\"user_storage.sqlite\");\n-        let pool = SqlitePool::new(db_path.to_str().unwrap()).unwrap();\n-        let conn = Mutex::new(pool.conn().unwrap());\n-        let local_symkey = SecretKey::generate();\n-        let realm_id = EntryID::default();\n         let user_manifest_id = alice.user_manifest_id;\n \n-        let manifest_storage = ManifestStorage::new(local_symkey, conn, realm_id).unwrap();\n         let user_storage =\n-            UserStorage::new(alice.local_device(), user_manifest_id, manifest_storage);\n+            UserStorage::from_db_dir(alice.local_device(), user_manifest_id, &db_path)\n+                .await\n+                .unwrap();\n \n-        user_storage.get_realm_checkpoint();\n-        user_storage.update_realm_checkpoint(64, &[]).unwrap();\n-        user_storage.get_need_sync_entries().unwrap();\n-        user_storage.load_user_manifest().unwrap();\n+        user_storage.get_realm_checkpoint().await;\n+        user_storage.update_realm_checkpoint(64, &[]).await.unwrap();\n+        user_storage.get_need_sync_entries().await.unwrap();\n \n         let user_manifest = LocalUserManifest {\n             base: UserManifest {\n@@ -144,8 +238,9 @@ mod tests {\n \n         user_storage\n             .set_user_manifest(user_manifest.clone())\n+            .await\n             .unwrap();\n \n-        assert_eq!(user_storage.get_user_manifest().unwrap(), user_manifest);\n+        assert_eq!(user_storage.get_user_manifest(), user_manifest);\n     }\n }"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/version.rs",
      "status": "modified",
      "patch": "@@ -7,6 +7,13 @@ use libparsec_types::EntryID;\n \n const STORAGE_REVISION: u32 = 1;\n \n+pub(crate) fn get_user_data_storage_db_path(data_base_dir: &Path, device: &LocalDevice) -> PathBuf {\n+    let slug = device.slug();\n+    let mut path = PathBuf::from(data_base_dir);\n+    path.extend([slug, format!(\"user_data-v{STORAGE_REVISION}.sqlite\")]);\n+    path\n+}\n+\n pub(crate) fn get_workspace_data_storage_db_path(\n     data_base_dir: &Path,\n     device: &LocalDevice,\n@@ -16,7 +23,7 @@ pub(crate) fn get_workspace_data_storage_db_path(\n     let mut path = PathBuf::from(data_base_dir);\n     path.extend([\n         slug,\n-        workspace_id.to_string(),\n+        workspace_id.hex(),\n         format!(\"workspace_data-v{STORAGE_REVISION}.sqlite\"),\n     ]);\n     path\n@@ -31,7 +38,7 @@ pub(crate) fn get_workspace_cache_storage_db_path(\n     let mut path = PathBuf::from(data_base_dir);\n     path.extend([\n         slug,\n-        workspace_id.to_string(),\n+        workspace_id.hex(),\n         format!(\"workspace_cache-v{STORAGE_REVISION}.sqlite\"),\n     ]);\n     path"
    },
    {
      "filename": "oxidation/libparsec/crates/core_fs/src/storage/workspace_storage.rs",
      "status": "modified",
      "patch": "@@ -1,58 +1,102 @@\n // Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n \n-use regex::Regex;\n-use std::collections::{HashMap, HashSet};\n-use std::hash::Hash;\n-use std::path::Path;\n-use std::sync::{Arc, Mutex, MutexGuard, TryLockError};\n+use std::{\n+    collections::{HashMap, HashSet},\n+    path::Path,\n+    sync::{Arc, Mutex},\n+};\n \n use libparsec_client_types::{\n-    LocalDevice, LocalFileManifest, LocalManifest, LocalWorkspaceManifest,\n+    LocalDevice, LocalFileManifest, LocalFolderManifest, LocalManifest, LocalUserManifest,\n+    LocalWorkspaceManifest,\n+};\n+use libparsec_platform_async::Mutex as AsyncMutex;\n+use libparsec_platform_local_db::{AutoVacuum, LocalDatabase, VacuumMode};\n+use libparsec_types::{BlockID, ChunkID, EntryID, FileDescriptor, Regex};\n+\n+use super::{\n+    chunk_storage::{ChunkStorage, Remanence},\n+    manifest_storage::{ChunkOrBlockID, ManifestStorage},\n };\n-use libparsec_types::{BlockID, ChunkID, EntryID, FileDescriptor};\n-\n-use super::chunk_storage::ChunkStorage;\n-use super::manifest_storage::{ChunkOrBlockID, ManifestStorage};\n-use crate::error::{FSError, FSResult};\n-use crate::storage::chunk_storage::{BlockStorage, BlockStorageTrait, ChunkStorageTrait};\n-use crate::storage::local_database::SqlitePool;\n-use crate::storage::version::{\n-    get_workspace_cache_storage_db_path, get_workspace_data_storage_db_path,\n+use crate::{\n+    error::{FSError, FSResult},\n+    storage::{\n+        chunk_storage::{BlockStorage, BlockStorageTrait, ChunkStorageTrait},\n+        version::{get_workspace_cache_storage_db_path, get_workspace_data_storage_db_path},\n+    },\n };\n \n+/// The default threshold at which when vacuuming the chunk storage, will start to do things.\n+pub const DEFAULT_CHUNK_VACUUM_THRESHOLD: usize = 512 * 1024 * 1024;\n+\n+/// The default cache size to store block\n pub const DEFAULT_WORKSPACE_STORAGE_CACHE_SIZE: u64 = 512 * 1024 * 1024;\n \n-#[derive(Default)]\n-struct Locker<T: Eq + Hash>(Mutex<HashMap<T, Arc<Mutex<()>>>>);\n+lazy_static::lazy_static! {\n+    pub static ref FAILSAFE_PATTERN_FILTER: Regex = {\n+        Regex::from_regex_str(\"^\\\\b$\").expect(\"Must be a valid regex\")\n+    };\n+}\n \n-enum Status {\n-    Locked,\n-    Released,\n+#[derive(Debug, Clone, PartialEq, Eq)]\n+/// A File or Folder local manifest.\n+pub enum LocalFileOrFolderManifest {\n+    File(LocalFileManifest),\n+    Folder(LocalFolderManifest),\n }\n \n-impl<T: Eq + Hash + Copy> Locker<T> {\n-    fn acquire(&self, id: T) -> Arc<Mutex<()>> {\n-        let mut map = self.0.lock().expect(\"Mutex is poisoned\");\n-        map.entry(id).or_insert_with(|| Arc::new(Mutex::new(())));\n-        map.get(&id).unwrap_or_else(|| unreachable!()).clone()\n+impl LocalFileOrFolderManifest {\n+    pub fn need_sync(&self) -> bool {\n+        match self {\n+            LocalFileOrFolderManifest::File(manifest) => manifest.need_sync,\n+            LocalFileOrFolderManifest::Folder(manifest) => manifest.need_sync,\n+        }\n     }\n-    fn release(&self, id: T, guard: MutexGuard<()>) {\n-        drop(guard);\n-        self.0.lock().expect(\"Mutex is poisoned\").remove(&id);\n+}\n+\n+impl TryFrom<LocalManifest> for LocalFileOrFolderManifest {\n+    type Error = LocalUserOrWorkspaceManifest;\n+\n+    fn try_from(value: LocalManifest) -> Result<Self, Self::Error> {\n+        match value {\n+            LocalManifest::File(manifest) => Ok(Self::File(manifest)),\n+            LocalManifest::Folder(manifest) => Ok(Self::Folder(manifest)),\n+            LocalManifest::Workspace(manifest) => Err(manifest.into()),\n+            LocalManifest::User(manifest) => Err(manifest.into()),\n+        }\n     }\n-    fn check(&self, id: T) -> Status {\n-        if let Some(mutex) = self.0.lock().expect(\"Mutex is poisoned\").get(&id) {\n-            if let Err(TryLockError::WouldBlock) = mutex.try_lock() {\n-                return Status::Locked;\n-            }\n+}\n+\n+impl From<LocalFileOrFolderManifest> for LocalManifest {\n+    fn from(value: LocalFileOrFolderManifest) -> Self {\n+        match value {\n+            LocalFileOrFolderManifest::File(manifest) => LocalManifest::File(manifest),\n+            LocalFileOrFolderManifest::Folder(manifest) => LocalManifest::Folder(manifest),\n         }\n+    }\n+}\n+\n+/// A User or Workspace manifest.\n+pub enum LocalUserOrWorkspaceManifest {\n+    User(LocalUserManifest),\n+    Workspace(LocalWorkspaceManifest),\n+}\n \n-        Status::Released\n+impl From<LocalUserManifest> for LocalUserOrWorkspaceManifest {\n+    fn from(value: LocalUserManifest) -> Self {\n+        Self::User(value)\n+    }\n+}\n+\n+impl From<LocalWorkspaceManifest> for LocalUserOrWorkspaceManifest {\n+    fn from(value: LocalWorkspaceManifest) -> Self {\n+        Self::Workspace(value)\n     }\n }\n \n /// WorkspaceStorage is implemented with interior mutability because\n-/// we want some parallelism between its fields (e.g entry_locks)\n+/// we want some parallelism between its fields (e.g open_fds)\n+// TODO: Currently we handle EntryID lock in Python, should be implemented here instead\n pub struct WorkspaceStorage {\n     pub device: LocalDevice,\n     pub workspace_id: EntryID,\n@@ -63,54 +107,69 @@ pub struct WorkspaceStorage {\n     manifest_storage: ManifestStorage,\n     prevent_sync_pattern: Mutex<Regex>,\n     prevent_sync_pattern_fully_applied: Mutex<bool>,\n-    entry_locks: Locker<EntryID>,\n+    lock_manifest_udpate: AsyncMutex<()>,\n+    /// Keep a copy of the workspace manifest to have it available at all time.\n+    /// (We don't rely on [ManifestStorage]'s cache since it can be cleared).\n+    workspace_manifest_copy: Mutex<LocalWorkspaceManifest>,\n }\n \n impl WorkspaceStorage {\n-    pub fn new(\n+    pub async fn new(\n         // Allow different type as Path, PathBuf, String, &str\n-        data_base_dir: impl AsRef<Path>,\n+        data_base_dir: impl AsRef<Path> + Send,\n         device: LocalDevice,\n         workspace_id: EntryID,\n+        prevent_sync_pattern: Regex,\n+        data_vacuum_threshold: usize,\n         cache_size: u64,\n     ) -> FSResult<Self> {\n         let data_path =\n             get_workspace_data_storage_db_path(data_base_dir.as_ref(), &device, workspace_id);\n         let cache_path =\n             get_workspace_cache_storage_db_path(data_base_dir.as_ref(), &device, workspace_id);\n \n-        let data_pool = SqlitePool::new(\n+        // TODO: once the auto_vacuum approach has been validated for the cache storage,\n+        // we should investigate whether it is a good fit for the data storage.\n+        let data_conn = LocalDatabase::from_path(\n             data_path\n                 .to_str()\n                 .expect(\"Non-Utf-8 character found in data_path\"),\n-        )?;\n-        let cache_pool = SqlitePool::new(\n+            VacuumMode::WithThreshold(data_vacuum_threshold),\n+        )\n+        .await?;\n+        let data_conn = Arc::new(data_conn);\n+        let cache_conn = LocalDatabase::from_path(\n             cache_path\n                 .to_str()\n                 .expect(\"Non-Utf-8 character found in cache_path\"),\n-        )?;\n+            VacuumMode::Automatic(AutoVacuum::Full),\n+        )\n+        .await?;\n \n         let block_storage = BlockStorage::new(\n             device.local_symkey.clone(),\n-            Mutex::new(cache_pool.conn()?),\n+            cache_conn,\n             cache_size,\n             device.time_provider.clone(),\n-        )?;\n+        )\n+        .await?;\n \n-        let manifest_storage = ManifestStorage::new(\n-            device.local_symkey.clone(),\n-            Mutex::new(data_pool.conn()?),\n-            workspace_id,\n-        )?;\n+        let manifest_storage =\n+            ManifestStorage::new(device.local_symkey.clone(), workspace_id, data_conn.clone())\n+                .await?;\n \n         let chunk_storage = ChunkStorage::new(\n             device.local_symkey.clone(),\n-            Mutex::new(data_pool.conn()?),\n+            data_conn,\n             device.time_provider.clone(),\n-        )?;\n+        )\n+        .await?;\n \n-        let (prevent_sync_pattern, prevent_sync_pattern_fully_applied) =\n-            manifest_storage.get_prevent_sync_pattern()?;\n+        // Populate the cache with the workspace manifest to be able to\n+        // access it synchronously at all time\n+        let workspace_manifest =\n+            WorkspaceStorage::load_workspace_manifest(&manifest_storage, workspace_id, &device)\n+                .await?;\n \n         // Instantiate workspace storage\n         let instance = Self {\n@@ -124,37 +183,24 @@ impl WorkspaceStorage {\n             chunk_storage,\n             manifest_storage,\n             // Pattern attributes\n-            prevent_sync_pattern: Mutex::new(prevent_sync_pattern),\n-            prevent_sync_pattern_fully_applied: Mutex::new(prevent_sync_pattern_fully_applied),\n-            // Locking structure\n-            entry_locks: Locker::default(),\n-        };\n+            prevent_sync_pattern: Mutex::new(prevent_sync_pattern.clone()),\n+            prevent_sync_pattern_fully_applied: Mutex::new(false),\n \n-        instance.block_storage.cleanup()?;\n-        instance.block_storage.run_vacuum()?;\n-        // Populate the cache with the workspace manifest to be able to\n-        // access it synchronously at all time\n-        instance.load_workspace_manifest()?;\n+            lock_manifest_udpate: AsyncMutex::new(()),\n+            workspace_manifest_copy: Mutex::new(workspace_manifest),\n+        };\n \n-        Ok(instance)\n-    }\n+        instance\n+            .set_prevent_sync_pattern(&prevent_sync_pattern)\n+            .await?;\n \n-    pub fn lock_entry_id(&self, entry_id: EntryID) -> Arc<Mutex<()>> {\n-        self.entry_locks.acquire(entry_id)\n-    }\n+        instance.block_storage.cleanup().await?;\n+        instance.block_storage.run_vacuum().await?;\n \n-    pub fn release_entry_id(&self, entry_id: EntryID, guard: MutexGuard<()>) {\n-        self.entry_locks.release(entry_id, guard)\n-    }\n-\n-    fn check_lock_status(&self, entry_id: EntryID) -> FSResult<()> {\n-        if let Status::Released = self.entry_locks.check(entry_id) {\n-            return Err(FSError::Runtime(entry_id));\n-        }\n-        Ok(())\n+        Ok(instance)\n     }\n \n-    pub fn get_prevent_sync_pattern(&self) -> Regex {\n+    pub fn get_prevent_sync_pattern(&self) -> libparsec_types::Regex {\n         self.prevent_sync_pattern\n             .lock()\n             .expect(\"Mutex is poisoned\")\n@@ -185,13 +231,33 @@ impl WorkspaceStorage {\n         fd\n     }\n \n-    pub fn load_file_descriptor(&self, fd: FileDescriptor) -> FSResult<LocalFileManifest> {\n-        match self.open_fds.lock().expect(\"Mutex is poisoned\").get(&fd) {\n-            Some(&entry_id) => match self.get_manifest(entry_id) {\n-                Ok(LocalManifest::File(manifest)) => Ok(manifest),\n-                _ => Err(FSError::LocalMiss(*entry_id)),\n-            },\n-            None => Err(FSError::InvalidFileDescriptor(fd)),\n+    pub fn load_file_descriptor_in_cache(&self, fd: FileDescriptor) -> FSResult<LocalFileManifest> {\n+        let entry_id = self\n+            .open_fds\n+            .lock()\n+            .expect(\"Mutex is poisoned\")\n+            .get(&fd)\n+            .cloned()\n+            .ok_or(FSError::InvalidFileDescriptor(fd))?;\n+\n+        match self.get_manifest_in_cache(&entry_id) {\n+            Some(LocalManifest::File(manifest)) => Ok(manifest),\n+            _ => Err(FSError::LocalMiss(*entry_id)),\n+        }\n+    }\n+\n+    pub async fn load_file_descriptor(&self, fd: FileDescriptor) -> FSResult<LocalFileManifest> {\n+        let entry_id = self\n+            .open_fds\n+            .lock()\n+            .expect(\"Mutex is poisoned\")\n+            .get(&fd)\n+            .copied()\n+            .ok_or(FSError::InvalidFileDescriptor(fd))?;\n+\n+        match self.get_manifest(entry_id).await {\n+            Ok(LocalManifest::File(manifest)) => Ok(manifest),\n+            _ => Err(FSError::LocalMiss(*entry_id)),\n         }\n     }\n \n@@ -201,197 +267,348 @@ impl WorkspaceStorage {\n \n     // Block interface\n \n-    pub fn set_clean_block(&self, block_id: BlockID, block: &[u8]) -> FSResult<()> {\n-        self.block_storage\n-            .set_chunk_upgraded(ChunkID::from(*block_id), block)\n+    pub async fn set_clean_block(\n+        &self,\n+        block_id: BlockID,\n+        block: &[u8],\n+    ) -> FSResult<HashSet<BlockID>> {\n+        self.block_storage.set_clean_block(block_id, block).await\n+    }\n+\n+    pub async fn is_clean_block(&self, block_id: BlockID) -> FSResult<bool> {\n+        self.block_storage.is_chunk(ChunkID::from(*block_id)).await\n     }\n \n-    pub fn clear_clean_block(&self, block_id: BlockID) {\n-        let _ = self.block_storage.clear_chunk(ChunkID::from(*block_id));\n+    pub async fn clear_clean_block(&self, block_id: BlockID) {\n+        let _: FSResult<()> = self\n+            .block_storage\n+            .clear_chunk(ChunkID::from(*block_id))\n+            .await;\n     }\n \n-    pub fn get_dirty_block(&self, block_id: BlockID) -> FSResult<Vec<u8>> {\n-        self.chunk_storage.get_chunk(ChunkID::from(*block_id))\n+    pub async fn get_dirty_block(&self, block_id: BlockID) -> FSResult<Vec<u8>> {\n+        self.chunk_storage.get_chunk(ChunkID::from(*block_id)).await\n     }\n \n     // Chunk interface\n \n-    pub fn get_chunk(&self, chunk_id: ChunkID) -> FSResult<Vec<u8>> {\n-        if let Ok(raw) = self.chunk_storage.get_chunk(chunk_id) {\n+    pub async fn get_chunk(&self, chunk_id: ChunkID) -> FSResult<Vec<u8>> {\n+        if let Ok(raw) = self.chunk_storage.get_chunk(chunk_id).await {\n             Ok(raw)\n-        } else if let Ok(raw) = self.block_storage.get_chunk(chunk_id) {\n+        } else if let Ok(raw) = self.block_storage.get_chunk(chunk_id).await {\n             Ok(raw)\n         } else {\n             Err(FSError::LocalMiss(*chunk_id))\n         }\n     }\n \n-    pub fn set_chunk(&self, chunk_id: ChunkID, block: &[u8]) -> FSResult<()> {\n-        self.chunk_storage.set_chunk(chunk_id, block)\n+    pub async fn set_chunk(&self, chunk_id: ChunkID, block: &[u8]) -> FSResult<()> {\n+        self.chunk_storage.set_chunk(chunk_id, block).await\n     }\n \n-    pub fn clear_chunk(&self, chunk_id: ChunkID, miss_ok: bool) -> FSResult<()> {\n-        let res = self.chunk_storage.clear_chunk(chunk_id);\n-        if !miss_ok {\n+    pub async fn clear_chunk(&self, chunk_id: ChunkID, miss_ok: bool) -> FSResult<()> {\n+        let res = self.chunk_storage.clear_chunk(chunk_id).await;\n+        if !miss_ok || res != Err(FSError::LocalMiss(*chunk_id)) {\n             return res;\n         }\n         Ok(())\n     }\n \n+    pub async fn clear_chunks(&self, chunk_ids: &[ChunkID]) -> FSResult<()> {\n+        self.chunk_storage.clear_chunks(chunk_ids).await\n+    }\n+\n+    pub async fn remove_clean_blocks(&self, block_ids: &[BlockID]) -> FSResult<()> {\n+        let chunk_ids = block_ids\n+            .iter()\n+            .map(|id| ChunkID::from(*id.as_bytes()))\n+            .collect::<Vec<_>>();\n+        self.block_storage.clear_chunks(&chunk_ids).await\n+    }\n+\n     // Helpers\n \n-    pub fn clear_memory_cache(&self, flush: bool) -> FSResult<()> {\n-        self.manifest_storage.clear_memory_cache(flush)\n+    pub async fn clear_memory_cache(&self, flush: bool) -> FSResult<()> {\n+        self.manifest_storage.clear_memory_cache(flush).await\n     }\n \n     // Checkpoint interface\n \n-    pub fn get_realm_checkpoint(&self) -> i64 {\n-        self.manifest_storage.get_realm_checkpoint()\n+    pub async fn get_realm_checkpoint(&self) -> i64 {\n+        self.manifest_storage.get_realm_checkpoint().await\n     }\n \n-    pub fn update_realm_checkpoint(\n+    pub async fn update_realm_checkpoint(\n         &self,\n         new_checkpoint: i64,\n         changed_vlobs: &[(EntryID, i64)],\n     ) -> FSResult<()> {\n         self.manifest_storage\n             .update_realm_checkpoint(new_checkpoint, changed_vlobs)\n+            .await\n     }\n \n-    pub fn get_need_sync_entries(&self) -> FSResult<(HashSet<EntryID>, HashSet<EntryID>)> {\n-        self.manifest_storage.get_need_sync_entries()\n+    pub async fn get_need_sync_entries(&self) -> FSResult<(HashSet<EntryID>, HashSet<EntryID>)> {\n+        self.manifest_storage.get_need_sync_entries().await\n     }\n \n     // Manifest interface\n \n-    fn load_workspace_manifest(&self) -> FSResult<()> {\n-        if self\n-            .manifest_storage\n-            .get_manifest(self.workspace_id)\n-            .is_err()\n-        {\n-            let timestamp = self.device.now();\n-            let manifest = LocalWorkspaceManifest::new(\n-                self.device.device_id.clone(),\n-                timestamp,\n-                Some(self.workspace_id),\n-                true,\n-            );\n-            self.manifest_storage.set_manifest(\n-                self.workspace_id,\n-                LocalManifest::Workspace(manifest),\n-                false,\n-                None,\n-            )?;\n+    async fn load_workspace_manifest(\n+        manifest_storage: &ManifestStorage,\n+        workspace_id: EntryID,\n+        device: &LocalDevice,\n+    ) -> FSResult<LocalWorkspaceManifest> {\n+        match manifest_storage.get_manifest(workspace_id).await {\n+            Ok(LocalManifest::Workspace(manifest)) => Ok(manifest),\n+            Ok(_) => panic!(\n+                \"Workspace manifest id is used for something other than a workspace manifest\"\n+            ),\n+            // It is possible to lack the workspace manifest in local if our\n+            // device hasn't tried to access it yet (and we are not the creator\n+            // of the workspace, in which case the workspacefs local db is\n+            // initialized with a non-speculative local manifest placeholder).\n+            // In such case it is easy to fall back on an empty manifest\n+            // which is a good enough approximation of the very first version\n+            // of the manifest (field `created` is invalid, but it will be\n+            // correction by the merge during sync).\n+            // This approach also guarantees the workspace root folder is always\n+            // consistent (ls/touch/mkdir always works on it), which is not the\n+            // case for the others files and folders (as their access may\n+            // require communication with the backend).\n+            // This is especially important when the workspace is accessed from\n+            // file system mountpoint given having a weird error popup when clicking\n+            // on the mountpoint from the file explorer really feel like a bug :/\n+            Err(_) => {\n+                let timestamp = device.now();\n+                let manifest = LocalWorkspaceManifest::new(\n+                    device.device_id.clone(),\n+                    timestamp,\n+                    Some(workspace_id),\n+                    true,\n+                );\n+                manifest_storage\n+                    .set_manifest(\n+                        workspace_id,\n+                        LocalManifest::Workspace(manifest.clone()),\n+                        false,\n+                        None,\n+                    )\n+                    .await\n+                    .and(Ok(manifest))\n+            }\n         }\n-\n-        Ok(())\n     }\n \n-    pub fn get_workspace_manifest(&self) -> FSResult<LocalWorkspaceManifest> {\n-        let cache = self\n-            .manifest_storage\n-            .cache\n+    pub fn get_workspace_manifest(&self) -> LocalWorkspaceManifest {\n+        self.workspace_manifest_copy\n             .lock()\n-            .expect(\"Mutex is poisoned\");\n-        match cache.get(&self.workspace_id) {\n-            Some(LocalManifest::Workspace(manifest)) => Ok(manifest.clone()),\n-            _ => Err(FSError::LocalMiss(*self.workspace_id)),\n-        }\n+            .expect(\"Mutex is poisoned\")\n+            .clone()\n     }\n \n-    pub fn get_manifest(&self, entry_id: EntryID) -> FSResult<LocalManifest> {\n-        self.manifest_storage.get_manifest(entry_id)\n+    pub async fn get_manifest(&self, entry_id: EntryID) -> FSResult<LocalManifest> {\n+        self.manifest_storage.get_manifest(entry_id).await\n     }\n \n-    pub fn set_manifest(\n+    pub fn set_manifest_in_cache(\n         &self,\n         entry_id: EntryID,\n-        manifest: LocalManifest,\n-        cache_only: bool,\n-        check_lock_status: bool,\n+        manifest: LocalFileOrFolderManifest,\n         removed_ids: Option<HashSet<ChunkOrBlockID>>,\n     ) -> FSResult<()> {\n-        if check_lock_status {\n-            self.check_lock_status(entry_id)?;\n-        }\n         self.manifest_storage\n-            .set_manifest(entry_id, manifest, cache_only, removed_ids)\n+            .set_manifest_cache_only(entry_id, manifest.into(), removed_ids);\n+        Ok(())\n     }\n \n-    pub fn ensure_manifest_persistent(\n+    pub async fn set_manifest(\n         &self,\n         entry_id: EntryID,\n-        check_lock_status: bool,\n+        manifest: LocalFileOrFolderManifest,\n+        cache_only: bool,\n+        removed_ids: Option<HashSet<ChunkOrBlockID>>,\n     ) -> FSResult<()> {\n-        if check_lock_status {\n-            self.check_lock_status(entry_id)?;\n-        }\n-        self.manifest_storage.ensure_manifest_persistent(entry_id)\n+        self.manifest_storage\n+            .set_manifest(entry_id, manifest.into(), cache_only, removed_ids)\n+            .await?;\n+        Ok(())\n     }\n \n-    #[allow(deprecated)]\n-    pub fn clear_manifest(&self, entry_id: EntryID, check_lock_status: bool) -> FSResult<()> {\n-        if check_lock_status {\n-            self.check_lock_status(entry_id)?;\n+    pub async fn set_workspace_manifest(&self, manifest: LocalWorkspaceManifest) -> FSResult<()> {\n+        if manifest.base.id != self.workspace_id {\n+            panic!(\"Trying to set a workspace manifest which id isn't the same as the WorkspaceStorage::id (manifest_id={}, workspace_id={})\", manifest.base.id, self.workspace_id)\n         }\n-        self.manifest_storage.clear_manifest(entry_id)\n+        let guard = self.lock_manifest_udpate.lock().await;\n+\n+        self.manifest_storage\n+            .set_manifest(self.workspace_id, manifest.clone().into(), false, None)\n+            .await?;\n+\n+        *self\n+            .workspace_manifest_copy\n+            .lock()\n+            .expect(\"Mutex is poisoned\") = manifest;\n+\n+        drop(guard);\n+        Ok(())\n+    }\n+\n+    pub async fn ensure_manifest_persistent(&self, entry_id: EntryID) -> FSResult<()> {\n+        self.manifest_storage\n+            .ensure_manifest_persistent(entry_id)\n+            .await\n+    }\n+\n+    #[cfg(any(test, feature = \"test-utils\"))]\n+    pub async fn clear_manifest(&self, entry_id: &EntryID) -> FSResult<()> {\n+        self.manifest_storage.clear_manifest(*entry_id).await\n+    }\n+\n+    /// Close the connections to the databases.\n+    /// Provide a way to manually close those connections.\n+    /// Event tho they will be closed when [WorkspaceStorage] is dropped.\n+    pub fn close_connections(&self) {\n+        self.manifest_storage.close_connection();\n+        self.chunk_storage.close_connection();\n+        self.block_storage.close_connection();\n     }\n \n     // Prevent sync pattern interface\n \n-    fn load_prevent_sync_pattern(&self) -> FSResult<()> {\n-        (\n-            *self.prevent_sync_pattern.lock().expect(\"Mutex is poisoned\"),\n-            *self\n-                .prevent_sync_pattern_fully_applied\n-                .lock()\n-                .expect(\"Mutex is poisoned\"),\n-        ) = self.manifest_storage.get_prevent_sync_pattern()?;\n+    fn load_prevent_sync_pattern(&self, re: &Regex, fully_applied: bool) {\n+        *self.prevent_sync_pattern.lock().expect(\"Mutex is poisoned\") = re.clone();\n+        *self\n+            .prevent_sync_pattern_fully_applied\n+            .lock()\n+            .expect(\"Mutex is poisoned\") = fully_applied;\n+    }\n+\n+    pub async fn set_prevent_sync_pattern(&self, pattern: &Regex) -> FSResult<()> {\n+        let fully_applied = self\n+            .manifest_storage\n+            .set_prevent_sync_pattern(pattern)\n+            .await?;\n+        self.load_prevent_sync_pattern(pattern, fully_applied);\n+        Ok(())\n+    }\n+\n+    pub async fn mark_prevent_sync_pattern_fully_applied(&self, pattern: &Regex) -> FSResult<()> {\n+        let fully_applied = self\n+            .manifest_storage\n+            .mark_prevent_sync_pattern_fully_applied(pattern)\n+            .await?;\n+        self.load_prevent_sync_pattern(pattern, fully_applied);\n         Ok(())\n     }\n \n-    pub fn set_prevent_sync_pattern(&self, pattern: &Regex) -> FSResult<()> {\n-        self.manifest_storage.set_prevent_sync_pattern(pattern)?;\n-        self.load_prevent_sync_pattern()\n+    pub async fn get_local_block_ids(&self, chunk_ids: &[ChunkID]) -> FSResult<Vec<ChunkID>> {\n+        self.block_storage.get_local_chunk_ids(chunk_ids).await\n+    }\n+\n+    pub async fn get_local_chunk_ids(&self, chunk_ids: &[ChunkID]) -> FSResult<Vec<ChunkID>> {\n+        self.chunk_storage.get_local_chunk_ids(chunk_ids).await\n     }\n \n-    pub fn mark_prevent_sync_pattern_fully_applied(&self, pattern: &Regex) -> FSResult<()> {\n+    pub async fn run_vacuum(&self) -> FSResult<()> {\n+        self.chunk_storage.run_vacuum().await\n+    }\n+\n+    /// Return `true` when the given manifest identified by `entry_id` is cached.\n+    pub fn is_manifest_cache_ahead_of_persistance(&self, entry_id: &EntryID) -> bool {\n         self.manifest_storage\n-            .mark_prevent_sync_pattern_fully_applied(pattern)?;\n-        self.load_prevent_sync_pattern()\n+            .is_manifest_cache_ahead_of_persistance(entry_id)\n+    }\n+\n+    pub fn get_manifest_in_cache(&self, entry_id: &EntryID) -> Option<LocalManifest> {\n+        self.manifest_storage.get_manifest_in_cache(entry_id)\n+    }\n+}\n+\n+#[async_trait::async_trait]\n+impl Remanence for WorkspaceStorage {\n+    fn is_block_remanent(&self) -> bool {\n+        self.block_storage.is_block_remanent()\n     }\n \n-    pub fn get_local_block_ids(&self, chunk_ids: &[ChunkID]) -> FSResult<Vec<ChunkID>> {\n-        self.block_storage.get_local_chunk_ids(chunk_ids)\n+    async fn enable_block_remanence(&self) -> FSResult<bool> {\n+        self.block_storage.enable_block_remanence().await\n     }\n \n-    pub fn get_local_chunk_ids(&self, chunk_ids: &[ChunkID]) -> FSResult<Vec<ChunkID>> {\n-        self.chunk_storage.get_local_chunk_ids(chunk_ids)\n+    async fn disable_block_remanence(&self) -> FSResult<Option<HashSet<BlockID>>> {\n+        self.block_storage.disable_block_remanence().await\n     }\n \n-    pub fn run_vacuum(&self) -> FSResult<()> {\n-        // TODO: Add some condition\n-        self.chunk_storage.run_vacuum()\n+    async fn clear_unreferenced_chunks(\n+        &self,\n+        chunk_ids: &[ChunkID],\n+        not_accessed_after: libparsec_types::DateTime,\n+    ) -> FSResult<()> {\n+        self.block_storage\n+            .clear_unreferenced_chunks(chunk_ids, not_accessed_after)\n+            .await\n     }\n }\n \n+pub async fn workspace_storage_non_speculative_init(\n+    data_base_dir: &Path,\n+    device: LocalDevice,\n+    workspace_id: EntryID,\n+) -> FSResult<()> {\n+    let data_path = get_workspace_data_storage_db_path(data_base_dir, &device, workspace_id);\n+    let conn = LocalDatabase::from_path(\n+        data_path\n+            .to_str()\n+            .expect(\"Non-Utf-8 character found in data_path\"),\n+        VacuumMode::default(),\n+    )\n+    .await?;\n+    let conn = Arc::new(conn);\n+    let manifest_storage =\n+        ManifestStorage::new(device.local_symkey.clone(), workspace_id, conn).await?;\n+    let timestamp = device.now();\n+    let manifest =\n+        LocalWorkspaceManifest::new(device.device_id, timestamp, Some(workspace_id), false);\n+\n+    manifest_storage\n+        .set_manifest(\n+            workspace_id,\n+            LocalManifest::Workspace(manifest),\n+            false,\n+            None,\n+        )\n+        .await?;\n+    manifest_storage.clear_memory_cache(true).await?;\n+    manifest_storage.close_connection();\n+\n+    Ok(())\n+}\n+\n #[cfg(test)]\n mod tests {\n+    // TODO:\n+    // We need to allow `await_holding_lock` because we use the `Locker` interface that use sync mutex.\n+    // Clippy don't like for a reason (We block the async runtime if a sync mutex block) that we hold blocking mutex between await block.\n+    // To remove that `allow` and to be able to use the `locker` interface of the workspace storage\n+    // We should rework the interface of the `Locker`.\n+    #![allow(clippy::await_holding_lock)]\n+\n     use crate::conftest::{alice_workspace_storage, TmpWorkspaceStorage};\n     use libparsec_client_types::Chunk;\n     use libparsec_tests_fixtures::{alice, tmp_path, Device, TmpPath};\n-    use libparsec_types::{Blocksize, DEFAULT_BLOCK_SIZE};\n+    use libparsec_types::{Blocksize, Regex, DEFAULT_BLOCK_SIZE};\n     use rstest::rstest;\n     use std::num::NonZeroU64;\n \n     use super::*;\n \n-    fn create_workspace_manifest(device: &LocalDevice) -> LocalWorkspaceManifest {\n+    fn create_workspace_manifest(\n+        device: &LocalDevice,\n+        workspace_id: EntryID,\n+    ) -> LocalWorkspaceManifest {\n         let author = device.device_id.clone();\n         let timestamp = device.now();\n-        LocalWorkspaceManifest::new(author, timestamp, None, false)\n+        LocalWorkspaceManifest::new(author, timestamp, Some(workspace_id), false)\n     }\n \n     fn create_file_manifest(device: &LocalDevice) -> LocalFileManifest {\n@@ -400,472 +617,452 @@ mod tests {\n         LocalFileManifest::new(author, EntryID::default(), timestamp, DEFAULT_BLOCK_SIZE)\n     }\n \n-    fn clear_cache(storage: &WorkspaceStorage) {\n-        storage.manifest_storage.cache.lock().unwrap().clear();\n-    }\n-\n-    #[rstest]\n-    fn test_lock_required(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n-        let manifest = create_workspace_manifest(&aws.device);\n-        let manifest = LocalManifest::Workspace(manifest);\n-        let manifest_id = manifest.id();\n-\n-        assert_eq!(\n-            aws.set_manifest(manifest_id, manifest, false, true, None)\n-                .unwrap_err(),\n-            FSError::Runtime(manifest_id)\n-        );\n-\n-        assert_eq!(\n-            aws.ensure_manifest_persistent(manifest_id, true)\n-                .unwrap_err(),\n-            FSError::Runtime(manifest_id)\n-        );\n-\n-        assert_eq!(\n-            aws.clear_manifest(manifest_id, true).unwrap_err(),\n-            FSError::Runtime(manifest_id)\n-        );\n+    async fn clear_cache(storage: &WorkspaceStorage) {\n+        storage\n+            .manifest_storage\n+            .clear_memory_cache(false)\n+            .await\n+            .expect(\"Failed to flush cache\");\n     }\n \n     #[rstest]\n-    fn test_basic_set_get_clear(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n-        let manifest = create_workspace_manifest(&aws.device);\n-        let manifest = LocalManifest::Workspace(manifest);\n-        let manifest_id = manifest.id();\n-\n-        let mutex = aws.lock_entry_id(manifest_id);\n-        let guard = mutex.lock().unwrap();\n+    #[tokio::test]\n+    async fn test_basic_set_get_clear(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        let aws = alice_workspace_storage.await;\n+        let manifest = create_file_manifest(&aws.device);\n+        let manifest_id = manifest.base.id;\n+        let manifest = LocalFileOrFolderManifest::File(manifest);\n+        let gen_manifest = manifest.clone().into();\n \n         // 1) No data\n         assert_eq!(\n-            aws.get_manifest(manifest_id).unwrap_err(),\n+            aws.get_manifest(manifest_id).await.unwrap_err(),\n             FSError::LocalMiss(*manifest_id)\n         );\n \n         // 2) Set data\n-        aws.set_manifest(manifest_id, manifest.clone(), false, true, None)\n+        aws.set_manifest(manifest_id, manifest.clone(), false, None)\n+            .await\n             .unwrap();\n-        assert_eq!(aws.get_manifest(manifest_id).unwrap(), manifest);\n+        assert_eq!(aws.get_manifest(manifest_id).await.unwrap(), gen_manifest);\n \n         // Make sure data are not only stored in cache\n-        clear_cache(&aws);\n-        assert_eq!(aws.get_manifest(manifest_id).unwrap(), manifest);\n+        clear_cache(&aws).await;\n+        assert_eq!(aws.get_manifest(manifest_id).await.unwrap(), gen_manifest);\n \n         // 3) Clear data\n-        aws.clear_manifest(manifest_id, true).unwrap();\n+        aws.clear_manifest(&manifest_id).await.unwrap();\n \n         assert_eq!(\n-            aws.get_manifest(manifest_id).unwrap_err(),\n+            aws.get_manifest(manifest_id).await.unwrap_err(),\n             FSError::LocalMiss(*manifest_id)\n         );\n \n         assert_eq!(\n-            aws.clear_manifest(manifest_id, true).unwrap_err(),\n+            aws.clear_manifest(&manifest_id).await.unwrap_err(),\n             FSError::LocalMiss(*manifest_id)\n         );\n-\n-        aws.release_entry_id(manifest_id, guard);\n     }\n \n     #[rstest]\n-    fn test_cache_set_get(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n-        let manifest = create_workspace_manifest(&aws.device);\n-        let manifest = LocalManifest::Workspace(manifest);\n-        let manifest_id = manifest.id();\n-\n-        let mutex = aws.lock_entry_id(manifest_id);\n-        let guard = mutex.lock().unwrap();\n+    #[tokio::test]\n+    async fn test_cache_set_get(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        let aws = alice_workspace_storage.await;\n+        let manifest = create_file_manifest(&aws.device);\n+        let manifest_id = manifest.base.id;\n+        let manifest = LocalFileOrFolderManifest::File(manifest);\n+        let gen_manifest = manifest.clone().into();\n \n         // 1) Set data\n-        aws.set_manifest(manifest_id, manifest.clone(), true, true, None)\n+        aws.set_manifest(manifest_id, manifest.clone(), true, None)\n+            .await\n             .unwrap();\n-        assert_eq!(aws.get_manifest(manifest_id).unwrap(), manifest);\n+        assert_eq!(aws.get_manifest(manifest_id).await.unwrap(), gen_manifest);\n \n         // Data should be set only in the cache\n-        clear_cache(&aws);\n+        clear_cache(&aws).await;\n         assert_eq!(\n-            aws.get_manifest(manifest_id).unwrap_err(),\n+            aws.get_manifest(manifest_id).await.unwrap_err(),\n             FSError::LocalMiss(*manifest_id)\n         );\n \n         // Re-set data\n-        aws.set_manifest(manifest_id, manifest.clone(), true, true, None)\n+        aws.set_manifest(manifest_id, manifest.clone(), true, None)\n+            .await\n             .unwrap();\n \n         // 2) Clear should work as expected\n-        aws.clear_manifest(manifest_id, true).unwrap();\n+        aws.clear_manifest(&manifest_id).await.unwrap();\n         assert_eq!(\n-            aws.get_manifest(manifest_id).unwrap_err(),\n+            aws.get_manifest(manifest_id).await.unwrap_err(),\n             FSError::LocalMiss(*manifest_id)\n         );\n \n         // Re-set data\n-        aws.set_manifest(manifest_id, manifest.clone(), true, true, None)\n+        aws.set_manifest(manifest_id, manifest, true, None)\n+            .await\n             .unwrap();\n \n         // 3) Flush data\n-        aws.ensure_manifest_persistent(manifest_id, true).unwrap();\n-        assert_eq!(aws.get_manifest(manifest_id).unwrap(), manifest);\n+        aws.ensure_manifest_persistent(manifest_id).await.unwrap();\n+        assert_eq!(aws.get_manifest(manifest_id).await.unwrap(), gen_manifest);\n \n         // Data should be persistent in real database\n-        clear_cache(&aws);\n-        assert_eq!(aws.get_manifest(manifest_id).unwrap(), manifest);\n+        clear_cache(&aws).await;\n+        assert_eq!(aws.get_manifest(manifest_id).await.unwrap(), gen_manifest);\n \n         // 4) Idempotency\n-        aws.ensure_manifest_persistent(manifest_id, true).unwrap();\n-\n-        aws.release_entry_id(manifest_id, guard);\n+        aws.ensure_manifest_persistent(manifest_id).await.unwrap();\n     }\n \n     #[rstest]\n     #[case(false, false)]\n     #[case(false, true)]\n     #[case(true, false)]\n     #[case(true, true)]\n-    fn test_chunk_clearing(\n-        alice_workspace_storage: TmpWorkspaceStorage,\n+    #[tokio::test]\n+    async fn test_chunk_clearing(\n+        #[future] alice_workspace_storage: TmpWorkspaceStorage,\n         #[case] cache_only: bool,\n         #[case] clear_manifest: bool,\n     ) {\n-        let aws = alice_workspace_storage;\n-        let mut _manifest = create_file_manifest(&aws.device);\n+        let aws = alice_workspace_storage.await;\n+        let mut file_manifest = create_file_manifest(&aws.device);\n         let data1 = b\"abc\";\n         let chunk1 = Chunk::new(0, NonZeroU64::new(3).unwrap());\n         let data2 = b\"def\";\n         let chunk2 = Chunk::new(3, NonZeroU64::new(6).unwrap());\n-        _manifest.blocks = vec![vec![chunk1.clone(), chunk2.clone()]];\n-        _manifest.size = 6;\n-        let manifest = LocalManifest::File(_manifest.clone());\n-        let manifest_id = manifest.id();\n-\n-        let mutex = aws.lock_entry_id(manifest_id);\n-        let guard = mutex.lock().unwrap();\n+        file_manifest.blocks = vec![vec![chunk1.clone(), chunk2.clone()]];\n+        file_manifest.size = 6;\n+        let manifest_id = file_manifest.base.id;\n+        let manifest = LocalFileOrFolderManifest::File(file_manifest.clone());\n \n         // Set chunks and manifest\n-        aws.set_chunk(chunk1.id, data1).unwrap();\n-        aws.set_chunk(chunk2.id, data2).unwrap();\n-        aws.set_manifest(manifest_id, manifest, false, true, None)\n+        aws.set_chunk(chunk1.id, data1).await.unwrap();\n+        aws.set_chunk(chunk2.id, data2).await.unwrap();\n+        aws.set_manifest(manifest_id, manifest, false, None)\n+            .await\n             .unwrap();\n \n         // Set a new version of the manifest without the chunks\n         let removed_ids = HashSet::from([\n             ChunkOrBlockID::ChunkID(chunk1.id),\n             ChunkOrBlockID::ChunkID(chunk2.id),\n         ]);\n-        _manifest.blocks.clear();\n-        let new_manifest = LocalManifest::File(_manifest.clone());\n+        file_manifest.blocks.clear();\n+        let new_manifest = LocalFileOrFolderManifest::File(file_manifest.clone());\n \n-        aws.set_manifest(\n-            manifest_id,\n-            new_manifest,\n-            cache_only,\n-            true,\n-            Some(removed_ids),\n-        )\n-        .unwrap();\n+        aws.set_manifest(manifest_id, new_manifest, cache_only, Some(removed_ids))\n+            .await\n+            .unwrap();\n \n         if cache_only {\n             // The chunks are still accessible\n-            assert_eq!(aws.get_chunk(chunk1.id).unwrap(), b\"abc\");\n-            assert_eq!(aws.get_chunk(chunk2.id).unwrap(), b\"def\");\n+            assert_eq!(aws.get_chunk(chunk1.id).await.unwrap(), b\"abc\");\n+            assert_eq!(aws.get_chunk(chunk2.id).await.unwrap(), b\"def\");\n         } else {\n             // The chunks are gone\n             assert_eq!(\n-                aws.get_chunk(chunk1.id).unwrap_err(),\n+                aws.get_chunk(chunk1.id).await.unwrap_err(),\n                 FSError::LocalMiss(*chunk1.id)\n             );\n             assert_eq!(\n-                aws.get_chunk(chunk2.id).unwrap_err(),\n+                aws.get_chunk(chunk2.id).await.unwrap_err(),\n                 FSError::LocalMiss(*chunk2.id)\n             );\n         }\n \n         // Now flush the manifest\n         if clear_manifest {\n-            aws.clear_manifest(manifest_id, true).unwrap();\n+            aws.clear_manifest(&manifest_id).await.unwrap();\n         } else {\n-            aws.ensure_manifest_persistent(manifest_id, true).unwrap();\n+            aws.ensure_manifest_persistent(manifest_id).await.unwrap();\n         }\n \n         // The chunks are gone\n         assert_eq!(\n-            aws.get_chunk(chunk1.id).unwrap_err(),\n+            aws.get_chunk(chunk1.id).await.unwrap_err(),\n             FSError::LocalMiss(*chunk1.id)\n         );\n         assert_eq!(\n-            aws.get_chunk(chunk2.id).unwrap_err(),\n+            aws.get_chunk(chunk2.id).await.unwrap_err(),\n             FSError::LocalMiss(*chunk2.id)\n         );\n \n         // Idempotency\n-        aws.ensure_manifest_persistent(manifest_id, true).unwrap();\n-\n-        aws.release_entry_id(manifest_id, guard);\n+        aws.ensure_manifest_persistent(manifest_id).await.unwrap();\n     }\n \n     #[rstest]\n-    fn test_cache_flushed_on_exit(alice: &Device, tmp_path: TmpPath) {\n+    #[tokio::test]\n+    async fn test_cache_flushed_on_exit(alice: &Device, tmp_path: TmpPath) {\n         let db_path = tmp_path.join(\"workspace_storage.sqlite\");\n-        let manifest = create_workspace_manifest(&alice.local_device());\n-        let manifest = LocalManifest::Workspace(manifest);\n-        let manifest_id = manifest.id();\n+        let manifest = create_file_manifest(&alice.local_device());\n+        let manifest_id = manifest.base.id;\n+        let manifest = LocalFileOrFolderManifest::File(manifest);\n         let workspace_id = EntryID::default();\n \n         let aws = WorkspaceStorage::new(\n             Path::new(&db_path),\n             alice.local_device(),\n             workspace_id,\n+            FAILSAFE_PATTERN_FILTER.clone(),\n+            DEFAULT_CHUNK_VACUUM_THRESHOLD,\n             DEFAULT_WORKSPACE_STORAGE_CACHE_SIZE,\n         )\n+        .await\n         .unwrap();\n \n-        let mutex = aws.lock_entry_id(manifest_id);\n-        let guard = mutex.lock().unwrap();\n-\n-        aws.set_manifest(manifest_id, manifest.clone(), true, true, None)\n+        aws.set_manifest(manifest_id, manifest.clone(), true, None)\n+            .await\n             .unwrap();\n \n-        aws.release_entry_id(manifest_id, guard);\n-\n-        drop(aws);\n+        aws.clear_memory_cache(true).await.unwrap();\n+        aws.close_connections();\n \n         let aws = WorkspaceStorage::new(\n             Path::new(&db_path),\n             alice.local_device(),\n             workspace_id,\n+            FAILSAFE_PATTERN_FILTER.clone(),\n+            DEFAULT_CHUNK_VACUUM_THRESHOLD,\n             DEFAULT_WORKSPACE_STORAGE_CACHE_SIZE,\n         )\n+        .await\n         .unwrap();\n \n-        assert_eq!(aws.get_manifest(manifest_id).unwrap(), manifest);\n+        assert_eq!(\n+            aws.get_manifest(manifest_id).await.unwrap(),\n+            manifest.into()\n+        );\n     }\n \n     #[rstest]\n-    fn test_clear_cache(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n-        let manifest1 = create_workspace_manifest(&aws.device);\n-        let manifest1 = LocalManifest::Workspace(manifest1);\n-        let manifest2 = create_workspace_manifest(&aws.device);\n-        let manifest2 = LocalManifest::Workspace(manifest2);\n-        let manifest1_id = manifest1.id();\n-        let manifest2_id = manifest2.id();\n-\n-        let mutex1 = aws.lock_entry_id(manifest1_id);\n-        let mutex2 = aws.lock_entry_id(manifest2_id);\n-        let guard1 = mutex1.lock().unwrap();\n-        let guard2 = mutex2.lock().unwrap();\n+    #[tokio::test]\n+    async fn test_clear_cache(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        let aws = alice_workspace_storage.await;\n+        let manifest1 = create_file_manifest(&aws.device);\n+        let manifest1_id = manifest1.base.id;\n+        let manifest1 = LocalFileOrFolderManifest::File(manifest1);\n+        let gen_manifest1 = manifest1.clone().into();\n+        let manifest2 = create_file_manifest(&aws.device);\n+        let manifest2_id = manifest2.base.id;\n+        let manifest2 = LocalFileOrFolderManifest::File(manifest2);\n+        let gen_manifest2 = manifest2.clone().into();\n \n         // Set manifest 1 and manifest 2, cache only\n-        aws.set_manifest(manifest1_id, manifest1.clone(), false, true, None)\n+        aws.set_manifest(manifest1_id, manifest1, false, None)\n+            .await\n             .unwrap();\n-        aws.set_manifest(manifest2_id, manifest2.clone(), true, true, None)\n+        aws.set_manifest(manifest2_id, manifest2.clone(), true, None)\n+            .await\n             .unwrap();\n \n         // Clear without flushing\n-        aws.clear_memory_cache(false).unwrap();\n+        aws.clear_memory_cache(false).await.unwrap();\n \n         // Manifest 1 is present but manifest2 got lost\n-        assert_eq!(aws.get_manifest(manifest1_id).unwrap(), manifest1);\n+        assert_eq!(aws.get_manifest(manifest1_id).await.unwrap(), gen_manifest1);\n         assert_eq!(\n-            aws.get_manifest(manifest2_id).unwrap_err(),\n+            aws.get_manifest(manifest2_id).await.unwrap_err(),\n             FSError::LocalMiss(*manifest2_id)\n         );\n \n         // Set Manifest 2, cache only\n-        aws.set_manifest(manifest2_id, manifest2.clone(), true, true, None)\n+        aws.set_manifest(manifest2_id, manifest2, true, None)\n+            .await\n             .unwrap();\n \n         // Clear with flushing\n-        aws.clear_memory_cache(true).unwrap();\n-        assert_eq!(aws.get_manifest(manifest2_id).unwrap(), manifest2);\n-\n-        aws.release_entry_id(manifest1_id, guard1);\n-        aws.release_entry_id(manifest2_id, guard2);\n+        aws.clear_memory_cache(true).await.unwrap();\n+        assert_eq!(aws.get_manifest(manifest2_id).await.unwrap(), gen_manifest2);\n     }\n \n     #[rstest]\n-    fn test_serialize_non_empty_local_file_manifest(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n-        let mut manifest = create_file_manifest(&aws.device);\n+    #[tokio::test]\n+    async fn test_serialize_non_empty_local_file_manifest(\n+        #[future] alice_workspace_storage: TmpWorkspaceStorage,\n+    ) {\n+        let aws = alice_workspace_storage.await;\n+        let mut file_manifest = create_file_manifest(&aws.device);\n         let chunk1 = Chunk::new(0, NonZeroU64::try_from(7).unwrap())\n             .evolve_as_block(b\"0123456\")\n             .unwrap();\n         let chunk2 = Chunk::new(7, NonZeroU64::try_from(8).unwrap());\n         let chunk3 = Chunk::new(8, NonZeroU64::try_from(10).unwrap());\n         let blocks = vec![vec![chunk1, chunk2], vec![chunk3]];\n-        manifest.size = 10;\n-        manifest.blocks = blocks;\n-        manifest.blocksize = Blocksize::try_from(8).unwrap();\n-        manifest.assert_integrity();\n-        let manifest = LocalManifest::File(manifest);\n-        let manifest_id = manifest.id();\n-\n-        let mutex = aws.lock_entry_id(manifest_id);\n-        let guard = mutex.lock().unwrap();\n-\n-        aws.set_manifest(manifest_id, manifest.clone(), false, true, None)\n+        file_manifest.size = 10;\n+        file_manifest.blocks = blocks;\n+        file_manifest.blocksize = Blocksize::try_from(8).unwrap();\n+        file_manifest.assert_integrity();\n+        let manifest_id = file_manifest.base.id;\n+        let manifest = LocalFileOrFolderManifest::File(file_manifest);\n+        let gen_manifest = manifest.clone().into();\n+\n+        aws.set_manifest(manifest_id, manifest, false, None)\n+            .await\n             .unwrap();\n-        assert_eq!(aws.get_manifest(manifest_id).unwrap(), manifest);\n-\n-        aws.release_entry_id(manifest_id, guard);\n+        assert_eq!(aws.get_manifest(manifest_id).await.unwrap(), gen_manifest);\n     }\n \n     #[rstest]\n-    fn test_realm_checkpoint(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n+    #[tokio::test]\n+    async fn test_realm_checkpoint(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        let aws = alice_workspace_storage.await;\n         let mut manifest = create_file_manifest(&aws.device);\n         let manifest_id = manifest.base.id;\n \n-        assert_eq!(aws.get_realm_checkpoint(), 0);\n+        assert_eq!(aws.get_realm_checkpoint().await, 0);\n         // Workspace storage starts with a speculative workspace manifest placeholder\n         assert_eq!(\n-            aws.get_need_sync_entries().unwrap(),\n+            aws.get_need_sync_entries().await.unwrap(),\n             (HashSet::from([aws.workspace_id]), HashSet::new())\n         );\n \n-        let mut workspace_manifest = create_workspace_manifest(&aws.device);\n+        let mut workspace_manifest = create_workspace_manifest(&aws.device, aws.workspace_id);\n         let base = workspace_manifest.to_remote(aws.device.device_id.clone(), aws.device.now());\n         workspace_manifest.base = base;\n         workspace_manifest.need_sync = false;\n-        let workspace_manifest = LocalManifest::Workspace(workspace_manifest);\n-        aws.set_manifest(aws.workspace_id, workspace_manifest, false, false, None)\n+        aws.set_workspace_manifest(workspace_manifest)\n+            .await\n             .unwrap();\n \n-        assert_eq!(aws.get_realm_checkpoint(), 0);\n+        assert_eq!(aws.get_realm_checkpoint().await, 0);\n         assert_eq!(\n-            aws.get_need_sync_entries().unwrap(),\n+            aws.get_need_sync_entries().await.unwrap(),\n             (HashSet::new(), HashSet::new())\n         );\n \n         aws.update_realm_checkpoint(11, &[(manifest_id, 22), (EntryID::default(), 33)])\n+            .await\n             .unwrap();\n \n-        assert_eq!(aws.get_realm_checkpoint(), 11);\n+        assert_eq!(aws.get_realm_checkpoint().await, 11);\n         assert_eq!(\n-            aws.get_need_sync_entries().unwrap(),\n+            aws.get_need_sync_entries().await.unwrap(),\n             (HashSet::new(), HashSet::new())\n         );\n \n         aws.set_manifest(\n             manifest_id,\n-            LocalManifest::File(manifest.clone()),\n-            false,\n+            LocalFileOrFolderManifest::File(manifest.clone()),\n             false,\n             None,\n         )\n+        .await\n         .unwrap();\n \n-        assert_eq!(aws.get_realm_checkpoint(), 11);\n+        assert_eq!(aws.get_realm_checkpoint().await, 11);\n         assert_eq!(\n-            aws.get_need_sync_entries().unwrap(),\n+            aws.get_need_sync_entries().await.unwrap(),\n             (HashSet::from([manifest_id]), HashSet::new())\n         );\n \n         manifest.need_sync = false;\n         aws.set_manifest(\n             manifest_id,\n-            LocalManifest::File(manifest),\n-            false,\n+            LocalFileOrFolderManifest::File(manifest),\n             false,\n             None,\n         )\n+        .await\n         .unwrap();\n \n-        assert_eq!(aws.get_realm_checkpoint(), 11);\n+        assert_eq!(aws.get_realm_checkpoint().await, 11);\n         assert_eq!(\n-            aws.get_need_sync_entries().unwrap(),\n+            aws.get_need_sync_entries().await.unwrap(),\n             (HashSet::new(), HashSet::new())\n         );\n \n         aws.update_realm_checkpoint(44, &[(manifest_id, 55), (EntryID::default(), 66)])\n+            .await\n             .unwrap();\n \n-        assert_eq!(aws.get_realm_checkpoint(), 44);\n+        assert_eq!(aws.get_realm_checkpoint().await, 44);\n         assert_eq!(\n-            aws.get_need_sync_entries().unwrap(),\n+            aws.get_need_sync_entries().await.unwrap(),\n             (HashSet::new(), HashSet::from([manifest_id]))\n         );\n     }\n \n     #[rstest]\n-    fn test_block_interface(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n+    #[tokio::test]\n+    async fn test_block_interface(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        let aws = alice_workspace_storage.await;\n         let data = b\"0123456\";\n         let chunk = Chunk::new(0, NonZeroU64::try_from(7).unwrap())\n             .evolve_as_block(data)\n             .unwrap();\n         let block_id = chunk.access.unwrap().id;\n \n-        aws.clear_clean_block(block_id);\n+        aws.clear_clean_block(block_id).await;\n \n         assert_eq!(\n-            aws.get_chunk(chunk.id).unwrap_err(),\n+            aws.get_chunk(chunk.id).await.unwrap_err(),\n             FSError::LocalMiss(*chunk.id)\n         );\n-        assert!(!aws.block_storage.is_chunk(chunk.id).unwrap());\n-        assert_eq!(aws.block_storage.get_total_size().unwrap(), 0);\n+        assert!(!aws.block_storage.is_chunk(chunk.id).await.unwrap());\n+        assert_eq!(aws.block_storage.get_total_size().await.unwrap(), 0);\n \n-        aws.set_clean_block(block_id, data).unwrap();\n-        assert_eq!(aws.get_chunk(chunk.id).unwrap(), data);\n-        assert!(aws.block_storage.is_chunk(chunk.id).unwrap());\n-        assert!(aws.block_storage.get_total_size().unwrap() >= 7);\n+        aws.set_clean_block(block_id, data).await.unwrap();\n+        assert_eq!(aws.get_chunk(chunk.id).await.unwrap(), data);\n+        assert!(aws.block_storage.is_chunk(chunk.id).await.unwrap());\n+        assert!(aws.block_storage.get_total_size().await.unwrap() >= 7);\n \n-        aws.clear_clean_block(block_id);\n+        aws.clear_clean_block(block_id).await;\n         assert_eq!(\n-            aws.get_chunk(chunk.id).unwrap_err(),\n+            aws.get_chunk(chunk.id).await.unwrap_err(),\n             FSError::LocalMiss(*chunk.id)\n         );\n-        assert!(!aws.block_storage.is_chunk(chunk.id).unwrap());\n-        assert_eq!(aws.block_storage.get_total_size().unwrap(), 0);\n+        assert!(!aws.block_storage.is_chunk(chunk.id).await.unwrap());\n+        assert_eq!(aws.block_storage.get_total_size().await.unwrap(), 0);\n \n-        aws.set_chunk(chunk.id, data).unwrap();\n-        assert_eq!(aws.get_dirty_block(block_id).unwrap(), data);\n+        aws.set_chunk(chunk.id, data).await.unwrap();\n+        assert_eq!(aws.get_dirty_block(block_id).await.unwrap(), data);\n     }\n \n     #[rstest]\n-    fn test_chunk_interface(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n+    #[tokio::test]\n+    async fn test_chunk_interface(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        let aws = alice_workspace_storage.await;\n         let data = b\"0123456\";\n         let chunk = Chunk::new(0, NonZeroU64::try_from(7).unwrap());\n \n         assert_eq!(\n-            aws.get_chunk(chunk.id).unwrap_err(),\n+            aws.get_chunk(chunk.id).await.unwrap_err(),\n             FSError::LocalMiss(*chunk.id)\n         );\n         assert_eq!(\n-            aws.clear_chunk(chunk.id, false).unwrap_err(),\n+            aws.clear_chunk(chunk.id, false).await.unwrap_err(),\n             FSError::LocalMiss(*chunk.id)\n         );\n-        aws.clear_chunk(chunk.id, true).unwrap();\n-        assert!(!aws.chunk_storage.is_chunk(chunk.id).unwrap());\n-        assert_eq!(aws.chunk_storage.get_total_size().unwrap(), 0);\n+        aws.clear_chunk(chunk.id, true).await.unwrap();\n+        assert!(!aws.chunk_storage.is_chunk(chunk.id).await.unwrap());\n+        assert_eq!(aws.chunk_storage.get_total_size().await.unwrap(), 0);\n \n-        aws.set_chunk(chunk.id, data).unwrap();\n-        assert_eq!(aws.get_chunk(chunk.id).unwrap(), data);\n-        assert!(aws.chunk_storage.is_chunk(chunk.id).unwrap());\n-        assert!(aws.chunk_storage.get_total_size().unwrap() >= 7);\n+        aws.set_chunk(chunk.id, data).await.unwrap();\n+        assert_eq!(aws.get_chunk(chunk.id).await.unwrap(), data);\n+        assert!(aws.chunk_storage.is_chunk(chunk.id).await.unwrap());\n+        assert!(aws.chunk_storage.get_total_size().await.unwrap() >= 7);\n \n-        aws.clear_chunk(chunk.id, false).unwrap();\n+        aws.clear_chunk(chunk.id, false).await.unwrap();\n         assert_eq!(\n-            aws.get_chunk(chunk.id).unwrap_err(),\n+            aws.get_chunk(chunk.id).await.unwrap_err(),\n             FSError::LocalMiss(*chunk.id)\n         );\n         assert_eq!(\n-            aws.clear_chunk(chunk.id, false).unwrap_err(),\n+            aws.clear_chunk(chunk.id, false).await.unwrap_err(),\n             FSError::LocalMiss(*chunk.id)\n         );\n-        assert!(!aws.chunk_storage.is_chunk(chunk.id).unwrap());\n-        assert_eq!(aws.chunk_storage.get_total_size().unwrap(), 0);\n-        aws.clear_chunk(chunk.id, true).unwrap();\n+        assert!(!aws.chunk_storage.is_chunk(chunk.id).await.unwrap());\n+        assert_eq!(aws.chunk_storage.get_total_size().await.unwrap(), 0);\n+        aws.clear_chunk(chunk.id, true).await.unwrap();\n     }\n \n     #[rstest]\n-    fn test_chunk_many(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n+    #[tokio::test]\n+    async fn test_chunk_many(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        let aws = alice_workspace_storage.await;\n         let data = b\"0123456\";\n \n         // More than the sqlite max argument limit to prevent regression\n@@ -875,55 +1072,61 @@ mod tests {\n         for _ in 0..chunks_number {\n             let c = Chunk::new(0, NonZeroU64::try_from(7).unwrap());\n             chunks.push(c.id);\n-            aws.chunk_storage.set_chunk(c.id, data).unwrap();\n+            aws.chunk_storage.set_chunk(c.id, data).await.unwrap();\n         }\n \n         assert_eq!(chunks.len(), chunks_number);\n-        let ret = aws.get_local_chunk_ids(&chunks).unwrap();\n+        let ret = aws.get_local_chunk_ids(&chunks).await.unwrap();\n         assert_eq!(ret.len(), chunks_number);\n     }\n \n     #[rstest]\n-    fn test_file_descriptor(alice_workspace_storage: TmpWorkspaceStorage) {\n-        let aws = alice_workspace_storage;\n+    #[tokio::test]\n+    async fn test_file_descriptor(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        let aws = alice_workspace_storage.await;\n         let manifest = create_file_manifest(&aws.device);\n         let manifest_id = manifest.base.id;\n \n         aws.set_manifest(\n             manifest_id,\n-            LocalManifest::File(manifest.clone()),\n-            false,\n+            LocalFileOrFolderManifest::File(manifest.clone()),\n             false,\n             None,\n         )\n+        .await\n         .unwrap();\n         let fd = aws.create_file_descriptor(manifest.clone());\n         assert_eq!(fd, FileDescriptor(1));\n \n-        assert_eq!(aws.load_file_descriptor(fd).unwrap(), manifest);\n+        assert_eq!(aws.load_file_descriptor(fd).await.unwrap(), manifest);\n \n         aws.remove_file_descriptor(fd);\n         assert_eq!(\n-            aws.load_file_descriptor(fd).unwrap_err(),\n+            aws.load_file_descriptor(fd).await.unwrap_err(),\n             FSError::InvalidFileDescriptor(fd)\n         );\n         assert_eq!(aws.remove_file_descriptor(fd), None);\n     }\n \n     #[rstest]\n-    fn test_run_vacuum(alice_workspace_storage: TmpWorkspaceStorage) {\n-        alice_workspace_storage.run_vacuum().unwrap();\n+    #[tokio::test]\n+    async fn test_run_vacuum(#[future] alice_workspace_storage: TmpWorkspaceStorage) {\n+        alice_workspace_storage.await.run_vacuum().await.unwrap();\n     }\n \n     #[rstest]\n-    fn test_garbage_collection(alice: &Device, tmp_path: TmpPath) {\n+    #[tokio::test]\n+    async fn test_garbage_collection(alice: &Device, tmp_path: TmpPath) {\n         let db_path = tmp_path.join(\"workspace_storage.sqlite\");\n         let aws = WorkspaceStorage::new(\n             Path::new(&db_path),\n             alice.local_device(),\n             EntryID::default(),\n+            FAILSAFE_PATTERN_FILTER.clone(),\n+            DEFAULT_CHUNK_VACUUM_THRESHOLD,\n             *DEFAULT_BLOCK_SIZE,\n         )\n+        .await\n         .unwrap();\n \n         let block_size = NonZeroU64::try_from(*DEFAULT_BLOCK_SIZE).unwrap();\n@@ -932,17 +1135,91 @@ mod tests {\n         let chunk2 = Chunk::new(0, block_size).evolve_as_block(&data).unwrap();\n         let chunk3 = Chunk::new(0, block_size).evolve_as_block(&data).unwrap();\n \n-        assert_eq!(aws.block_storage.get_nb_blocks().unwrap(), 0);\n+        assert_eq!(aws.block_storage.get_nb_blocks().await.unwrap(), 0);\n         aws.set_clean_block(chunk1.access.unwrap().id, &data)\n+            .await\n             .unwrap();\n-        assert_eq!(aws.block_storage.get_nb_blocks().unwrap(), 1);\n+        assert_eq!(aws.block_storage.get_nb_blocks().await.unwrap(), 1);\n         aws.set_clean_block(chunk2.access.unwrap().id, &data)\n+            .await\n             .unwrap();\n-        assert_eq!(aws.block_storage.get_nb_blocks().unwrap(), 1);\n+        assert_eq!(aws.block_storage.get_nb_blocks().await.unwrap(), 1);\n         aws.set_clean_block(chunk3.access.unwrap().id, &data)\n+            .await\n             .unwrap();\n-        assert_eq!(aws.block_storage.get_nb_blocks().unwrap(), 1);\n-        aws.block_storage.clear_all_blocks().unwrap();\n-        assert_eq!(aws.block_storage.get_nb_blocks().unwrap(), 0);\n+        assert_eq!(aws.block_storage.get_nb_blocks().await.unwrap(), 1);\n+        aws.block_storage.clear_all_blocks().await.unwrap();\n+        assert_eq!(aws.block_storage.get_nb_blocks().await.unwrap(), 0);\n+    }\n+\n+    #[rstest]\n+    #[tokio::test]\n+    async fn test_invalid_regex(tmp_path: TmpPath, alice: &Device) {\n+        use crate::storage::manifest_storage::prevent_sync_pattern::dsl::*;\n+        use diesel::{BoolExpressionMethods, ExpressionMethods, QueryDsl, RunQueryDsl};\n+\n+        const INVALID_REGEX: &str = \"[\";\n+        let wid = EntryID::default();\n+        let valid_regex = Regex::from_regex_str(\"ok\").unwrap();\n+\n+        let db_dir = tmp_path.join(\"invalid-regex\");\n+        let db_path = get_workspace_data_storage_db_path(&db_dir, &alice.local_device(), wid);\n+        let conn = LocalDatabase::from_path(db_path.to_str().unwrap(), VacuumMode::default())\n+            .await\n+            .unwrap();\n+\n+        let workspace_storage = WorkspaceStorage::new(\n+            db_dir.clone(),\n+            alice.local_device(),\n+            wid,\n+            FAILSAFE_PATTERN_FILTER.clone(),\n+            DEFAULT_CHUNK_VACUUM_THRESHOLD,\n+            *DEFAULT_BLOCK_SIZE,\n+        )\n+        .await\n+        .unwrap();\n+\n+        // Ensure the entry is present.\n+        workspace_storage\n+            .set_prevent_sync_pattern(&valid_regex)\n+            .await\n+            .unwrap();\n+\n+        // Corrupt the db with an invalid regex.\n+        conn.exec(|conn| {\n+            diesel::update(prevent_sync_pattern.filter(_id.eq(0).and(pattern.ne(INVALID_REGEX))))\n+                .set((pattern.eq(INVALID_REGEX), fully_applied.eq(false)))\n+                .execute(conn)\n+        })\n+        .await\n+        .unwrap();\n+\n+        drop(workspace_storage);\n+        drop(conn);\n+\n+        let workspace_storage = WorkspaceStorage::new(\n+            db_dir,\n+            alice.local_device(),\n+            wid,\n+            FAILSAFE_PATTERN_FILTER.clone(),\n+            DEFAULT_CHUNK_VACUUM_THRESHOLD,\n+            *DEFAULT_BLOCK_SIZE,\n+        )\n+        .await\n+        .unwrap();\n+        workspace_storage.get_prevent_sync_pattern();\n+    }\n+\n+    #[rstest]\n+    #[tokio::test]\n+    #[should_panic]\n+    async fn inserting_different_workspace_manifest(\n+        #[future] alice_workspace_storage: TmpWorkspaceStorage,\n+    ) {\n+        let aws = alice_workspace_storage.await;\n+        let workspace_manifest = create_workspace_manifest(&aws.device, EntryID::default());\n+\n+        // Should panic because we insert a workspace manifest which id is different than `aws.workspace_id`\n+        let _ = aws.set_workspace_manifest(workspace_manifest).await;\n     }\n }"
    },
    {
      "filename": "oxidation/libparsec/crates/platform_async/src/lib.rs",
      "status": "modified",
      "patch": "@@ -11,10 +11,14 @@ pub mod wasm32;\n pub use wasm32 as platform;\n \n pub use flume as channel;\n-pub use futures::{lock::Mutex, prelude::*, select};\n+#[cfg(target_arch = \"wasm32\")]\n+pub use futures::lock::{Mutex, MutexGuard};\n+pub use futures::{self, prelude::*, select};\n pub use platform::{\n     join_set::JoinSet,\n     sleep,\n     sync::{watch, Notify},\n     task::{spawn, Task},\n };\n+#[cfg(not(target_arch = \"wasm32\"))]\n+pub use tokio::sync::{Mutex, MutexGuard, RwLock, RwLockReadGuard, RwLockWriteGuard, TryLockError};"
    },
    {
      "filename": "oxidation/libparsec/crates/platform_local_db/Cargo.toml",
      "status": "added",
      "patch": "@@ -0,0 +1,26 @@\n+[package]\n+name = \"libparsec_platform_local_db\"\n+version = \"0.0.0\"\n+edition = \"2021\"\n+\n+# See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html\n+\n+[features]\n+# Enable to open database in memory.\n+test-utils = [\"lazy_static\"]\n+\n+[dependencies]\n+async-trait = \"0.1.61\"\n+tokio = { version = \"1.24.1\", features = [\"fs\", \"sync\"] }\n+thiserror = \"1.0.37\"\n+diesel = { version = \"2.0.2\", features = [\"sqlite\", \"returning_clauses_for_sqlite_3_35\"], default-features = false }\n+# We add this dependency to have sqlite3 bundled into our code.\n+libsqlite3-sys = { version = \"0.25.2\", features = [\"bundled\"] }\n+libparsec_platform_async = { path = \"../platform_async\" }\n+lazy_static = { version = \"1.4.0\", optional = true }\n+\n+[dev-dependencies]\n+libparsec_tests_fixtures = { path = \"../tests_fixtures\" }\n+pretty_assertions = \"1.3.0\"\n+\n+rstest = \"0.16.0\""
    },
    {
      "filename": "oxidation/libparsec/crates/platform_local_db/src/error.rs",
      "status": "added",
      "patch": "@@ -0,0 +1,56 @@\n+// Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+//! Module that contain code about error manipulation of `local-db`.\n+\n+/// Possible errors when manipulating the database.\n+#[derive(thiserror::Error, Debug)]\n+pub enum DatabaseError {\n+    /// The database has been closed.\n+    #[error(\"Database is closed\")]\n+    Closed,\n+    /// The database returned an error.\n+    #[error(\"{}\", .1.message())]\n+    DieselDatabaseError(\n+        diesel::result::DatabaseErrorKind,\n+        Box<dyn diesel::result::DatabaseErrorInformation + Send + Sync>,\n+    ),\n+    /// Diesel generated an error\n+    #[error(\"{0}\")]\n+    Diesel(diesel::result::Error),\n+    /// Diesel cannot connect to the database.\n+    #[error(\"{0}\")]\n+    DieselConnectionError(diesel::result::ConnectionError),\n+}\n+\n+impl PartialEq for DatabaseError {\n+    fn eq(&self, other: &Self) -> bool {\n+        use DatabaseError::*;\n+\n+        match (self, other) {\n+            (Closed, Closed) => true,\n+            (DieselConnectionError(left), DieselConnectionError(right)) => left.eq(right),\n+            (Diesel(left), Diesel(right)) => left.eq(right),\n+            (DieselDatabaseError(_, left), DieselDatabaseError(_, right)) => {\n+                left.message() == right.message()\n+            }\n+            _ => false,\n+        }\n+    }\n+}\n+\n+impl From<diesel::result::Error> for DatabaseError {\n+    fn from(e: diesel::result::Error) -> Self {\n+        match e {\n+            diesel::result::Error::DatabaseError(kind, err) => Self::DieselDatabaseError(kind, err),\n+            _ => Self::Diesel(e),\n+        }\n+    }\n+}\n+\n+impl From<diesel::result::ConnectionError> for DatabaseError {\n+    fn from(e: diesel::result::ConnectionError) -> Self {\n+        Self::DieselConnectionError(e)\n+    }\n+}\n+\n+/// A result wrapper that return [DatabaseError] on error.\n+pub type DatabaseResult<T> = Result<T, DatabaseError>;"
    },
    {
      "filename": "oxidation/libparsec/crates/platform_local_db/src/executor.rs",
      "status": "added",
      "patch": "@@ -0,0 +1,240 @@\n+// Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+//! Module that wrap an [diesel::SqliteConnection] behind a executor to allow to have an async manner to executor sql queries.\n+\n+use std::thread::JoinHandle;\n+\n+use diesel::{connection::SimpleConnection, SqliteConnection};\n+use libparsec_platform_async::channel;\n+\n+use crate::{DatabaseError, DatabaseResult};\n+\n+/// The executor that manage and send job to the background executor.\n+pub(crate) struct SqliteExecutor {\n+    /// The channel that will be used to send job to the background executor.\n+    job_sender: Option<channel::Sender<Operation>>,\n+    /// The handle to the [BackgroundSqliteExecutor].\n+    handle: Option<JoinHandle<()>>,\n+}\n+\n+/// A list of operation that can be executed on the background executor.\n+enum Operation {\n+    /// Operate a full vacuum, meaning executing `VACUUM` on the sqlite connection\n+    /// and re-opening the connection to force cleanup of the driver.\n+    FullVacuum(channel::Sender<DatabaseResult<()>>),\n+    /// Execute a standard job.\n+    Job(JobFunc),\n+}\n+\n+/// A type alias for function that will be sent to the background executor.\n+type JobFunc = Box<dyn FnOnce(&mut SqliteConnection) + Send>;\n+\n+impl SqliteExecutor {\n+    /// Spawn the executor in a thread.\n+    pub fn spawn<F>(connection: SqliteConnection, reopen_connection: F) -> Self\n+    where\n+        F: Send + (Fn(SqliteConnection) -> DatabaseResult<SqliteConnection>) + 'static,\n+    {\n+        let (job_sender, job_receiver) = channel::bounded(32);\n+        let background_executor = BackgroundSqliteExecutor {\n+            job_receiver,\n+            connection,\n+        };\n+        // TODO: currently if the thread panic the error is printed to stderr,\n+        // we should instead have a proper panic handler that log an error\n+        let handle = std::thread::Builder::new()\n+            .name(\"SqliteExecutor\".to_string())\n+            .spawn(move || background_executor.serve(reopen_connection))\n+            .expect(\"failed to spawn thread\");\n+\n+        Self {\n+            job_sender: Some(job_sender),\n+            handle: Some(handle),\n+        }\n+    }\n+\n+    /// Execute the provided closure to execute a query on the sqlite connection.\n+    /// Will return the result when finished.\n+    pub fn exec<F, R>(&self, job: F) -> ExecJob<R>\n+    where\n+        F: (FnOnce(&mut SqliteConnection) -> R) + Send + 'static,\n+        R: Send + 'static,\n+    {\n+        let (tx, rx) = channel::bounded::<R>(1);\n+        let wrapped_job = move |conn: &mut SqliteConnection| {\n+            let res = job(conn);\n+            // If send fails it means the caller's future has been dropped\n+            // (hence dropping `rx`). In theory there is nothing wrong about\n+            // it, however we log it anyway given the caller's unexpected drop\n+            // may also be the sign of a bug...\n+            if tx.send(res).is_err() {\n+                // TODO: replace this by a proper warning log !\n+                eprintln!(\"Caller has left\");\n+            }\n+        };\n+        let wrapped_job = Box::new(wrapped_job);\n+\n+        let sender = self\n+            .job_sender\n+            .as_ref()\n+            .expect(\"Job sender cannot be none before calling `drop`\");\n+\n+        ExecJob {\n+            job: Operation::Job(wrapped_job),\n+            sender: sender.clone(),\n+            result_recv: rx,\n+        }\n+    }\n+\n+    /// Run a full vacuum, meaning that it will execute a standar vacuum and re-open the database connection to force cleanup.\n+    pub fn full_vacuum(&self) -> ExecJob<DatabaseResult<()>> {\n+        let (tx, rx) = channel::bounded(1);\n+        ExecJob {\n+            job: Operation::FullVacuum(tx),\n+            sender: self\n+                .job_sender\n+                .as_ref()\n+                .expect(\"Job sender cannot be none before calling `drop`\")\n+                .clone(),\n+            result_recv: rx,\n+        }\n+    }\n+}\n+\n+/// The structure generated by [SqliteExecutor::exec].\n+/// You need to call [ExecJob::send] to effectively execute the job.\n+#[must_use]\n+pub(crate) struct ExecJob<R>\n+where\n+    R: Send + 'static,\n+{\n+    /// The job to execute\n+    job: Operation,\n+    /// The channel to send the job to.\n+    sender: channel::Sender<Operation>,\n+    /// The channel that will receive the result of the job.\n+    result_recv: channel::Receiver<R>,\n+}\n+\n+impl<R> ExecJob<R>\n+where\n+    R: Send + 'static,\n+{\n+    /// Send the job to the background executor and return the result when it finish.\n+    pub async fn send(self) -> DatabaseResult<R> {\n+        let ExecJob {\n+            job,\n+            sender,\n+            result_recv,\n+        } = self;\n+\n+        sender\n+            .send_async(job)\n+            .await\n+            .map_err(|_| DatabaseError::Closed)?;\n+        drop(sender);\n+\n+        result_recv\n+            .recv_async()\n+            .await\n+            .map_err(|_| DatabaseError::Closed)\n+    }\n+}\n+\n+impl Drop for SqliteExecutor {\n+    fn drop(&mut self) {\n+        drop(self.job_sender.take());\n+        if let Some(handle) = self.handle.take() {\n+            // An error is returned in case the joined thread has panicked\n+            // We can ignore the error given it should have already been\n+            // logged as part of the panic handling system.\n+            let _ = handle.join();\n+        }\n+    }\n+}\n+\n+/// The background executor that manage the sqlite connection on a separated thread.\n+struct BackgroundSqliteExecutor {\n+    /// The channel that will receive job to execute.\n+    job_receiver: channel::Receiver<Operation>,\n+    /// The connection to the sqlite database.\n+    connection: SqliteConnection,\n+}\n+\n+impl BackgroundSqliteExecutor {\n+    /// Start the background executor to listen for incoming jobs.\n+    /// This method will stop when all sender channel are closed and no more job are present on the channel queue.\n+    fn serve<F>(self, reopen_connection: F)\n+    where\n+        F: Fn(SqliteConnection) -> DatabaseResult<SqliteConnection>,\n+    {\n+        let BackgroundSqliteExecutor {\n+            job_receiver,\n+            mut connection,\n+        } = self;\n+        for operation in job_receiver.into_iter() {\n+            match operation {\n+                Operation::FullVacuum(res_tx) => {\n+                    // Run a full vacuum, meaning that it will execute a standar vacuum\n+                    // and re-open the database connection to force cleanup.\n+                    let res = connection\n+                        .batch_execute(\"VACUUM\")\n+                        .map_err(DatabaseError::from)\n+                        .and_then(|_| reopen_connection(connection));\n+\n+                    match res {\n+                        Ok(conn) => {\n+                            connection = conn;\n+                            res_tx\n+                                .send(Ok(()))\n+                                .expect(\"Failed to send the result of the full vacuum operation\");\n+                        }\n+                        // Oh no, we have an error, the background executor will stop just after notifying the caller.\n+                        Err(err) => {\n+                            res_tx\n+                                .send(Err(err))\n+                                .expect(\"Failed to send the result of the full vacuum operation\");\n+                            return;\n+                        }\n+                    }\n+                }\n+                Operation::Job(job) => job(&mut connection),\n+            }\n+        }\n+    }\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+    use crate::DatabaseError;\n+    use diesel::{connection::SimpleConnection, Connection, SqliteConnection};\n+\n+    use super::SqliteExecutor;\n+\n+    #[tokio::test]\n+    async fn fail_reopen_database() {\n+        let connection = SqliteConnection::establish(\":memory:\").unwrap();\n+        let executor = SqliteExecutor::spawn(connection, |_conn| Err(crate::DatabaseError::Closed));\n+\n+        // Basic SQL query to see if the Executor is working properly.\n+        executor\n+            .exec(|conn| conn.batch_execute(\"VACUUM\"))\n+            .send()\n+            .await\n+            .unwrap()\n+            .unwrap();\n+\n+        let err = executor.full_vacuum().send().await.unwrap().unwrap_err();\n+\n+        // TODO: Check that we've outputted a log warning saying we failed to reopen the database connection.\n+        assert_eq!(err, DatabaseError::Closed);\n+\n+        // Because `full_vacuum` failed, the executor should be closed.\n+        let err = executor\n+            .exec(|conn| conn.batch_execute(\"VACUUM\"))\n+            .send()\n+            .await\n+            .unwrap_err();\n+\n+        assert_eq!(err, DatabaseError::Closed);\n+    }\n+}"
    },
    {
      "filename": "oxidation/libparsec/crates/platform_local_db/src/lib.rs",
      "status": "added",
      "patch": "@@ -0,0 +1,355 @@\n+// Parsec Cloud (https://parsec.cloud) Copyright (c) BUSL-1.1 (eventually AGPL-3.0) 2016-present Scille SAS\n+//! Manage a local database connection.\n+\n+#![warn(clippy::missing_docs_in_private_items)]\n+#![warn(clippy::missing_errors_doc)]\n+#![warn(clippy::missing_panics_doc)]\n+#![warn(clippy::missing_safety_doc)]\n+#![deny(clippy::future_not_send)]\n+#![deny(clippy::undocumented_unsafe_blocks)]\n+\n+use std::{\n+    path::{Path, PathBuf},\n+    sync::Mutex,\n+};\n+\n+use diesel::{connection::SimpleConnection, sqlite::SqliteConnection, Connection};\n+use executor::{ExecJob, SqliteExecutor};\n+\n+mod error;\n+mod executor;\n+mod option;\n+#[cfg(feature = \"test-utils\")]\n+mod test_utils;\n+\n+pub use error::{DatabaseError, DatabaseResult};\n+use libparsec_platform_async::futures::TryFutureExt;\n+pub use option::AutoVacuum;\n+use option::SqliteOptions;\n+#[cfg(feature = \"test-utils\")]\n+pub use test_utils::{test_clear_local_db_in_memory_mock, test_toggle_local_db_in_memory_mock};\n+\n+/// Maximum number of parameters to be sent in a single SQL query.\n+/// In theory SQLite provide a `SQLITE_MAX_VARIABLE_NUMBER` that is:\n+///\n+/// - 999 for sqlite < 3.32.0\n+/// - 32_766 for sqlite >= 3.32.0\n+///\n+/// ref: <https://www.sqlite.org/limits.html#max_variable_number>\n+///\n+/// However this misleading given the query will most likely reach other limits\n+/// before this one when passing a huge number of params:\n+///\n+/// - `SELECT (?, ?, ...)` will reach `SQLITE_MAX_COLUMN`.\n+/// - `SELECT 1 WHERE ? AND ? AND ...` will reach `SQLITE_MAX_EXPR_DEPTH`.\n+///\n+/// So to reach `SQLITE_MAX_VARIABLE_NUMBER`, one should put multiple SQL expression\n+/// in a single execution: `SELECT (?, ?, ...); SELECT (?, ?, ...); ...`\n+pub const LOCAL_DATABASE_MAX_VARIABLE_NUMBER: usize = 999;\n+\n+/// How vacuumming should be done for the database.\n+#[derive(Debug, Clone, Copy)]\n+pub enum VacuumMode {\n+    /// Use a threshold, when the disk usage is above the provided value calling [LocalDatabase::vacuum] will reduce the disk usage.\n+    /// We use this threshold to limit how often we run a full vacuum (meaning executing `VACUUM` and re-opening the database).\n+    WithThreshold(usize),\n+    /// Configure the sqlite driver to vacuum for us.\n+    Automatic(AutoVacuum),\n+}\n+\n+impl VacuumMode {\n+    /// Return the auto vacuum value (if set).\n+    fn auto_vacuum(&self) -> Option<AutoVacuum> {\n+        if let VacuumMode::Automatic(value) = self {\n+            Some(*value)\n+        } else {\n+            None\n+        }\n+    }\n+}\n+\n+impl Default for VacuumMode {\n+    fn default() -> Self {\n+        Self::Automatic(AutoVacuum::Full)\n+    }\n+}\n+\n+/// Help manage and execute sql query to sqlite in an async manner.\n+pub struct LocalDatabase {\n+    /// The database path that was provided in [LocalDatabase::from_path].\n+    path: PathBuf,\n+    /// Flag that will be `true` when the database has been open in memory (RAM).\n+    #[cfg(feature = \"test-utils\")]\n+    is_in_memory: bool,\n+    /// The executor that will execute the sql query.\n+    executor: Mutex<Option<SqliteExecutor>>,\n+    /// How we vacuum the database.\n+    vacuum_mode: VacuumMode,\n+}\n+\n+impl LocalDatabase {\n+    /// Create a new database connection with the provided flag.\n+    ///\n+    /// Since `test-utils` is enabled, it will try to open a database in memory if we configured it to allow it.\n+    ///\n+    /// # Errors\n+    ///\n+    /// We can have an error when we can't create a database connection in memory.\n+    #[cfg(feature = \"test-utils\")]\n+    pub async fn from_path(path: &str, vacuum_mode: VacuumMode) -> DatabaseResult<Self> {\n+        let real_path = AsRef::<Path>::as_ref(&path).to_path_buf();\n+\n+        let (executor, is_in_memory) = match test_utils::maybe_open_sqlite_in_memory(&real_path) {\n+            Some(executor) => (executor, true),\n+            None => {\n+                let executor =\n+                    new_sqlite_connection_from_path(path, vacuum_mode.auto_vacuum()).await?;\n+                (executor, false)\n+            }\n+        };\n+        Ok(Self {\n+            path: real_path,\n+            is_in_memory,\n+            executor: Mutex::new(Some(executor)),\n+            vacuum_mode,\n+        })\n+    }\n+\n+    /// Create a new database connection with the provided path.\n+    ///\n+    /// # Errors\n+    ///\n+    /// We can have an error if we can't open the connection to the database.\n+    #[cfg(not(feature = \"test-utils\"))]\n+    pub async fn from_path(path: &str, vacuum_mode: VacuumMode) -> DatabaseResult<Self> {\n+        let executor = new_sqlite_connection_from_path(path, vacuum_mode.auto_vacuum()).await?;\n+        Ok(Self {\n+            path: AsRef::<Path>::as_ref(&path).to_path_buf(),\n+            executor: Mutex::new(Some(executor)),\n+            vacuum_mode,\n+        })\n+    }\n+}\n+\n+/// Create a new [SqliteExecutor] from the provided `path`.\n+async fn new_sqlite_connection_from_path(\n+    path: &str,\n+    auto_vacuum: Option<option::AutoVacuum>,\n+) -> DatabaseResult<SqliteExecutor> {\n+    if let Some(prefix) = PathBuf::from(path).parent() {\n+        tokio::fs::create_dir_all(prefix).await.map_err(|e| {\n+            diesel::result::ConnectionError::BadConnection(format!(\n+                \"Can't create sub-directory `{}`: {e}\",\n+                prefix\n+                    .to_str()\n+                    .expect(\"We generate the Path from a `&str` so UTF-8 is already checked\")\n+            ))\n+        })?;\n+    }\n+\n+    let connection = SqliteConnection::establish(path)?;\n+    let path = path.to_string();\n+    let executor = SqliteExecutor::spawn(connection, move |_conn| {\n+        SqliteConnection::establish(&path).map_err(DatabaseError::from)\n+    });\n+    let pragma_options = SqliteOptions::default()\n+        .journal_mode(option::JournalMode::Wal)\n+        .synchronous(option::Synchronous::Normal)\n+        .to_sql_batch_query();\n+\n+    executor\n+        .exec(move |conn| conn.batch_execute(&pragma_options))\n+        .send()\n+        .and_then(|_| async {\n+            if let Some(auto_vacuum) = auto_vacuum {\n+                auto_vacuum.safely_set_value(&executor).await?;\n+            }\n+            Ok(())\n+        })\n+        .await\n+        .and(Ok(executor))\n+}\n+\n+impl LocalDatabase {\n+    /// Close the actual connection to the database.\n+    pub fn close(&self) {\n+        let _executor = self.executor.lock().expect(\"Mutex is poisoned\").take();\n+        #[cfg(feature = \"test-utils\")]\n+        if let Some(executor) = _executor {\n+            if self.is_in_memory {\n+                test_utils::return_sqlite_in_memory_db(&self.path, executor);\n+            }\n+        }\n+    }\n+\n+    /// Return an approximate amount of bytes sqlite is using on the filesystem.\n+    pub async fn get_disk_usage(&self) -> usize {\n+        use std::ffi::OsStr;\n+\n+        /// Get the size of a file.\n+        async fn get_file_size(path: &Path) -> usize {\n+            tokio::fs::metadata(path)\n+                .await\n+                .map(|meta| meta.len() as usize)\n+                .ok()\n+                .unwrap_or_default()\n+        }\n+\n+        /// Add a suffix to a file extension.\n+        /// I.e.: with the suffix `-wal`, `.sqlite3` become `.sqlite3-wal`.\n+        fn add_suffix_to_extension(path: &Path, suffix: &str) -> PathBuf {\n+            let mut extension = path.extension().unwrap_or_default().to_os_string();\n+\n+            extension.extend([AsRef::<OsStr>::as_ref(suffix)]);\n+\n+            path.with_extension(extension)\n+        }\n+\n+        let wal_filename = add_suffix_to_extension(&self.path, \"-wal\");\n+        let shared_filename = add_suffix_to_extension(&self.path, \"-shm\");\n+\n+        get_file_size(&self.path).await\n+            + get_file_size(&wal_filename).await\n+            + get_file_size(&shared_filename).await\n+    }\n+\n+    /// Vacuum the database shrink it's size.\n+    /// The method will do nothing if we have configured the auto vacuum to [AutoVacuum::Full].\n+    ///\n+    /// # Errors\n+    ///\n+    /// Can fail if we can't execute the vacuum query.\n+    pub async fn vacuum(&self) -> DatabaseResult<()> {\n+        match self.vacuum_mode {\n+            VacuumMode::Automatic(_) => Ok(()),\n+            VacuumMode::WithThreshold(threshold) => {\n+                let disk_usage = self.get_disk_usage().await;\n+\n+                if disk_usage < threshold {\n+                    return Ok(());\n+                }\n+\n+                let vacuum_job = {\n+                    let guard = self.executor.lock().expect(\"Mutex is poisoned\");\n+                    guard\n+                        .as_ref()\n+                        .map(|exec| exec.full_vacuum())\n+                        .ok_or(DatabaseError::Closed)\n+                }?;\n+                let res = self.exec_job(vacuum_job).await;\n+\n+                if res.is_err() {\n+                    self.close();\n+                }\n+                res\n+            }\n+        }\n+    }\n+}\n+\n+impl Drop for LocalDatabase {\n+    fn drop(&mut self) {\n+        self.close()\n+    }\n+}\n+\n+impl LocalDatabase {\n+    /// Like [LocalDatabase::exec] but allow to provide a closure that transform the resulting error (if any) into another error type.\n+    ///\n+    /// # Errors\n+    ///\n+    /// Will return a error on the same condition in [LocalDatabase::exec] so refer to it.\n+    pub async fn exec_with_error_handler<F, HANDLER, R, E>(\n+        &self,\n+        executor: F,\n+        error_handler: HANDLER,\n+    ) -> Result<R, E>\n+    where\n+        F: (FnOnce(&mut SqliteConnection) -> diesel::result::QueryResult<R>) + Send + 'static,\n+        HANDLER: FnOnce(diesel::result::Error) -> E + Send,\n+        R: Send + 'static,\n+        E: From<DatabaseError>,\n+    {\n+        self.exec(executor).await.map_err(|e| match e {\n+            DatabaseError::Diesel(diesel_error) => error_handler(diesel_error),\n+            _ => E::from(e),\n+        })\n+    }\n+\n+    /// Execute the provided closure.\n+    ///\n+    /// # Errors\n+    ///\n+    /// Will return an error if the database is close or if the job execution result in error, see [LocalDatabase::exec_job] for more information.\n+    pub async fn exec<F, R>(&self, job: F) -> DatabaseResult<R>\n+    where\n+        F: (FnOnce(&mut SqliteConnection) -> diesel::result::QueryResult<R>) + Send + 'static,\n+        R: Send + 'static,\n+    {\n+        let job = {\n+            let guard = self.executor.lock().expect(\"Mutex is poisoned\");\n+            guard\n+                .as_ref()\n+                .map(|exec| exec.exec(job))\n+                .ok_or(DatabaseError::Closed)\n+        }?;\n+        self.exec_job(job).await\n+    }\n+\n+    /// Execute a `ExecJob` and parse its result.\n+    ///\n+    /// # Errors\n+    ///\n+    /// Will return a errors if the execution of the job failed for any reason.\n+    ///\n+    /// Note: If the error is considered as critical, it will consider that the driver can't be used and will close the connection like [LocalDatabase::close].\n+    async fn exec_job<R, E>(&self, job: ExecJob<Result<R, E>>) -> DatabaseResult<R>\n+    where\n+        R: Send + 'static,\n+        DatabaseError: From<E>,\n+        E: Send,\n+    {\n+        let res = job.send().await?.map_err(DatabaseError::from);\n+        if let Err(DatabaseError::DieselDatabaseError(kind, _err)) = res.as_ref() {\n+            match kind {\n+                diesel::result::DatabaseErrorKind::UniqueViolation\n+                | diesel::result::DatabaseErrorKind::ForeignKeyViolation\n+                | diesel::result::DatabaseErrorKind::UnableToSendCommand\n+                | diesel::result::DatabaseErrorKind::SerializationFailure\n+                | diesel::result::DatabaseErrorKind::ReadOnlyTransaction\n+                | diesel::result::DatabaseErrorKind::NotNullViolation\n+                | diesel::result::DatabaseErrorKind::CheckViolation => (),\n+                // We want to remove the ability to send job when the error is `CloseConnection`\n+                // (the database is close so no way we could execute those jobs).\n+                diesel::result::DatabaseErrorKind::ClosedConnection => {\n+                    // TODO: improve logging with tracing or log see: #3930\n+                    eprintln!(\"The sqlite connection shouldn't be close at that step\");\n+                    self.close();\n+                }\n+                // And on unknown error, we could be more picky and only close the connection on specific unknown error (for example only close the connection on `disk full`)\n+                // But checking for those is hard and implementation specific (we need to check against a `&str` which formatting could change).\n+                _ => {\n+                    // TODO: improve logging with tracing or log see: #3930\n+                    eprintln!(\"Diesel unknown error: {kind:?}\");\n+                    self.close();\n+                }\n+            }\n+        }\n+        res\n+    }\n+}\n+\n+impl std::fmt::Debug for LocalDatabase {\n+    fn fmt(&self, f: &mut std::fmt::Formatter<'_>) -> std::fmt::Result {\n+        let mut fmt = f.debug_struct(\"LocalDatabase\");\n+        fmt.field(\"path\", &self.path)\n+            .field(\"vacuum_mode\", &self.vacuum_mode);\n+\n+        #[cfg(feature = \"test-utils\")]\n+        {\n+            fmt.field(\"is_in_memory\", &self.is_in_memory);\n+        }\n+\n+        fmt.finish_non_exhaustive()\n+    }\n+}"
    }
  ],
  "fix_category": NaN,
  "root_cause_category": "Hard to classify",
  "root_cause_subcategory": NaN
}