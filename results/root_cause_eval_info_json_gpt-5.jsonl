{"id": 4, "ground_truth": "Async Wait", "predicted": "Logic", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nCurrently `fedimint-cli spend <amount>` will give us `<amount>` if it has those notes on hand, and give more than we ask for if it doesn't.\r\n\r\nI was just writing a test and this behavior caused my test to be flaky. It would be nice for this reason (and probably in general) to be able to request exact amounts from `fedimint-cli spend`. \n\n### Code Patch:\n@@ -20,7 +20,9 @@ use fedimint_ln_client::{\n     PayType,\n };\n use fedimint_ln_common::contracts::ContractId;\n-use fedimint_mint_client::{MintClientModule, OOBNotes};\n+use fedimint_mint_client::{\n+    MintClientModule, OOBNotes, SelectNotesWithAtleastAmount, SelectNotesWithExactAmount,\n+};\n use fedimint_wallet_client::{WalletClientModule, WithdrawState};\n use futures::StreamExt;\n use itertools::Itertools;\n@@ -29,7 +31,7 @@ use serde::{Deserialize, Serialize};\n use serde_json::json;\n use time::format_description::well_known::iso8601;\n use time::OffsetDateTime;\n-use tracing::{debug, info};\n+use tracing::{debug, info, warn};\n \n use crate::{metadata_from_clap_cli, LnInvoiceResponse};\n \n@@ -58,7 +60,18 @@ pub enum ClientCmd {\n     /// Reissue notes received from a third party to avoid double spends\n     Reissue { oob_notes: OOBNotes },\n     /// Prepare notes to send to a third party as a payment\n-    Spend { amount: Amount },\n+    Spend {\n+        /// The amount of e-cash to spend\n+        amount: Amount,\n+        /// If the exact amount cannot be represented, return e-cash of a higher\n+        /// value instead of failing\n+        #[clap(long)]\n+        allow_overpay: bool,\n+        /// After how many seconds we will try to reclaim the e-cash if it\n+        /// hasn't been redeemed by the recipient. Defaults to one week.\n+        #[clap(long, default_value_t = 60 * 60 * 24 * 7)]\n+        timeout: u64,\n+    },\n     /// Verifies the signatures of e-cash notes, but *not* if they have been\n     /// spent already\n     Validate { oob_notes: OOBNotes },\n@@ -188,11 +201,34 @@ pub async fn handle_command(\n \n             Ok(serde_json::to_value(amount).unwrap())\n         }\n-        ClientCmd::Spend { amount } => {\n-            let (operation, notes) = client\n-                .get_first_module::<MintClientModule>()\n-                .spend_notes(amount, Duration::from_secs(3600), ())\n-                .await?;\n+        ClientCmd::Spend {\n+            amount,\n+            allow_overpay,\n+            timeout,\n+        } => {\n+            warn!(\"The client will try to double-spend these notes after the duration specified by the --timeout option to recover any unclaimed e-cash.\");\n+\n+            let mint_module = client.get_first_module::<MintClientModule>();\n+            let timeout = Duration::from_secs(timeout);\n+            let (operation, notes) = if allow_overpay {\n+                let (operation, notes) = mint_module\n+                    .spend_notes_with_selector(&SelectNotesWithAtleastAmount, amount, timeout, ())\n+                    .await?;\n+\n+                let overspend_amount = notes.total_amount() - amount;\n+                if overspend_amount != Amount::ZERO {\n+                    warn!(\n+                        \"Selected notes {} worth more than requested\",\n+                        overspend_amount\n+                    );\n+                }\n+\n+                (operation, notes)\n+            } else {\n+                mint_module\n+                    .spend_notes_with_selector(&SelectNotesWithExactAmount, amount, timeout, ())\n+                    .await?\n+            };\n             info!(\"Spend e-cash operation: {operation}\");\n \n             Ok(json!({\n\n@@ -1428,6 +1428,33 @@ impl<Note: Send> NotesSelector<Note> for SelectNotesWithAtleastAmount {\n     }\n }\n \n+/// Select notes with total amount of *exactly* `request_amount`. If the amount\n+/// cannot be represented with the available denominations an error is returned,\n+/// this **does not** mean that the balance is too low.\n+pub struct SelectNotesWithExactAmount;\n+\n+#[apply(async_trait_maybe_send!)]\n+impl<Note: Send> NotesSelector<Note> for SelectNotesWithExactAmount {\n+    async fn select_notes(\n+        &self,\n+        #[cfg(not(target_family = \"wasm\"))] stream: impl futures::Stream<Item = (Amount, Note)> + Send,\n+        #[cfg(target_family = \"wasm\")] stream: impl futures::Stream<Item = (Amount, Note)>,\n+        requested_amount: Amount,\n+    ) -> anyhow::Result<TieredMulti<Note>> {\n+        let notes = select_notes_from_stream(stream, requested_amount).await?;\n+\n+        if notes.total_amount() != requested_amount {\n+            bail!(\n+                \"Could not select notes with exact amount. Requested amount: {}. Selected amount: {}\",\n+                requested_amount,\n+                notes.total_amount()\n+            );\n+        }\n+\n+        Ok(notes)\n+    }\n+}\n+\n // We are using a greedy algorithm to select notes. We start with the largest\n // then proceed to the lowest tiers/denominations.\n // But there is a catch: we don't know if there are enough notes in the lowest\n"}
{"id": 9, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nCurrent algo to merge default CLI options with custom ones is primitive and flaky. E.g. it relies on sorting which doesn't work well for this use case in the first place.\n\n### Code Patch:\n@@ -52,6 +52,8 @@ jobs:\n         condition: eq( variables['Agent.OS'], 'Windows_NT' )\n       - script: cargo build --all\n         displayName: Cargo build\n+      - script: cargo test --all\n+        displayName: Cargo test vibranium\n       - script: cd cli && cargo test --all\n-        displayName: Cargo test\n+        displayName: Cargo test vibranium-cli\n \n\n@@ -1,7 +1,8 @@\n-extern crate log;\n-\n use std::process::{Command, Child};\n+\n use crate::config;\n+use crate::utils;\n+\n use support::SupportedBlockchainClients;\n \n pub mod error;\n@@ -36,21 +37,21 @@ impl<'a> Node<'a> {\n     let client_options: Vec<String> = match &config.client_options {\n       Some(options) => {\n         match client.parse() {\n-          Ok(SupportedBlockchainClients::Parity) => merge_defaults_with_options_for(SupportedBlockchainClients::Parity, options.to_vec()),\n-          Ok(SupportedBlockchainClients::Geth) => merge_defaults_with_options_for(SupportedBlockchainClients::Geth, options.to_vec()),\n+          Ok(SupportedBlockchainClients::Parity) => utils::merge_cli_options(\n+            support::default_options_from(SupportedBlockchainClients::Parity),\n+            options.to_vec()\n+          ),\n+          Ok(SupportedBlockchainClients::Geth) => utils::merge_cli_options(\n+            support::default_options_from(SupportedBlockchainClients::Geth),\n+            options.to_vec()\n+          ),\n           Err(_err) => options.to_vec(),\n         }\n       }\n       None => {\n         match project_config.blockchain {\n-          Some(config) => config.options.unwrap_or_else(|| {\n-            match client.parse() {\n-              Ok(SupportedBlockchainClients::Parity) => support::default_options_for(SupportedBlockchainClients::Parity),\n-              Ok(SupportedBlockchainClients::Geth) => support::default_options_for(SupportedBlockchainClients::Geth),\n-              Err(_err) => vec![],\n-            }\n-          }),\n-          None => support::default_options_for(SupportedBlockchainClients::Parity)\n+          Some(config) => config.options.unwrap_or_else(|| try_default_options_from(&client)),\n+          None => try_default_options_from(&client)\n         }\n       }\n     };\n@@ -70,10 +71,11 @@ impl<'a> Node<'a> {\n   }\n }\n \n-fn merge_defaults_with_options_for(client: SupportedBlockchainClients, options: Vec<String>) -> Vec<String> {\n-  let mut combined = support::default_options_for(client);\n-  combined.append(&mut options.clone());\n-  combined.sort();\n-  combined.dedup();\n-  combined\n+\n+fn try_default_options_from(client: &str) -> Vec<String> {\n+  match client.parse() {\n+    Ok(SupportedBlockchainClients::Parity) => support::default_options_from(SupportedBlockchainClients::Parity),\n+    Ok(SupportedBlockchainClients::Geth) => support::default_options_from(SupportedBlockchainClients::Geth),\n+    Err(_err) => vec![],\n+  }\n }\n\n@@ -30,7 +30,7 @@ impl ToString for SupportedBlockchainClients {\n   }\n }\n \n-pub fn default_options_for(client: SupportedBlockchainClients) -> Vec<String> {\n+pub fn default_options_from(client: SupportedBlockchainClients) -> Vec<String> {\n   match client {\n     SupportedBlockchainClients::Parity => {\n       vec![\"--config\".to_string(), \"dev\".to_string()]\n\n@@ -2,6 +2,7 @@ pub mod blockchain;\n pub mod project_generator;\n pub mod compiler;\n pub mod config;\n+mod utils;\n \n use std::process::{ExitStatus, Output};\n use std::path::PathBuf;\n\n@@ -113,7 +113,7 @@ pub fn default_project_config() -> config::ProjectConfig {\n     }),\n     blockchain: Some(config::ProjectCmdExecutionConfig {\n       cmd: Some(blockchain::support::SupportedBlockchainClients::Parity.to_string()),\n-      options: Some(blockchain::support::default_options_for(blockchain::support::SupportedBlockchainClients::Parity))\n+      options: Some(blockchain::support::default_options_from(blockchain::support::SupportedBlockchainClients::Parity))\n     }),\n   }\n }\n\n@@ -0,0 +1,89 @@\n+pub fn merge_cli_options(a: Vec<String>, b: Vec<String>) -> Vec<String> {\n+\n+  let mut merged = vec![];\n+\n+  for (i, e) in a.iter().enumerate() {\n+    if !e.starts_with(\"--\") {\n+      continue;\n+    }\n+\n+    merged.push(e.to_owned());\n+\n+    if let Some(next) = a.get(i+1) {\n+      if !next.starts_with(\"--\") && b.contains(&e) {\n+        let ii = b.iter().position(|x| x == e).unwrap();\n+        if let Some(b_next) = b.get(ii+1) {\n+          if !b_next.starts_with(\"--\") {\n+            merged.push(b_next.to_owned());\n+          }\n+        }\n+      } else {\n+        if !next.starts_with(\"--\") {\n+          merged.push(next.to_owned())\n+        }\n+      }\n+    }\n+  }\n+\n+  for e in b {\n+    if !merged.contains(&e) {\n+      merged.push(e.to_owned());\n+    }\n+  }\n+\n+  merged\n+}\n+\n+#[cfg(test)]\n+mod tests {\n+\n+  mod merge_cli_options {\n+\n+    use super::super::merge_cli_options;\n+\n+    #[test]\n+    fn it_should_merge_two_vec_of_options() {\n+      let a = vec![\"--one\".to_string(),\"--two\".to_string()];\n+      let b = vec![\"--three\".to_string(), \"--four\".to_string()];\n+\n+      let merged = merge_cli_options(a, b);\n+      assert_eq!(merged, vec![\"--one\".to_string(), \"--two\".to_string(), \"--three\".to_string(), \"--four\".to_string()]);\n+    }\n+\n+    #[test]\n+    fn it_should_keep_option_values_in_place() {\n+      let a = vec![\"--one\".to_string(),\"value1\".to_string(), \"--three\".to_string()];\n+      let b = vec![\"--two\".to_string(), \"value2\".to_string()];\n+\n+      let merged = merge_cli_options(a, b);\n+      assert_eq!(merged, vec![\"--one\".to_string(), \"value1\".to_string(), \"--three\".to_string(), \"--two\".to_string(), \"value2\".to_string()]);\n+    }\n+\n+    #[test]\n+    fn it_should_override_options() {\n+      let a = vec![\"--one\".to_string(),\"value1\".to_string(), \"--two\".to_string()];\n+      let b = vec![\"--one\".to_string(), \"value2\".to_string()];\n+\n+      let merged = merge_cli_options(a, b);\n+      assert_eq!(merged, vec![\"--one\".to_string(), \"value2\".to_string(), \"--two\".to_string()]);\n+    }\n+\n+    #[test]\n+    fn it_should_override_options_with_no_value_given() {\n+      let a = vec![\"--one\".to_string(),\"value1\".to_string(), \"--two\".to_string(), \"value2\".to_string()];\n+      let b = vec![\"--one\".to_string(), \"value1\".to_string(), \"--two\".to_string()];\n+\n+      let merged = merge_cli_options(a, b);\n+      assert_eq!(merged, vec![\"--one\".to_string(), \"value1\".to_string(), \"--two\".to_string()]);\n+    }\n+\n+    #[test]\n+    fn it_should_throw_out_duplicates_and_keep_the_last() {\n+      let a = vec![\"--one\".to_string(),\"value1\".to_string()];\n+      let b = vec![\"--two\".to_string(), \"value1\".to_string(), \"--two\".to_string(), \"value2\".to_string()];\n+\n+      let merged = merge_cli_options(a, b);\n+      assert_eq!(merged, vec![\"--one\".to_string(), \"value1\".to_string(), \"--two\".to_string(), \"value2\".to_string()]);\n+    }\n+  }\n+}\n"}
{"id": 78, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nAfter a1e25956b6e04e9c678293318967db3f2e4b905a some of the cancellation tests have become a bit flaky. It seems io_uring doesn't behave consistent when `IOSQE_ASYNC` is and isn't set, but I'm a 100% sure yet.\n\n### Code Patch:\n@@ -77,7 +77,7 @@ pub trait Cancel {\n     /// If the operation was found and canceled this returns `Ok(())`.\n     ///\n     /// If this is called on an [`AsyncIterator`] it will cause them to return\n-    /// `None` (eventuaully, it may still return pending items).\n+    /// `None` (eventually, it may still return pending items).\n     ///\n     /// [`AsyncIterator`]: std::async_iter::AsyncIterator\n     fn cancel(&mut self) -> CancelOp;\n\n@@ -467,6 +467,7 @@ impl ReadBuf {\n     }\n \n     /// Returns the remaining spare capacity of the buffer.\n+    #[allow(clippy::needless_pass_by_ref_mut)] // See https://github.com/rust-lang/rust-clippy/issues/12905.\n     pub fn spare_capacity_mut(&mut self) -> &mut [MaybeUninit<u8>] {\n         if let Some(ptr) = self.owned {\n             let unused_len = self.shared.buf_size as usize - ptr.len();\n\n@@ -231,6 +231,7 @@ impl Ring {\n     ///\n     /// This only required when starting the ring in disabled mode, see\n     /// [`Config::disable`].\n+    #[allow(clippy::needless_pass_by_ref_mut)]\n     pub fn enable(&mut self) -> io::Result<()> {\n         self.sq\n             .register(libc::IORING_REGISTER_ENABLE_RINGS, ptr::null(), 0)\n@@ -348,6 +349,7 @@ impl Ring {\n     }\n \n     /// Wake [`SharedSubmissionQueue::blocked_futures`].\n+    #[allow(clippy::needless_pass_by_ref_mut)]\n     fn wake_blocked_futures(&mut self) {\n         // This not particullary efficient, but with a large enough number of\n         // entries, `IORING_SETUP_SQPOLL` and suffcient calls to [`Ring::poll`]\n\n@@ -27,7 +27,7 @@ fn to_file_descriptor() {\n     let waker = Waker::new();\n \n     let open_file = OpenOptions::new().open(sq, LOREM_IPSUM_5.path.into());\n-    let direct_fd = dbg!(waker.block_on(open_file)).unwrap();\n+    let direct_fd = waker.block_on(open_file).unwrap();\n     let regular_fd = waker.block_on(direct_fd.to_file_descriptor()).unwrap();\n \n     check_fs_fd(waker, regular_fd, direct_fd);\n\n@@ -6,7 +6,6 @@ use std::io;\n use std::ops::Bound;\n use std::os::fd::{AsFd, AsRawFd, RawFd};\n use std::panic::{self, AssertUnwindSafe};\n-use std::pin::Pin;\n \n use a10::fd::{AsyncFd, File};\n use a10::fs::{Open, OpenOptions};\n@@ -17,7 +16,7 @@ use a10::io::{\n use a10::{Extract, Ring, SubmissionQueue};\n \n use crate::util::{\n-    bind_and_listen_ipv4, block_on, defer, expect_io_errno, init, is_send, is_sync, poll_nop,\n+    bind_and_listen_ipv4, block_on, cancel_all, defer, expect_io_errno, init, is_send, is_sync,\n     remove_test_file, require_kernel, start_op, tcp_ipv4_socket, test_queue, Waker, LOREM_IPSUM_5,\n     LOREM_IPSUM_50,\n };\n@@ -732,15 +731,9 @@ fn cancel_all_accept() {\n     let listener = waker.block_on(tcp_ipv4_socket(sq));\n     bind_and_listen_ipv4(&listener);\n \n-    let mut accept = listener.accept::<libc::sockaddr_in>();\n-    // Poll the future to schedule the operation, can't use `start_op` as the\n-    // address doesn't implement `fmt::Debug`.\n-    assert!(poll_nop(Pin::new(&mut accept)).is_pending());\n+    let mut accept = listener.accept::<a10::net::NoAddress>();\n \n-    let n = waker\n-        .block_on(listener.cancel_all())\n-        .expect(\"failed to cancel all calls\");\n-    assert!(n == 1);\n+    cancel_all(&waker, &listener, || start_op(&mut accept), 1);\n \n     expect_io_errno(waker.block_on(accept), libc::ECANCELED);\n }\n@@ -755,19 +748,13 @@ fn cancel_all_twice_accept() {\n     let listener = waker.block_on(tcp_ipv4_socket(sq));\n     bind_and_listen_ipv4(&listener);\n \n-    let mut accept = listener.accept::<libc::sockaddr_in>();\n-    // Poll the future to schedule the operation, can't use `start_op` as the\n-    // address doesn't implement `fmt::Debug`.\n-    assert!(poll_nop(Pin::new(&mut accept)).is_pending());\n+    let mut accept = listener.accept::<a10::net::NoAddress>();\n \n+    cancel_all(&waker, &listener, || start_op(&mut accept), 1);\n     let n = waker\n         .block_on(listener.cancel_all())\n-        .expect(\"failed to cancel all calls\");\n-    assert!(n == 1);\n-    let n2 = waker\n-        .block_on(listener.cancel_all())\n-        .expect(\"failed to cancel all calls\");\n-    assert!(n2 == 0);\n+        .expect(\"failed to cancel all operations\");\n+    assert_eq!(n, 0);\n \n     expect_io_errno(waker.block_on(accept), libc::ECANCELED);\n }\n@@ -783,7 +770,7 @@ fn cancel_all_no_operation_in_progress() {\n \n     let n = waker\n         .block_on(socket.cancel_all())\n-        .expect(\"failed to cancel\");\n+        .expect(\"failed to cancel all operations\");\n     assert_eq!(n, 0);\n }\n \n\n@@ -20,9 +20,9 @@ use a10::{Extract, Ring};\n \n use crate::async_fd::io::{BadBuf, BadBufSlice, BadReadBuf, BadReadBufSlice};\n use crate::util::{\n-    bind_and_listen_ipv4, bind_ipv4, block_on, expect_io_errno, expect_io_error_kind, init,\n-    is_send, is_sync, new_socket, next, poll_nop, require_kernel, syscall, tcp_ipv4_socket,\n-    test_queue, udp_ipv4_socket, Waker,\n+    bind_and_listen_ipv4, bind_ipv4, block_on, cancel, expect_io_errno, expect_io_error_kind, init,\n+    is_send, is_sync, new_socket, next, require_kernel, start_mulitshot_op, start_op, syscall,\n+    tcp_ipv4_socket, test_queue, udp_ipv4_socket, Waker,\n };\n \n const DATA1: &[u8] = b\"Hello, World!\";\n@@ -121,14 +121,10 @@ fn cancel_accept() {\n     let listener = waker.block_on(tcp_ipv4_socket(sq));\n     bind_and_listen_ipv4(&listener);\n \n-    let accept = listener.accept::<(libc::sockaddr_storage, libc::socklen_t)>();\n-    let mut accept = std::pin::pin!(accept);\n-    // Poll the future to schedule the operation, can't use `start_op` as the\n-    // address doesn't implement `fmt::Debug`.\n-    assert!(poll_nop(accept.as_mut()).is_pending());\n+    let mut accept = listener.accept::<NoAddress>();\n \n     // Then cancel the accept multishot call.\n-    waker.block_on(accept.as_mut().cancel()).unwrap();\n+    cancel(&waker, &mut accept, start_op);\n \n     expect_io_errno(waker.block_on(accept), libc::ECANCELED);\n }\n@@ -254,9 +250,7 @@ fn cancel_multishot_accept() {\n     let c_addr1 = peer_addr(client1.as_fd()).expect(\"failed to get address\");\n \n     // Then cancel the accept multishot call.\n-    waker\n-        .block_on(accept_stream.cancel())\n-        .expect(\"failed to cancel\");\n+    cancel(&waker, &mut accept_stream, start_mulitshot_op);\n \n     // We should still be able to accept the second connection.\n     let client2 = waker\n@@ -387,13 +381,8 @@ fn connect() {\n     client.write_all(DATA2).expect(\"failed to write\");\n     buf.clear();\n     buf.reserve(DATA2.len() + 1);\n-    let mut buf = waker.block_on(stream.read(buf)).expect(\"failed to read\");\n+    let buf = waker.block_on(stream.read(buf)).expect(\"failed to read\");\n     assert_eq!(buf, DATA2);\n-\n-    // Dropping the stream should closing it.\n-    drop(stream);\n-    let n = client.read(&mut buf).expect(\"failed to read\");\n-    assert_eq!(n, 0);\n }\n \n #[test]\n\n@@ -15,7 +15,6 @@ use std::task::{self, Poll, Wake};\n use std::thread;\n use std::time::{Duration, Instant};\n \n-use a10::cancel::Cancel;\n use a10::fs::{Open, OpenOptions};\n use a10::io::ReadBufPool;\n use a10::msg::{msg_listener, send_msg, try_send_msg, MsgListener, MsgToken, SendMsg};\n@@ -24,7 +23,7 @@ use a10::{fd, mem, process, Config, Ring, SubmissionQueue};\n \n mod util;\n use util::{\n-    defer, expect_io_errno, init, is_send, is_sync, next, poll_nop, require_kernel,\n+    cancel, defer, expect_io_errno, init, is_send, is_sync, next, poll_nop, require_kernel,\n     start_mulitshot_op, start_op, test_queue, Waker,\n };\n \n@@ -279,7 +278,7 @@ fn message_sending() {\n \n     let (msg_listener, msg_token) = msg_listener(sq.clone()).unwrap();\n     let mut msg_listener = pin!(msg_listener);\n-    start_mulitshot_op(msg_listener.as_mut());\n+    start_mulitshot_op(&mut msg_listener);\n \n     // Send some messages.\n     try_send_msg(&sq, msg_token, DATA1).unwrap();\n@@ -340,10 +339,9 @@ fn cancel_oneshot_poll() {\n \n     let (receiver, sender) = pipe2().unwrap();\n \n-    let mut receiver_read = pin!(oneshot_poll(&sq, receiver.as_fd(), libc::POLLIN as _));\n-    start_op(&mut receiver_read);\n+    let mut receiver_read = oneshot_poll(&sq, receiver.as_fd(), libc::POLLIN as _);\n \n-    waker.block_on(receiver_read.cancel()).unwrap();\n+    cancel(&waker, &mut receiver_read, start_op);\n     expect_io_errno(waker.block_on(receiver_read), libc::ECANCELED);\n     drop(sender);\n }\n@@ -359,7 +357,7 @@ fn test_multishot_poll() {\n     let (mut receiver, mut sender) = pipe2().unwrap();\n \n     let mut receiver_read = pin!(multishot_poll(&sq, receiver.as_fd(), libc::POLLIN as _));\n-    start_mulitshot_op(Pin::new(&mut receiver_read));\n+    start_mulitshot_op(&mut receiver_read);\n \n     let mut buf = vec![0; DATA.len() + 1];\n     for _ in 0..3 {\n@@ -384,10 +382,9 @@ fn cancel_multishot_poll() {\n \n     let (receiver, sender) = pipe2().unwrap();\n \n-    let mut receiver_read = pin!(multishot_poll(&sq, receiver.as_fd(), libc::POLLIN as _));\n-    start_mulitshot_op(receiver_read.as_mut());\n+    let mut receiver_read = multishot_poll(&sq, receiver.as_fd(), libc::POLLIN as _);\n \n-    waker.block_on(receiver_read.cancel()).unwrap();\n+    cancel(&waker, &mut receiver_read, start_mulitshot_op);\n     assert!(waker.block_on(next(receiver_read)).is_none());\n     drop(sender);\n }\n@@ -400,7 +397,7 @@ fn drop_multishot_poll() {\n \n     let mut receiver_read = multishot_poll(&sq, receiver.as_fd(), libc::POLLIN as _);\n \n-    start_mulitshot_op(Pin::new(&mut receiver_read));\n+    start_mulitshot_op(&mut receiver_read);\n \n     drop(receiver_read);\n     drop(receiver);\n@@ -478,12 +475,15 @@ fn process_wait_on_cancel() {\n     let mut process = Command::new(\"sleep\").arg(\"1000\").spawn().unwrap();\n \n     let mut future = process::wait_on(sq, &process, libc::WEXITED);\n-    let result = poll_nop(Pin::new(&mut future));\n-    if !result.is_pending() {\n-        panic!(\"unexpected result, expected it to return Poll::Pending\");\n-    }\n \n-    waker.block_on(future.cancel()).unwrap();\n+    cancel(&waker, &mut future, |future| {\n+        // NOTE: can't use `start_op` as `siginfo_t` doesn't implemented\n+        // `fmt::Debug`.\n+        let result = poll_nop(Pin::new(future));\n+        if !result.is_pending() {\n+            panic!(\"unexpected result, expected it to return Poll::Pending\");\n+        }\n+    });\n \n     process.kill().unwrap();\n     process.wait().unwrap();\n\n@@ -18,6 +18,8 @@ use std::task::{self, Poll};\n use std::thread::{self, Thread};\n use std::{fmt, mem, panic, process, ptr, str};\n \n+use a10::cancel::Cancel;\n+use a10::fd::Descriptor;\n use a10::net::socket;\n use a10::{AsyncFd, Ring, SubmissionQueue};\n \n@@ -203,6 +205,69 @@ impl Waker {\n     }\n }\n \n+/// Cancel `operation`.\n+///\n+/// `Future`s are inert and we can't determine if an operation has actually been\n+/// started or the submission queue was full. This means that we can't ensure\n+/// that the operation has been queued with the Kernel. This means that in some\n+/// cases we won't actually cancel the operation simply because it hasn't\n+/// started. This introduces flakyness in the tests.\n+///\n+/// To work around this we have this cancel function. If we fail to cancel the\n+/// operation we try to start the operation using `start_op`, before canceling t\n+/// again. Looping until the operation is canceled.\n+#[track_caller]\n+pub(crate) fn cancel<O, F>(waker: &Arc<Waker>, operation: &mut O, start_op: F)\n+where\n+    O: Cancel,\n+    F: Fn(&mut O),\n+{\n+    for _ in 0..100 {\n+        match waker.block_on(operation.cancel()) {\n+            Ok(()) => return,\n+            Err(ref err) if err.kind() == io::ErrorKind::NotFound => {}\n+            Err(err) => panic!(\"unexpected error canceling operation: {err}\"),\n+        }\n+\n+        start_op(operation);\n+    }\n+    panic!(\"couldn't cancel operation\");\n+}\n+\n+/// Cancel all operations of `fd`.\n+///\n+/// `Future`s are inert and we can't determine if an operation has actually been\n+/// started or the submission queue was full. This means that we can't ensure\n+/// that the operation has been queued with the Kernel. This means that in some\n+/// cases we won't actually cancel any operations simply because they haven't\n+/// started. This introduces flakyness in the tests.\n+///\n+/// To work around this we have this cancel all function. If we fail to get the\n+/// `expected` number of canceled operations we try to start the operations\n+/// using `start_op`, before canceling them again. Looping until we get the\n+/// expected number of canceled operations.\n+#[track_caller]\n+pub(crate) fn cancel_all<D: Descriptor, F: FnMut()>(\n+    waker: &Arc<Waker>,\n+    fd: &AsyncFd<D>,\n+    mut start_op: F,\n+    expected: usize,\n+) {\n+    let mut canceled = 0;\n+    for _ in 0..100 {\n+        let n = waker\n+            .block_on(fd.cancel_all())\n+            .expect(\"failed to cancel all operations\");\n+        canceled += n;\n+        if canceled >= expected {\n+            return;\n+        }\n+\n+        start_op();\n+    }\n+    panic!(\"couldn't cancel all expected operations\");\n+}\n+\n /// Start an A10 operation, assumes `future` is a A10 `Future`.\n #[track_caller]\n pub(crate) fn start_op<Fut>(future: &mut Fut)\n@@ -218,7 +283,7 @@ where\n \n /// Start an A10 multishot operation, assumes `iter` is a A10 `AsyncIterator`.\n #[track_caller]\n-pub(crate) fn start_mulitshot_op<I>(iter: I)\n+pub(crate) fn start_mulitshot_op<I>(iter: &mut I)\n where\n     I: AsyncIterator + Unpin,\n     I::Item: fmt::Debug,\n"}
{"id": 81, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nI am attempting to switch from the standard library's RwLock to parking_lot's. Unfortunately when making this switch thread sanitizer started to complain about data races. I was able to isolate that this is just a matter of the RwLock, since this diff fixes the issue by switching to std::sync::RwLock - https://github.com/matanmarkind/active_standby/commit/bb1cd755e37d9ebb42bc0871a721d4d492dd59c7.\r\n\r\nChecking out the base commit, ee51de677f1722b9b1a0ec70ac9e9462d87bf4a1, and running the following results in failures about half the time with parking_lot, while never failing with the std RwLock.\r\n\r\n`\r\nRUST_BACKTRACE=full RUSTFLAGS=\"-Zsanitizer=thread -g\" cargo +nightly bench shared_wguard_rw_contention -Z build-std --target x86_64-unknown-linux-gnu --quiet\r\n`\r\n\r\n```\r\nrunning 0 tests\r\n\r\ntest result: ok. 0 passed; 0 failed; 0 ignored; 0 measured; 127 filtered out; finished in 0.00s\r\n\r\n\r\nrunning 1 test\r\n==================\r\nWARNING: ThreadSanitizer: data race (pid=516081)\r\n  Write of size 4 at 0x7b04000016b8 by thread T11:\r\n    #0 <bench::AddOne as active_standby::types::UpdateTables<i32,()>>::apply_first /home/matan/rust/active_standby/benches/bench.rs:29 (bench-d7c10858bf64896f+0xbbae9) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 <bench::AddOne as active_standby::types::UpdateTables<i32,()>>::apply_second /home/matan/rust/active_standby/benches/bench.rs:32 (bench-d7c10858bf64896f+0xbbae9)\r\n    #2 active_standby::shared::aslock::AsLockWriteGuard<T>::update_tables::{{closure}} /home/matan/rust/active_standby/src/shared/aslock.rs:169 (bench-d7c10858bf64896f+0xbbae9)\r\n    #3 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbbae9)\r\n    #4 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0xc44cc) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #5 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0xc44cc) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #6 active_standby::shared::aslock::AsLock<T>::write /home/matan/rust/active_standby/src/shared/aslock.rs:99 (bench-d7c10858bf64896f+0xbbc50) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #7 bench::shared::AsLock::write /home/matan/rust/active_standby/src/macros.rs:231 (bench-d7c10858bf64896f+0xbebc3) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #8 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:181 (bench-d7c10858bf64896f+0xbd50a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #9 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbd50a)\r\n    #10 std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:498 (bench-d7c10858bf64896f+0xc3b6e) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #11 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0xc3b6e)\r\n    #12 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0xc3b6e)\r\n    #13 __rust_try.llvm.3183576308868158445 :? (bench-d7c10858bf64896f+0xc3d91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #14 __rust_try.llvm.3183576308868158445 :? (bench-d7c10858bf64896f+0xc3d91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #15 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0xbb9eb) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #16 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xbb9eb)\r\n    #17 std::thread::Builder::spawn_unchecked_::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:497 (bench-d7c10858bf64896f+0xb3ee2) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #18 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xb3ee2)\r\n    #19 std::thread::Builder::spawn_unchecked_::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:497 (bench-d7c10858bf64896f+0xb3ee2) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #20 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xb3ee2)\r\n    #21 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0x131f7f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #22 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0x131f7f)\r\n    #23 std::sys::unix::thread::Thread::new::thread_start /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:108 (bench-d7c10858bf64896f+0x1310bc) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n\r\n  Previous read of size 4 at 0x7b04000016b8 by thread T4:\r\n    #0 bench::benchmarks::shared_wguard_rw_contention::{{closure}}::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:169 (bench-d7c10858bf64896f+0xbdb1a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbdb1a)\r\n    #2 std::thread::Builder::spawn_unchecked_::{{closure}}::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:498 (bench-d7c10858bf64896f+0xc368e) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #3 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0xc368e)\r\n    #4 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0xc368e)\r\n    #5 __rust_try.llvm.3183576308868158445 :? (bench-d7c10858bf64896f+0xc3d91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #6 __rust_try.llvm.3183576308868158445 :? (bench-d7c10858bf64896f+0xc3d91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #7 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0xbb4ab) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #8 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xbb4ab)\r\n    #9 std::thread::Builder::spawn_unchecked_::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:497 (bench-d7c10858bf64896f+0xb3ae2) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #10 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xb3ae2)\r\n    #11 std::thread::Builder::spawn_unchecked_::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:497 (bench-d7c10858bf64896f+0xb3ae2) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #12 core::ops::function::FnOnce::call_once{{vtable-shim}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xb3ae2)\r\n    #13 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0x131f7f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #14 <alloc::boxed::Box<F,A> as core::ops::function::FnOnce<Args>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:1854 (bench-d7c10858bf64896f+0x131f7f)\r\n    #15 std::sys::unix::thread::Thread::new::thread_start /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:108 (bench-d7c10858bf64896f+0x1310bc) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n\r\n  Location is heap block of size 16 at 0x7b04000016b0 allocated by main thread:\r\n    #0 malloc /rustc/llvm/src/llvm-project/compiler-rt/lib/tsan/rtl/tsan_interceptors_posix.cpp:667 (bench-d7c10858bf64896f+0x29ac1) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 std::sys::unix::alloc::<impl core::alloc::global::GlobalAlloc for std::alloc::System>::alloc /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/alloc.rs:14 (bench-d7c10858bf64896f+0x13c21c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #2 __rdl_alloc /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/alloc.rs:355 (bench-d7c10858bf64896f+0x13c21c)\r\n    #3 alloc::alloc::alloc /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/alloc.rs:87 (bench-d7c10858bf64896f+0xbbdbe) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #4 alloc::alloc::Global::alloc_impl /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/alloc.rs:169 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #5 <alloc::alloc::Global as core::alloc::Allocator>::allocate /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/alloc.rs:229 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #6 alloc::alloc::exchange_malloc /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/alloc.rs:318 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #7 alloc::boxed::Box<T>::new /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/boxed.rs:195 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #8 active_standby::shared::aslock::AsLock<T>::from_identical /home/matan/rust/active_standby/src/shared/aslock.rs:61 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #9 active_standby::shared::aslock::AsLock<T>::new /home/matan/rust/active_standby/src/shared/aslock.rs:128 (bench-d7c10858bf64896f+0xbbdbe)\r\n    #10 bench::shared::AsLock::new /home/matan/rust/active_standby/src/macros.rs:242 (bench-d7c10858bf64896f+0xbec20) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #11 bench::benchmarks::shared_wguard_rw_contention /home/matan/rust/active_standby/benches/bench.rs:162 (bench-d7c10858bf64896f+0xc5954) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #12 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:161 (bench-d7c10858bf64896f+0xbcb55) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #13 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbcb55)\r\n    #14 core::ops::function::FnMut::call_mut /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:150 (bench-d7c10858bf64896f+0xf0a6d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #15 test::bench::Bencher::bench /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:56 (bench-d7c10858bf64896f+0xf0a6d)\r\n    #16 test::bench::benchmark::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0x10c609) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #17 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0x10c609)\r\n    #18 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x10c609)\r\n    #19 __rust_try.llvm.16455064722209264270 :? (bench-d7c10858bf64896f+0x10ca21) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #20 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x10c3a3) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #21 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xea395) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #22 test::bench::benchmark /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0xf1388) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #23 test::run_test /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:546 (bench-d7c10858bf64896f+0xce11f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #24 test::run_tests /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:381 (bench-d7c10858bf64896f+0xcc9ff) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #25 test::console::run_tests_console /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/console.rs:286 (bench-d7c10858bf64896f+0x103ccd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #26 test::test_main /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:116 (bench-d7c10858bf64896f+0xcaa60) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #27 test::test_main_static /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:135 (bench-d7c10858bf64896f+0xcac7c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #28 bench::main /home/matan/rust/active_standby/benches/bench.rs:1 (bench-d7c10858bf64896f+0xbcc4c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #29 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbdc0f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #30 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbdc0f)\r\n    #31 std::rt::lang_start::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:145 (bench-d7c10858bf64896f+0xbf75d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #32 core::ops::function::impls::<impl core::ops::function::FnOnce<A> for &F>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:259 (bench-d7c10858bf64896f+0x135f45) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #33 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135f45)\r\n    #34 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #35 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #36 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x135da0) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #37 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x135da0)\r\n    #38 std::rt::lang_start_internal::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x135da0)\r\n    #39 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135da0)\r\n    #40 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #41 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x13464a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #42 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x13464a)\r\n    #43 std::rt::lang_start_internal /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x123159) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #44 main ??:? (bench-d7c10858bf64896f+0xbd2fa) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #45 __libc_start_main /build/glibc-eX1tMB/glibc-2.31/csu/../csu/libc-start.c:308 (libc.so.6+0x270b2) (BuildId: 099b9225bcb0d019d9d60884be583eb31bb5f44e)\r\n\r\n  Thread T11 (tid=516093, running) created by main thread at:\r\n    #0 pthread_create /rustc/llvm/src/llvm-project/compiler-rt/lib/tsan/rtl/tsan_interceptors_posix.cpp:1022 (bench-d7c10858bf64896f+0x2aecd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 std::sys::unix::thread::Thread::new /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:87 (bench-d7c10858bf64896f+0x130ea1) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #2 std::thread::Builder::spawn_unchecked_ /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:526 (bench-d7c10858bf64896f+0xb55bf) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #3 std::thread::Builder::spawn_unchecked /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:456 (bench-d7c10858bf64896f+0xb55bf)\r\n    #4 std::thread::Builder::spawn /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:388 (bench-d7c10858bf64896f+0xb55bf)\r\n    #5 std::thread::spawn /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:649 (bench-d7c10858bf64896f+0xb55bf)\r\n    #6 bench::benchmarks::shared_wguard_rw_contention /home/matan/rust/active_standby/benches/bench.rs:180 (bench-d7c10858bf64896f+0xc5a05) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #7 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:161 (bench-d7c10858bf64896f+0xbcb55) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #8 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbcb55)\r\n    #9 core::ops::function::FnMut::call_mut /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:150 (bench-d7c10858bf64896f+0xf0a6d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #10 test::bench::Bencher::bench /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:56 (bench-d7c10858bf64896f+0xf0a6d)\r\n    #11 test::bench::benchmark::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0x10c609) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #12 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0x10c609)\r\n    #13 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x10c609)\r\n    #14 __rust_try.llvm.16455064722209264270 :? (bench-d7c10858bf64896f+0x10ca21) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #15 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x10c3a3) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #16 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xea395) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #17 test::bench::benchmark /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0xf1388) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #18 test::run_test /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:546 (bench-d7c10858bf64896f+0xce11f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #19 test::run_tests /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:381 (bench-d7c10858bf64896f+0xcc9ff) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #20 test::console::run_tests_console /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/console.rs:286 (bench-d7c10858bf64896f+0x103ccd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #21 test::test_main /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:116 (bench-d7c10858bf64896f+0xcaa60) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #22 test::test_main_static /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:135 (bench-d7c10858bf64896f+0xcac7c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #23 bench::main /home/matan/rust/active_standby/benches/bench.rs:1 (bench-d7c10858bf64896f+0xbcc4c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #24 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbdc0f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #25 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbdc0f)\r\n    #26 std::rt::lang_start::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:145 (bench-d7c10858bf64896f+0xbf75d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #27 core::ops::function::impls::<impl core::ops::function::FnOnce<A> for &F>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:259 (bench-d7c10858bf64896f+0x135f45) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #28 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135f45)\r\n    #29 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #30 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #31 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x135da0) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #32 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x135da0)\r\n    #33 std::rt::lang_start_internal::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x135da0)\r\n    #34 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135da0)\r\n    #35 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #36 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x13464a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #37 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x13464a)\r\n    #38 std::rt::lang_start_internal /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x123159) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #39 main ??:? (bench-d7c10858bf64896f+0xbd2fa) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #40 __libc_start_main /build/glibc-eX1tMB/glibc-2.31/csu/../csu/libc-start.c:308 (libc.so.6+0x270b2) (BuildId: 099b9225bcb0d019d9d60884be583eb31bb5f44e)\r\n\r\n  Thread T4 (tid=516086, running) created by main thread at:\r\n    #0 pthread_create /rustc/llvm/src/llvm-project/compiler-rt/lib/tsan/rtl/tsan_interceptors_posix.cpp:1022 (bench-d7c10858bf64896f+0x2aecd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #1 std::sys::unix::thread::Thread::new /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys/unix/thread.rs:87 (bench-d7c10858bf64896f+0x130ea1) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #2 std::thread::Builder::spawn_unchecked_ /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:526 (bench-d7c10858bf64896f+0xb7eff) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #3 std::thread::Builder::spawn_unchecked /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:456 (bench-d7c10858bf64896f+0xb7eff)\r\n    #4 std::thread::Builder::spawn /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:388 (bench-d7c10858bf64896f+0xb7eff)\r\n    #5 std::thread::spawn /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/thread/mod.rs:649 (bench-d7c10858bf64896f+0xb7eff)\r\n    #6 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:167 (bench-d7c10858bf64896f+0xbf47a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #7 core::iter::adapters::map::map_fold::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/adapters/map.rs:84 (bench-d7c10858bf64896f+0xbf47a)\r\n    #8 core::iter::traits::iterator::Iterator::fold /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:2285 (bench-d7c10858bf64896f+0xbf47a)\r\n    #9 <core::iter::adapters::map::Map<I,F> as core::iter::traits::iterator::Iterator>::fold /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/adapters/map.rs:124 (bench-d7c10858bf64896f+0xbf47a)\r\n    #10 core::iter::traits::iterator::Iterator::for_each /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:773 (bench-d7c10858bf64896f+0xc044b) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #11 <alloc::vec::Vec<T,A> as alloc::vec::spec_extend::SpecExtend<T,I>>::spec_extend /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/spec_extend.rs:40 (bench-d7c10858bf64896f+0xc044b)\r\n    #12 <alloc::vec::Vec<T> as alloc::vec::spec_from_iter_nested::SpecFromIterNested<T,I>>::from_iter /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/spec_from_iter_nested.rs:62 (bench-d7c10858bf64896f+0xc044b)\r\n    #13 <alloc::vec::Vec<T> as alloc::vec::spec_from_iter::SpecFromIter<T,I>>::from_iter /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/spec_from_iter.rs:33 (bench-d7c10858bf64896f+0xc044b)\r\n    #14 <alloc::vec::Vec<T> as core::iter::traits::collect::FromIterator<T>>::from_iter /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/alloc/src/vec/mod.rs:2552 (bench-d7c10858bf64896f+0xc59d4) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #15 core::iter::traits::iterator::Iterator::collect /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/iter/traits/iterator.rs:1778 (bench-d7c10858bf64896f+0xc59d4)\r\n    #16 bench::benchmarks::shared_wguard_rw_contention /home/matan/rust/active_standby/benches/bench.rs:164 (bench-d7c10858bf64896f+0xc59d4)\r\n    #17 bench::benchmarks::shared_wguard_rw_contention::{{closure}} /home/matan/rust/active_standby/benches/bench.rs:161 (bench-d7c10858bf64896f+0xbcb55) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #18 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbcb55)\r\n    #19 core::ops::function::FnMut::call_mut /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:150 (bench-d7c10858bf64896f+0xf0a6d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #20 test::bench::Bencher::bench /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:56 (bench-d7c10858bf64896f+0xf0a6d)\r\n    #21 test::bench::benchmark::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0x10c609) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #22 <core::panic::unwind_safe::AssertUnwindSafe<F> as core::ops::function::FnOnce<()>>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/panic/unwind_safe.rs:271 (bench-d7c10858bf64896f+0x10c609)\r\n    #23 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x10c609)\r\n    #24 __rust_try.llvm.16455064722209264270 :? (bench-d7c10858bf64896f+0x10ca21) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #25 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x10c3a3) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #26 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0xea395) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #27 test::bench::benchmark /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/bench.rs:208 (bench-d7c10858bf64896f+0xf1388) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #28 test::run_test /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:546 (bench-d7c10858bf64896f+0xce11f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #29 test::run_tests /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:381 (bench-d7c10858bf64896f+0xcc9ff) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #30 test::console::run_tests_console /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/console.rs:286 (bench-d7c10858bf64896f+0x103ccd) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #31 test::test_main /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:116 (bench-d7c10858bf64896f+0xcaa60) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #32 test::test_main_static /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/test/src/lib.rs:135 (bench-d7c10858bf64896f+0xcac7c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #33 bench::main /home/matan/rust/active_standby/benches/bench.rs:1 (bench-d7c10858bf64896f+0xbcc4c) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #34 core::ops::function::FnOnce::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:227 (bench-d7c10858bf64896f+0xbdc0f) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #35 std::sys_common::backtrace::__rust_begin_short_backtrace /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/sys_common/backtrace.rs:122 (bench-d7c10858bf64896f+0xbdc0f)\r\n    #36 std::rt::lang_start::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:145 (bench-d7c10858bf64896f+0xbf75d) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #37 core::ops::function::impls::<impl core::ops::function::FnOnce<A> for &F>::call_once /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/core/src/ops/function.rs:259 (bench-d7c10858bf64896f+0x135f45) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #38 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135f45)\r\n    #39 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #40 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #41 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x135da0) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #42 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x135da0)\r\n    #43 std::rt::lang_start_internal::{{closure}} /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x135da0)\r\n    #44 std::panicking::try::do_call /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:492 (bench-d7c10858bf64896f+0x135da0)\r\n    #45 __rust_try.llvm.10611294404910455758 :? (bench-d7c10858bf64896f+0x136c91) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #46 std::panicking::try /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panicking.rs:456 (bench-d7c10858bf64896f+0x13464a) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #47 std::panic::catch_unwind /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/panic.rs:137 (bench-d7c10858bf64896f+0x13464a)\r\n    #48 std::rt::lang_start_internal /home/matan/.rustup/toolchains/nightly-x86_64-unknown-linux-gnu/lib/rustlib/src/rust/library/std/src/rt.rs:128 (bench-d7c10858bf64896f+0x123159) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #49 main ??:? (bench-d7c10858bf64896f+0xbd2fa) (BuildId: aeaa5e27ed135c87da734c367fba412e16bdbc40)\r\n    #50 __libc_start_main /build/glibc-eX1tMB/glibc-2.31/csu/../csu/libc-start.c:308 (libc.so.6+0x270b2) (BuildId: 099b9225bcb0d019d9d60884be583eb31bb5f44e)\r\n\r\nSUMMARY: ThreadSanitizer: data race /home/matan/rust/active_standby/benches/bench.rs:29 in <bench::AddOne as active_standby::types::UpdateTables<i32,()>>::apply_first\r\n==================\r\ntest benchmarks::shared_wguard_rw_contention         ... bench:       5,859 ns/iter (+/- 4,566)\r\n\r\ntest result: ok. 0 passed; 0 failed; 0 ignored; 1 measured; 20 filtered out; finished in 5.00s\r\n\r\nThreadSanitizer: reported 1 warnings\r\nerror: bench failed\r\n```\r\n\r\n\r\n\n\n### Code Patch:\n@@ -988,8 +988,8 @@ impl RawRwLock {\n                 if let Err(x) = self.state.compare_exchange_weak(\n                     state,\n                     state | WRITER_PARKED_BIT,\n-                    Ordering::Relaxed,\n-                    Ordering::Relaxed,\n+                    Ordering::Acquire,\n+                    Ordering::Acquire,\n                 ) {\n                     state = x;\n                     continue;\n"}
{"id": 50, "ground_truth": "Concurrency", "predicted": "Unordered data", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nLooks like [`test_swap_offchain()` is flaky right now](https://github.com/0xPolygonMiden/miden-client/actions/runs/9591340149/job/26448074126#step:8:352). We should review and fix it.\n\n### Code Patch:\n@@ -2,6 +2,7 @@\n \n ## v0.5.0 (TBD)\n \n+* Fix flaky integration tests (#410).\n * Add conversions for `NoteRecordDetails` (#392).\n \n ## v0.4.0 (2024-07-05)\n\n@@ -841,16 +841,22 @@ async fn test_update_ignored_tag() {\n         .unwrap();\n \n     // Ignored notes are only retrieved for \"Ignored\" or \"All\" filters\n-    assert_eq!(client_2.get_input_notes(NoteFilter::All).unwrap().len(), 1);\n-    assert_eq!(client_2.get_input_notes(NoteFilter::Ignored).unwrap().len(), 1);\n-    assert_eq!(client_2.get_input_notes(NoteFilter::Expected).unwrap().len(), 0);\n+    let all_notes = client_2.get_input_notes(NoteFilter::All).unwrap();\n+    let ignored_notes = client_2.get_input_notes(NoteFilter::Ignored).unwrap();\n+    let expected_notes = client_2.get_input_notes(NoteFilter::Expected).unwrap();\n+    assert!(all_notes.iter().any(|candidate_note| candidate_note.id() == note.id()));\n+    assert!(ignored_notes.iter().any(|candidate_note| candidate_note.id() == note.id()));\n+    assert!(expected_notes.iter().all(|candidate_note| candidate_note.id() != note.id()));\n \n     client_2.add_note_tag(untracked_tag).unwrap();\n \n     // After adding tag, the note stops being ignored\n-    assert_eq!(client_2.get_input_notes(NoteFilter::All).unwrap().len(), 1);\n-    assert_eq!(client_2.get_input_notes(NoteFilter::Ignored).unwrap().len(), 0);\n-    assert_eq!(client_2.get_input_notes(NoteFilter::Expected).unwrap().len(), 1);\n+    let all_notes = client_2.get_input_notes(NoteFilter::All).unwrap();\n+    let ignored_notes = client_2.get_input_notes(NoteFilter::Ignored).unwrap();\n+    let expected_notes = client_2.get_input_notes(NoteFilter::Expected).unwrap();\n+    assert!(all_notes.iter().any(|candidate_note| candidate_note.id() == note.id()));\n+    assert!(ignored_notes.iter().all(|candidate_note| candidate_note.id() != note.id()));\n+    assert!(expected_notes.iter().any(|candidate_note| candidate_note.id() == note.id()));\n }\n \n #[tokio::test]\n\n@@ -6,7 +6,7 @@ use miden_client::{\n use miden_objects::{\n     accounts::{AccountId, AccountStorageType},\n     assets::{Asset, FungibleAsset, TokenSymbol},\n-    notes::{NoteExecutionHint, NoteFile, NoteTag, NoteType},\n+    notes::{NoteExecutionHint, NoteFile, NoteId, NoteTag, NoteType},\n };\n \n use super::common::*;\n@@ -66,17 +66,17 @@ async fn test_swap_fully_onchain() {\n \n     // mint 1000 BTC for accountA\n     println!(\"minting 1000 btc for account A\");\n-    mint(\n+    let account_a_mint_note_id = mint(\n         &mut client_with_faucets,\n         account_a.id(),\n         btc_faucet_account.id(),\n         NoteType::Public,\n         BTC_MINT_AMOUNT,\n     )\n     .await;\n-    println!(\"minting 1000 eth for account B\");\n     // mint 1000 ETH for accountB\n-    mint(\n+    println!(\"minting 1000 eth for account B\");\n+    let account_b_mint_note_id = mint(\n         &mut client_with_faucets,\n         account_b.id(),\n         eth_faucet_account.id(),\n@@ -87,23 +87,27 @@ async fn test_swap_fully_onchain() {\n \n     // Sync and consume note for accountA\n     client1.sync_state().await.unwrap();\n-    let client_1_notes = client1.get_input_notes(miden_client::store::NoteFilter::All).unwrap();\n-    assert_eq!(client_1_notes.len(), 1);\n+    let client_1_consumable_notes = client1.get_consumable_notes(Some(account_a.id())).unwrap();\n+    assert!(client_1_consumable_notes\n+        .iter()\n+        .any(|(note, _)| note.id() == account_a_mint_note_id));\n \n     println!(\"Consuming mint note on first client...\");\n     let tx_template =\n-        TransactionTemplate::ConsumeNotes(account_a.id(), vec![client_1_notes[0].id()]);\n+        TransactionTemplate::ConsumeNotes(account_a.id(), vec![account_a_mint_note_id]);\n     let tx_request = client1.build_transaction_request(tx_template).unwrap();\n     execute_tx_and_sync(&mut client1, tx_request).await;\n \n     // Sync and consume note for accountB\n     client2.sync_state().await.unwrap();\n-    let client_2_notes = client2.get_input_notes(miden_client::store::NoteFilter::All).unwrap();\n-    assert_eq!(client_2_notes.len(), 1);\n+    let client_2_consumable_notes = client2.get_consumable_notes(Some(account_b.id())).unwrap();\n+    assert!(client_2_consumable_notes\n+        .iter()\n+        .any(|(note, _)| note.id() == account_b_mint_note_id));\n \n     println!(\"Consuming mint note on second client...\");\n     let tx_template =\n-        TransactionTemplate::ConsumeNotes(account_b.id(), vec![client_2_notes[0].id()]);\n+        TransactionTemplate::ConsumeNotes(account_b.id(), vec![account_b_mint_note_id]);\n     let tx_request = client2.build_transaction_request(tx_template).unwrap();\n     execute_tx_and_sync(&mut client2, tx_request).await;\n \n@@ -272,7 +276,7 @@ async fn test_swap_offchain() {\n \n     // mint 1000 BTC for accountA\n     println!(\"minting 1000 btc for account A\");\n-    mint(\n+    let account_a_mint_note_id = mint(\n         &mut client_with_faucets,\n         account_a.id(),\n         btc_faucet_account.id(),\n@@ -282,7 +286,7 @@ async fn test_swap_offchain() {\n     .await;\n     // mint 1000 ETH for accountB\n     println!(\"minting 1000 eth for account B\");\n-    mint(\n+    let account_b_mint_note_id = mint(\n         &mut client_with_faucets,\n         account_b.id(),\n         eth_faucet_account.id(),\n@@ -293,23 +297,27 @@ async fn test_swap_offchain() {\n \n     // Sync and consume note for accountA\n     client1.sync_state().await.unwrap();\n-    let client_1_notes = client1.get_input_notes(miden_client::store::NoteFilter::All).unwrap();\n-    assert_eq!(client_1_notes.len(), 1);\n+    let client_1_consumable_notes = client1.get_consumable_notes(Some(account_a.id())).unwrap();\n+    assert!(client_1_consumable_notes\n+        .iter()\n+        .any(|(note, _)| note.id() == account_a_mint_note_id));\n \n     println!(\"Consuming mint note on first client...\");\n     let tx_template =\n-        TransactionTemplate::ConsumeNotes(account_a.id(), vec![client_1_notes[0].id()]);\n+        TransactionTemplate::ConsumeNotes(account_a.id(), vec![account_a_mint_note_id]);\n     let tx_request = client1.build_transaction_request(tx_template).unwrap();\n     execute_tx_and_sync(&mut client1, tx_request).await;\n \n     // Sync and consume note for accountB\n     client2.sync_state().await.unwrap();\n-    let client_2_notes = client2.get_input_notes(miden_client::store::NoteFilter::All).unwrap();\n-    assert_eq!(client_2_notes.len(), 1);\n+    let client_2_consumable_notes = client2.get_consumable_notes(Some(account_b.id())).unwrap();\n+    assert!(client_2_consumable_notes\n+        .iter()\n+        .any(|(note, _)| note.id() == account_b_mint_note_id));\n \n     println!(\"Consuming mint note on second client...\");\n     let tx_template =\n-        TransactionTemplate::ConsumeNotes(account_b.id(), vec![client_2_notes[0].id()]);\n+        TransactionTemplate::ConsumeNotes(account_b.id(), vec![account_b_mint_note_id]);\n     let tx_request = client2.build_transaction_request(tx_template).unwrap();\n     execute_tx_and_sync(&mut client2, tx_request).await;\n \n@@ -468,8 +476,7 @@ fn build_swap_tag(\n     .unwrap()\n }\n \n-/// Mints a note from faucet_account_id for basic_account_id, waits for inclusion and returns it\n-/// with 1000 units of the corresponding fungible asset\n+/// Mints a note from faucet_account_id for basic_account_id with 1000 units of the corresponding fungible asset, waits for inclusion and returns the note id.\n ///\n /// `basic_account_id` does not need to be tracked by the client, but `faucet_account_id` does\n async fn mint(\n@@ -478,7 +485,7 @@ async fn mint(\n     faucet_account_id: AccountId,\n     note_type: NoteType,\n     mint_amount: u64,\n-) {\n+) -> NoteId {\n     // Create a Mint Tx for 1000 units of our fungible asset\n     let fungible_asset = FungibleAsset::new(faucet_account_id, mint_amount).unwrap();\n     let tx_template =\n@@ -487,4 +494,6 @@ async fn mint(\n     println!(\"Minting Asset\");\n     let tx_request = client.build_transaction_request(tx_template).unwrap();\n     execute_tx_and_sync(client, tx_request.clone()).await;\n+\n+    tx_request.expected_output_notes()[0].id()\n }\n"}
{"id": 26, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n#### Problem\r\n #18278 ignores 2 local-cluster tests failing consistently in CI\r\n\r\n\r\n#### Proposed Solution\r\nIdentify root cause of test failures, fix, and re-enable them\r\n\r\n\n\n### Code Patch:\n@@ -18,13 +18,11 @@ use {\n         crds_gossip_error::CrdsGossipError,\n         crds_value::CrdsValue,\n         ping_pong::PingCache,\n+        weighted_shuffle::weighted_shuffle,\n     },\n     itertools::Itertools,\n     lru::LruCache,\n-    rand::{\n-        distributions::{Distribution, WeightedIndex},\n-        Rng,\n-    },\n+    rand::Rng,\n     rayon::{prelude::*, ThreadPool},\n     solana_runtime::bloom::{AtomicBloom, Bloom},\n     solana_sdk::{\n@@ -239,10 +237,10 @@ impl CrdsGossipPull {\n         }\n         let mut peers = {\n             let mut rng = rand::thread_rng();\n-            let num_samples = peers.len() * 2;\n-            let index = WeightedIndex::new(weights).unwrap();\n-            let sample_peer = move || peers[index.sample(&mut rng)];\n-            repeat_with(sample_peer).take(num_samples)\n+            let mut seed = [0u8; 32];\n+            rng.fill(&mut seed[..]);\n+            let index = weighted_shuffle(&weights, seed);\n+            index.into_iter().map(|i| peers[i])\n         };\n         let peer = {\n             let mut rng = rand::thread_rng();\n@@ -916,7 +914,7 @@ pub(crate) mod tests {\n             &node_keypair.pubkey(),\n             0,\n         )));\n-        let node = CrdsGossipPull::default();\n+        let mut node = CrdsGossipPull::default();\n         let mut pings = Vec::new();\n         let ping_cache = Mutex::new(PingCache::new(\n             Duration::from_secs(20 * 60), // ttl\n@@ -954,25 +952,47 @@ pub(crate) mod tests {\n             ),\n             Err(CrdsGossipError::NoPeers)\n         );\n-        let new = ContactInfo::new_localhost(&solana_sdk::pubkey::new_rand(), 0);\n+        let now = 1625029781069;\n+        let new = ContactInfo::new_localhost(&solana_sdk::pubkey::new_rand(), now);\n         ping_cache\n             .lock()\n             .unwrap()\n             .mock_pong(new.id, new.gossip, Instant::now());\n         let new = CrdsValue::new_unsigned(CrdsData::ContactInfo(new));\n-        crds.insert(new.clone(), 0).unwrap();\n+        crds.insert(new.clone(), now).unwrap();\n         let req = node.new_pull_request(\n             &thread_pool,\n             &crds,\n             &node_keypair,\n             0,\n+            now,\n+            None,\n+            &HashMap::new(),\n+            PACKET_DATA_SIZE,\n+            &ping_cache,\n+            &mut pings,\n+        );\n+        let (peer, _) = req.unwrap();\n+        assert_eq!(peer, *new.contact_info().unwrap());\n+\n+        node.mark_pull_request_creation_time(new.contact_info().unwrap().id, now);\n+        let offline = ContactInfo::new_localhost(&solana_sdk::pubkey::new_rand(), now);\n+        let offline = CrdsValue::new_unsigned(CrdsData::ContactInfo(offline));\n+        crds.insert(offline, now).unwrap();\n+        let req = node.new_pull_request(\n+            &thread_pool,\n+            &crds,\n+            &node_keypair,\n             0,\n+            now,\n             None,\n             &HashMap::new(),\n             PACKET_DATA_SIZE,\n             &ping_cache,\n             &mut pings,\n         );\n+        // Even though the offline node should have higher weight, we shouldn't request from it\n+        // until we receive a ping.\n         let (peer, _) = req.unwrap();\n         assert_eq!(peer, *new.contact_info().unwrap());\n     }\n\n@@ -2848,14 +2848,12 @@ fn test_hard_fork_invalidates_tower() {\n \n #[test]\n #[serial]\n-#[ignore]\n fn test_no_optimistic_confirmation_violation_with_tower() {\n     do_test_optimistic_confirmation_violation_with_or_without_tower(true);\n }\n \n #[test]\n #[serial]\n-#[ignore]\n fn test_optimistic_confirmation_violation_without_tower() {\n     do_test_optimistic_confirmation_violation_with_or_without_tower(false);\n }\n"}
{"id": 19, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Flakiness Type\n\nAssertion failure\n\n### Name of Test\n\nextractors::start_time::tests::start_time_from_timestamp\n\n### Link to Test Run\n\nhttps://github.com/getsentry/relay/actions/runs/8050926586/job/21987677539\n\n### Details\n\nSeems like an issue between the generation of the `now` timestamp and the `system_time` timestamp.  We're likely crossing from second `n` to second `n+1` between calls.\r\n\r\n```\r\n---- extractors::start_time::tests::start_time_from_timestamp stdout ----\r\nthread 'extractors::start_time::tests::start_time_from_timestamp' panicked at relay-server\\src\\extractors\\start_time.rs:74:9:\r\nassertion `left == right` failed\r\n  left: 9\r\n right: 10\r\n\r\n\r\nfailures:\r\n    extractors::start_time::tests::start_time_from_timestamp\r\n\r\ntest result: FAILED. 225 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 4.77s\r\n\r\nerror: test failed, to rerun pass `-p relay-server --lib`\r\n```\n\n### Code Patch:\n@@ -63,14 +63,14 @@ mod tests {\n \n     #[test]\n     fn start_time_from_timestamp() {\n-        let elapsed = 10;\n+        let elapsed = Duration::from_secs(10);\n         let now = Instant::now();\n-        let system_time =\n-            SystemTime::now().duration_since(UNIX_EPOCH).unwrap() - Duration::from_secs(elapsed);\n+        let system_time = SystemTime::now().duration_since(UNIX_EPOCH).unwrap() - elapsed;\n         let start_time =\n             StartTime::from_timestamp_millis(system_time.as_millis() as u64).into_inner();\n \n         // Check that the difference between the now and generated start_time is about 10s.\n-        assert_eq!((now - start_time).as_secs(), elapsed);\n+        assert!((now - start_time) < elapsed + Duration::from_millis(50));\n+        assert!((now - start_time) > elapsed - Duration::from_millis(50));\n     }\n }\n"}
{"id": 80, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n`PendingAcks::reordering_threshold` defaults to 1. This causes an ACK to be sent immediately when a packet number is skipped:\r\n\r\nhttps://github.com/quinn-rs/quinn/blob/8076ffe94d38813ce0220af9d3438e7bfb5e8429/quinn-proto/src/connection/spaces.rs#L607-L609\r\n\r\nAs of #1613, we can randomly skip packet numbers, especially early in a connection. This causes a few extra ACKs to be sent at unpredictable times. This causes tests which count the number of ACKs sent in an interval to fail. We should modify such tests to use a reordering threshold greater than 1 to prevent these extra ACKs from being sent, or perhaps expose the number of skipped packets to allow them to account for the induced ACKs.\n\n### Code Patch:\n@@ -47,6 +47,8 @@ pub struct TransportConfig {\n     pub(crate) allow_spin: bool,\n     pub(crate) datagram_receive_buffer_size: Option<usize>,\n     pub(crate) datagram_send_buffer_size: usize,\n+    #[cfg(test)]\n+    pub(crate) deterministic_packet_numbers: bool,\n \n     pub(crate) congestion_controller_factory: Box<dyn congestion::ControllerFactory + Send + Sync>,\n }\n@@ -266,6 +268,16 @@ impl TransportConfig {\n         self\n     }\n \n+    /// Whether to force every packet number to be used\n+    ///\n+    /// By default, packet numbers are occasionally skipped to ensure peers aren't ACKing packets\n+    /// before they see them.\n+    #[cfg(test)]\n+    pub(crate) fn deterministic_packet_numbers(&mut self, enabled: bool) -> &mut Self {\n+        self.deterministic_packet_numbers = enabled;\n+        self\n+    }\n+\n     /// How to construct new `congestion::Controller`s\n     ///\n     /// Typically the refcounted configuration of a `congestion::Controller`,\n@@ -317,6 +329,8 @@ impl Default for TransportConfig {\n             allow_spin: true,\n             datagram_receive_buffer_size: Some(STREAM_RWND as usize),\n             datagram_send_buffer_size: 1024 * 1024,\n+            #[cfg(test)]\n+            deterministic_packet_numbers: false,\n \n             congestion_controller_factory: Box::new(Arc::new(congestion::CubicConfig::default())),\n         }\n\n@@ -326,6 +326,12 @@ impl Connection {\n             authentication_failures: 0,\n             error: None,\n             retry_token: Bytes::new(),\n+            #[cfg(test)]\n+            packet_number_filter: match config.deterministic_packet_numbers {\n+                false => PacketNumberFilter::new(&mut rng),\n+                true => PacketNumberFilter::disabled(),\n+            },\n+            #[cfg(not(test))]\n             packet_number_filter: PacketNumberFilter::new(&mut rng),\n \n             path_response: None,\n\n@@ -726,6 +726,15 @@ impl PacketNumberFilter {\n         }\n     }\n \n+    #[cfg(test)]\n+    pub(super) fn disabled() -> Self {\n+        Self {\n+            next_skipped_packet_number: u64::MAX,\n+            prev_skipped_packet_number: None,\n+            exponent: u32::MAX,\n+        }\n+    }\n+\n     pub(super) fn peek(&self, space: &PacketSpace) -> u64 {\n         let n = space.next_packet_number;\n         if n != self.next_skipped_packet_number {\n\n@@ -2212,8 +2212,8 @@ fn packet_splitting_not_necessary_after_higher_mtu_discovered() {\n #[test]\n fn single_ack_eliciting_packet_triggers_ack_after_delay() {\n     let _guard = subscribe();\n-    let mut pair = Pair::default();\n-    let (client_ch, _) = pair.connect();\n+    let mut pair = Pair::default_with_deterministic_pns();\n+    let (client_ch, _) = pair.connect_with(client_config_with_deterministic_pns());\n     pair.drive();\n \n     let stats_after_connect = pair.client_conn_mut(client_ch).stats();\n@@ -2275,8 +2275,8 @@ fn single_ack_eliciting_packet_triggers_ack_after_delay() {\n #[test]\n fn immediate_ack_triggers_ack() {\n     let _guard = subscribe();\n-    let mut pair = Pair::default();\n-    let (client_ch, _) = pair.connect();\n+    let mut pair = Pair::default_with_deterministic_pns();\n+    let (client_ch, _) = pair.connect_with(client_config_with_deterministic_pns());\n     pair.drive();\n \n     let acks_after_connect = pair.client_conn_mut(client_ch).stats().frame_rx.acks;\n@@ -2294,8 +2294,8 @@ fn immediate_ack_triggers_ack() {\n #[test]\n fn out_of_order_ack_eliciting_packet_triggers_ack() {\n     let _guard = subscribe();\n-    let mut pair = Pair::default();\n-    let (client_ch, server_ch) = pair.connect();\n+    let mut pair = Pair::default_with_deterministic_pns();\n+    let (client_ch, server_ch) = pair.connect_with(client_config_with_deterministic_pns());\n     pair.drive();\n \n     let default_mtu = pair.mtu;\n@@ -2352,8 +2352,8 @@ fn out_of_order_ack_eliciting_packet_triggers_ack() {\n #[test]\n fn single_ack_eliciting_packet_with_ce_bit_triggers_immediate_ack() {\n     let _guard = subscribe();\n-    let mut pair = Pair::default();\n-    let (client_ch, _) = pair.connect();\n+    let mut pair = Pair::default_with_deterministic_pns();\n+    let (client_ch, _) = pair.connect_with(client_config_with_deterministic_pns());\n     pair.drive();\n \n     let stats_after_connect = pair.client_conn_mut(client_ch).stats();\n@@ -2388,7 +2388,7 @@ fn single_ack_eliciting_packet_with_ce_bit_triggers_immediate_ack() {\n }\n \n fn setup_ack_frequency_test(max_ack_delay: Duration) -> (Pair, ConnectionHandle, ConnectionHandle) {\n-    let mut client_config = client_config();\n+    let mut client_config = client_config_with_deterministic_pns();\n     let mut ack_freq_config = AckFrequencyConfig::default();\n     ack_freq_config\n         .ack_eliciting_threshold(10u32.into())\n@@ -2398,7 +2398,7 @@ fn setup_ack_frequency_test(max_ack_delay: Duration) -> (Pair, ConnectionHandle,\n         .ack_frequency_config(Some(ack_freq_config))\n         .mtu_discovery_config(None); // To keep traffic cleaner\n \n-    let mut pair = Pair::default();\n+    let mut pair = Pair::default_with_deterministic_pns();\n     pair.latency = Duration::from_millis(10); // Need latency to avoid an RTT = 0\n     let (client_ch, server_ch) = pair.connect_with(client_config);\n     pair.drive();\n\n@@ -37,6 +37,14 @@ pub(super) struct Pair {\n }\n \n impl Pair {\n+    pub(super) fn default_with_deterministic_pns() -> Self {\n+        let mut cfg = server_config();\n+        let mut transport = TransportConfig::default();\n+        transport.deterministic_packet_numbers(true);\n+        cfg.transport = Arc::new(transport);\n+        Self::new(Default::default(), cfg)\n+    }\n+\n     pub(super) fn new(endpoint_config: Arc<EndpointConfig>, server_config: ServerConfig) -> Self {\n         let server = Endpoint::new(endpoint_config.clone(), Some(Arc::new(server_config)), true);\n         let client = Endpoint::new(endpoint_config, None, true);\n@@ -466,6 +474,14 @@ pub(super) fn client_config() -> ClientConfig {\n     ClientConfig::new(Arc::new(client_crypto()))\n }\n \n+pub(super) fn client_config_with_deterministic_pns() -> ClientConfig {\n+    let mut cfg = ClientConfig::new(Arc::new(client_crypto()));\n+    let mut transport = TransportConfig::default();\n+    transport.deterministic_packet_numbers(true);\n+    cfg.transport = Arc::new(transport);\n+    cfg\n+}\n+\n pub(super) fn client_config_with_certs(certs: Vec<rustls::Certificate>) -> ClientConfig {\n     ClientConfig::new(Arc::new(client_crypto_with_certs(certs)))\n }\n"}
{"id": 57, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Summary**\r\n\r\nhttps://github.com/datafuselabs/databend/actions/runs/6321697555/job/17166517616?pr=12998\r\n\r\n```\r\n0: statement failed: ClickHouse client error: . Code: 500, Text = TableLockExpired. Code: 2016, Text = the acquired table lock has expired..\r\n[SQL] merge into t_stored_merge as t1 using (select * from t_source_merge as t2)  on t1.a = t2.a  when matched then update set t1.c = t2.c  when not matched then insert (a,c) values(t2.a,t2.c);\r\n```\r\nat tests/sqllogictests/suites/ee/01_ee_system/01_0001_computed_column:250\n\n### Code Patch:\n@@ -3220,7 +3220,7 @@ impl<KV: kvapi::KVApi<Error = MetaError> + ?Sized> SchemaApi for KV {\n             let if_then = vec![txn_op_put_with_expire(\n                 &key,\n                 serialize_struct(&lock_meta)?,\n-                req.expire_at,\n+                SeqV::<()>::now_ms() / 1000 + req.expire_secs,\n             )];\n \n             let txn_req = TxnRequest {\n@@ -3284,7 +3284,7 @@ impl<KV: kvapi::KVApi<Error = MetaError> + ?Sized> SchemaApi for KV {\n             let if_then = vec![txn_op_put_with_expire(\n                 &key,\n                 serialize_struct(&lock_meta)?,\n-                req.expire_at,\n+                SeqV::<()>::now_ms() / 1000 + req.expire_secs,\n             )];\n \n             let txn_req = TxnRequest {\n\n@@ -5237,7 +5237,7 @@ impl SchemaApiTestSuite {\n             info!(\"--- create table lock revision 1\");\n             let req1 = CreateLockRevReq {\n                 lock_key: LockKey::Table { table_id },\n-                expire_at: (Utc::now().timestamp() + 2) as u64,\n+                expire_secs: 2,\n                 user: \"root\".to_string(),\n                 node: \"node1\".to_string(),\n                 query_id: \"query1\".to_string(),\n@@ -5247,7 +5247,7 @@ impl SchemaApiTestSuite {\n             info!(\"--- create table lock revision 2\");\n             let req2 = CreateLockRevReq {\n                 lock_key: LockKey::Table { table_id },\n-                expire_at: (Utc::now().timestamp() + 2) as u64,\n+                expire_secs: 2,\n                 user: \"root\".to_string(),\n                 node: \"node1\".to_string(),\n                 query_id: \"query2\".to_string(),\n@@ -5267,7 +5267,7 @@ impl SchemaApiTestSuite {\n             info!(\"--- extend table lock revision 2 expire\");\n             let req4 = ExtendLockRevReq {\n                 lock_key: LockKey::Table { table_id },\n-                expire_at: (Utc::now().timestamp() + 4) as u64,\n+                expire_secs: 4,\n                 revision: res2.revision,\n                 acquire_lock: true,\n             };\n\n@@ -109,7 +109,7 @@ pub struct ListLockRevReq {\n #[derive(serde::Serialize, serde::Deserialize, Clone, Debug, PartialEq, Eq)]\n pub struct CreateLockRevReq {\n     pub lock_key: LockKey,\n-    pub expire_at: u64,\n+    pub expire_secs: u64,\n     pub user: String,\n     pub node: String,\n     pub query_id: String,\n@@ -123,7 +123,7 @@ pub struct CreateLockRevReply {\n #[derive(serde::Serialize, serde::Deserialize, Clone, Debug, PartialEq, Eq)]\n pub struct ExtendLockRevReq {\n     pub lock_key: LockKey,\n-    pub expire_at: u64,\n+    pub expire_secs: u64,\n     pub revision: u64,\n     pub acquire_lock: bool,\n }\n\n@@ -14,7 +14,6 @@\n \n use std::sync::Arc;\n \n-use chrono::Utc;\n use common_exception::Result;\n use common_meta_app::schema::CreateLockRevReq;\n use common_meta_app::schema::DeleteLockRevReq;\n@@ -65,7 +64,7 @@ pub trait LockExt: Lock {\n             user,\n             node,\n             query_id,\n-            expire_at: Utc::now().timestamp() as u64 + expire_secs,\n+            expire_secs,\n         }\n     }\n \n@@ -92,7 +91,7 @@ pub trait LockExt: Lock {\n             lock_key: self.gen_lock_key(),\n             revision,\n             acquire_lock,\n-            expire_at: Utc::now().timestamp() as u64 + expire_secs,\n+            expire_secs,\n         }\n     }\n }\n\n@@ -272,7 +272,7 @@ impl FuseTable {\n             });\n     }\n \n-    // TODO refactor, it is called by segment compaction and re-cluster now\n+    // TODO refactor, it is called by segment compaction\n     #[async_backtrace::framed]\n     pub async fn commit_mutation(\n         &self,\n"}
{"id": 52, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nDetected by: https://github.com/paritytech/subxt/actions/runs/6745755385/job/18338194383\r\n\r\n\r\n```bash\r\n--- STDERR:              integration-tests full_client::frame::staking::tx_bond ---\r\nthread 'full_client::frame::staking::tx_bond' panicked at testing/integration-tests/src/full_client/frame/staking.rs:216:5:\r\nassertion failed: `Err(Transaction(Invalid(\"Invalid transaction: Transaction is outdated\")))` does not match `Err(Error::Runtime(DispatchError::Module(err)))`\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n```\n\n### Code Patch:\n@@ -436,10 +436,26 @@ impl<T: Config + Send + Sync + 'static> Backend<T> for UnstableBackend<T> {\n         &self,\n         extrinsic: &[u8],\n     ) -> Result<StreamOfResults<TransactionStatus<T::Hash>>, Error> {\n-        // First, subscribe to all new block hashes\n-        let mut new_blocks = self.follow_handle.subscribe().events().filter_map(|ev| {\n+        // We care about new and finalized block hashes.\n+        enum SeenBlock<Ref> {\n+            New(Ref),\n+            Finalized(Vec<Ref>),\n+        }\n+        enum SeenBlockMarker {\n+            New,\n+            Finalized,\n+        }\n+\n+        // First, subscribe to all new and finalized block refs.\n+        // - we subscribe to new refs so that when we see `BestChainBlockIncluded`, we\n+        //   can try to return a block ref for the best block.\n+        // - we subscribe to finalized refs so that when we see `Finalized`, we can\n+        //   guarantee that when we return here, the finalized block we report has been\n+        //   reported from chainHead_follow already.\n+        let mut seen_blocks_sub = self.follow_handle.subscribe().events().filter_map(|ev| {\n             std::future::ready(match ev {\n-                FollowEvent::NewBlock(ev) => Some(ev.block_hash),\n+                FollowEvent::NewBlock(ev) => Some(SeenBlock::New(ev.block_hash)),\n+                FollowEvent::Finalized(ev) => Some(SeenBlock::Finalized(ev.finalized_block_hashes)),\n                 _ => None,\n             })\n         });\n@@ -453,8 +469,9 @@ impl<T: Config + Send + Sync + 'static> Backend<T> for UnstableBackend<T> {\n         let mut seen_blocks = HashMap::new();\n         let mut done = false;\n \n-        // If we see the finalized event, we start waiting until we find a block that\n-        // matches, so we can guarantee to return a pinned block hash.\n+        // If we see the finalized event, we start waiting until we find a finalized block that\n+        // matches, so we can guarantee to return a pinned block hash and be properly in sync\n+        // with chainHead_follow.\n         let mut finalized_hash: Option<T::Hash> = None;\n \n         // Now we can attempt to associate tx events with pinned blocks.\n@@ -465,25 +482,42 @@ impl<T: Config + Send + Sync + 'static> Backend<T> for UnstableBackend<T> {\n                     return Poll::Ready(None);\n                 }\n \n-                // Save any pinned blocks. Keep doing this until no more, so that we always have the most uptodate\n-                // pinned blocks when we are looking at our tx events.\n-                if let Poll::Ready(Some(block_ref)) = new_blocks.poll_next_unpin(cx) {\n-                    seen_blocks.insert(block_ref.hash(), block_ref);\n+                // Make a note of new or finalized blocks that have come in since we started the TX.\n+                if let Poll::Ready(Some(seen_block)) = seen_blocks_sub.poll_next_unpin(cx) {\n+                    match seen_block {\n+                        SeenBlock::New(block_ref) => {\n+                            // Optimization: once we have a `finalized_hash`, we only care about finalized\n+                            // block refs now and can avoid bothering to save new blocks.\n+                            if finalized_hash.is_none() {\n+                                seen_blocks\n+                                    .insert(block_ref.hash(), (SeenBlockMarker::New, block_ref));\n+                            }\n+                        }\n+                        SeenBlock::Finalized(block_refs) => {\n+                            for block_ref in block_refs {\n+                                seen_blocks.insert(\n+                                    block_ref.hash(),\n+                                    (SeenBlockMarker::Finalized, block_ref),\n+                                );\n+                            }\n+                        }\n+                    }\n                     continue;\n                 }\n \n                 // If we have a finalized hash, we are done looking for tx events and we are just waiting\n                 // for a pinned block with a matching hash (which must appear eventually given it's finalized).\n                 if let Some(hash) = &finalized_hash {\n-                    if let Some(block_ref) = seen_blocks.remove(hash) {\n+                    if let Some((SeenBlockMarker::Finalized, block_ref)) = seen_blocks.remove(hash)\n+                    {\n                         // Found it! Hand back the event with a pinned block. We're done.\n                         done = true;\n                         let ev = TransactionStatus::InFinalizedBlock {\n                             hash: block_ref.into(),\n                         };\n                         return Poll::Ready(Some(Ok(ev)));\n                     } else {\n-                        // Keep waiting for more new blocks until we find it (get rid of any other block refs\n+                        // Keep waiting for more finalized blocks until we find it (get rid of any other block refs\n                         // now, since none of them were what we were looking for anyway).\n                         seen_blocks.clear();\n                         continue;\n@@ -517,8 +551,8 @@ impl<T: Config + Send + Sync + 'static> Backend<T> for UnstableBackend<T> {\n                         // block that likely isn't accessible. We have no guarantee that a best\n                         // block on the node a tx was sent to will ever be known about on the\n                         // chainHead_follow subscription.\n-                        let block_ref = match seen_blocks.get(&block.hash).cloned() {\n-                            Some(block_ref) => block_ref.into(),\n+                        let block_ref = match seen_blocks.get(&block.hash) {\n+                            Some((_, block_ref)) => block_ref.clone().into(),\n                             None => BlockRef::from_hash(block.hash),\n                         };\n                         TransactionStatus::InBestBlock { hash: block_ref }\n"}
{"id": 47, "ground_truth": "Concurrency", "predicted": "Async Wait", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n> Uhmm this seems to make the HTTP node tests fail sometimes, but not consistently. Not sure why, will look into it. @rbehjati let me know if you have any suggestions on why that may be happening\r\n\r\nso actually I tried again from `main`, and it seems that tests are flaky there too. To reproduce:\r\n\r\n```bash\r\nexport RUST_LOG=debug\r\ncd oak_runtime\r\ncargo test --package oak_runtime --lib -- node --nocapture\r\n```\r\n\r\nand then rerun the `cargo test` a few times. I got a failure roughtly 1 out of 5 times.\r\n\r\n<details><summary>logs</summary>\r\n<p>\r\n\r\n```\r\n    Finished test [unoptimized + debuginfo] target(s) in 0.10s\r\n     Running target/debug/deps/oak_runtime-f41ce2eeb10d11e5\r\n\r\nrunning 19 tests\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 10859620126471780718 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 14154969170481411718 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 6028826501016594447 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 16096544177349775597 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((10859620126471780718, 14154969170481411718))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 16698263373585990938 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9092960943208256323 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 244963718117020840 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((16096544177349775597, 9092960943208256323))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((16698263373585990938, 244963718117020840))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 2638176703059791071 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 15754702411688475858 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 16409535109310484427 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((6028826501016594447, 15754702411688475858))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 11927899315350453922 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9360967439716317465 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((2638176703059791071, 16409535109310484427))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 11888636142326083607 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_other_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export not found\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((11927899315350453922, 11888636142326083607))\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 10552541542430126522 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 1\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 12335443945007934671 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 15583588925823706518 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 2196097866962909478 maps to Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((10552541542430126522, 12335443945007934671))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 1767916906041923217 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 1\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 3120311075007141291 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((15583588925823706518, 2196097866962909478))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 1291357445641949921 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9571224628934017081 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 1\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(11927899315350453922, Message { bytes: [10, 9, 9, 150, 229, 110, 9, 144, 7, 68, 216], handles: [15583588925823706518] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(15754702411688475858)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((3120311075007141291, 9571224628934017081))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 3555956587273005122 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(11927899315350453922, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(15754702411688475858) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 6330247248751785548 maps to Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(11927899315350453922)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 14923236747254485945 maps to Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((3555956587273005122, 14923236747254485945))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 6028826501016594447 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(2638176703059791071, Message { bytes: [10, 9, 9, 66, 40, 30, 5, 173, 75, 89, 49], handles: [3555956587273005122] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 6028826501016594447 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((1291357445641949921, 6330247248751785548))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(2638176703059791071, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(11927899315350453922) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(10552541542430126522, Message { bytes: [10, 9, 9, 225, 82, 139, 65, 193, 210, 235, 17], handles: [1291357445641949921] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(2638176703059791071)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(10552541542430126522, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(2638176703059791071) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(10552541542430126522)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\ntest node::wasm::tests::wasm_starting_module_without_content_fails ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(10552541542430126522) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 580815582432483639 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((1767916906041923217, 580815582432483639))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): wait_on_channels: channels not ready, parking thread Thread { id: ThreadId(16), name: None }\r\n[2020-09-10T20:47:15Z DEBUG rustls::anchors] add_pem_file processed 1 valid and 0 invalid certs\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9155368652591746135 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG rustls::anchors] add_pem_file processed 1 valid and 0 invalid certs\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Waiting for invocation channel\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG rustls::anchors] add_pem_file processed 1 valid and 0 invalid certs\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): wait_on_channels: channels not ready, parking thread Thread { id: ThreadId(19), name: None }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(12335443945007934671)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((9360967439716317465, 9155368652591746135))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 11080559098524874464 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::dns] resolving host=\"localhost\"\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(12335443945007934671) -> Ok(Some(Message { bytes: [10, 9, 9, 225, 82, 139, 65, 193, 210, 235, 17], handles: [11080559098524874464] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(12335443945007934671)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(12335443945007934671) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Starting HTTP server pseudo-Node on: [::]:2527\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to [::1]:2527\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::dns] resolving host=\"localhost\"\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::dns] resolving host=\"localhost\"\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connected to [::1]:2527\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] ThreadId(18): Started HTTP server pseudo-node on port 2527\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] No cached session for DNSNameRef(\"localhost\")\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] Not resuming any session\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::http] Received incoming TLS stream: TcpStream { addr: V6([::1]:2527), peer: V6([::1]:33922), fd: 21 }\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to [::1]:2525\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Waiting for invocation channel\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to 127.0.0.1:2525\r\nthread 'node::http::tests::test_https_server_can_serve_https_requests' panicked at 'assertion failed: resp.is_ok()', src/node/http/tests.rs:91[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(11888636142326083607)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 15642419903814064830 maps to Channel 1 WRITE\r\n:5\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\ntest node::http::tests::test_https_server_can_serve_https_requests ... FAILED\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(11888636142326083607) -> Ok(Some(Message { bytes: [10, 9, 9, 150, 229, 110, 9, 144, 7, 68, 216], handles: [15642419903814064830] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(11888636142326083607)\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Waiting for invocation channel\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }, Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [2, 2, 2] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(11888636142326083607) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to [::1]:2526\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Starting HTTP server pseudo-Node on: [::]:2526\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to 127.0.0.1:2526\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }, Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [2, 2, 2] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connected to 127.0.0.1:2526\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] ThreadId(14): Started HTTP server pseudo-node on port 2526\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_channel_with_more_confidential_label_from_non_public_node_with_privilege_err ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::http] Received incoming TLS stream: TcpStream { addr: V6([::ffff:127.0.0.1]:2526), peer: V6([::ffff:127.0.0.1]:33940), fd: 22 }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 11705927381946070715 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 490605741076439707 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 5932870466176810717 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 13595452974177765549 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG rustls::server::hs] decided upon suite SupportedCipherSuite { suite: TLS13_CHACHA20_POLY1305_SHA256, kx: BulkOnly, bulk: CHACHA20_POLY1305, hash: SHA256, sign: None, enc_key_len: 32, fixed_iv_len: 12, explicit_nonce_len: 0 }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((11705927381946070715, 13595452974177765549))\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] flushed 57 bytes\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((490605741076439707, 5932870466176810717))\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::node::http] Client-connection error: Custom { kind: InvalidData, error: CorruptMessage }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::node::http] Error when processing TLS stream: Custom { kind: Other, error: \"TLS Error: Custom { kind: InvalidData, error: CorruptMessage }\" }\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(16409535109310484427)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] read 0 bytes\r\n[2020-09-10T20:47:15Z ERROR oak_runtime] Wasm module signature verification failed for oak_module: ring::error::Unspecified\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 10709409905608189541 maps to Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::conn] parse error (connection closed before message completed) with 0 bytes\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::dispatch] read_head error: connection closed before message completed\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(16409535109310484427) -> Ok(Some(Message { bytes: [10, 9, 9, 66, 40, 30, 5, 173, 75, 89, 49], handles: [10709409905608189541] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 13595452974177765549 => Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 11705927381946070715 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(16409535109310484427)\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG hyper::server::shutdown] signal received, starting graceful shutdown\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(16409535109310484427) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] HTTP server pseudo-node terminated with Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] test-node: Starting HTTP server pseudo-Node on: [::]:2525\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_verify_module_signature_fails ... [2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\nok[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 15583588925823706518 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 2196097866962909478 => Channel 1 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] ThreadId(15): Started HTTP server pseudo-node on port 2525\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 15642419903814064830 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG hyper::server::shutdown] signal received, starting graceful shutdown\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] HTTP server pseudo-node terminated with Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 15583588925823706518 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 2196097866962909478 => Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 8188998132612729810 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 15642419903814064830 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 5748044951194180441 maps to Channel 0 READ\r\n\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Ok((8188998132612729810, 5748044951194180441))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 1 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_write(8188998132612729810, Message { bytes: [14, 12, 88], handles: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=1, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_write(8188998132612729810, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_read(5748044951194180441)\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=1, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): cannot read from Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_read(5748044951194180441) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: [8188998132612729810, 5748044951194180441]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] Using ciphersuite TLS13_CHACHA20_POLY1305_SHA256\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\ntest node::http::tests::test_https_server_cannot_serve_http_requests ... ok\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\ntest tests::create_channel_with_more_confidential_label_from_public_untrusted_node_ok ... ok\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] Not resuming\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] TLS1.3 encrypted extensions: [ServerNameAck]\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] ALPN protocol is None\r\n[2020-09-10T20:47:15Z WARN  rustls::session] Sending fatal alert BadCertificate\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::node::http] Client-connection error: Custom { kind: UnexpectedEof, error: \"tls handshake eof\" }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::node::http] Error when processing TLS stream: Custom { kind: Other, error: \"TLS Error: Custom { kind: UnexpectedEof, error: \\\"tls handshake eof\\\" }\" }\r\n[2020-09-10T20:47:15Z DEBUG rustls::anchors] add_pem_file processed 1 valid and 0 invalid certs\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::dns] resolving host=\"localhost\"\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connecting to [::1]:2527\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::http] Received incoming TLS stream: TcpStream { addr: V6([::1]:2527), peer: V6([::1]:33932), fd: 5 }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::connect::http] connected to [::1]:2527\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] No cached session for DNSNameRef(\"localhost\")\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] Not resuming any session\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\ntest tests::create_node_more_confidential_label_ok ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_node_invalid_configuration_err ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 0\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 1052340951842084367 maps to Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 279083569327136254 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Ok((1052340951842084367, 279083569327136254))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_write(1052340951842084367, Message { bytes: [14, 12, 88], handles: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_write(1052340951842084367, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_read(279083569327136254)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_read(279083569327136254) -> Ok(Some(Message { bytes: [14, 12, 88], handles: [] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: [279083569327136254, 1052340951842084367]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG rustls::server::hs] decided upon suite SupportedCipherSuite { suite: TLS13_CHACHA20_POLY1305_SHA256, kx: BulkOnly, bulk: CHACHA20_POLY1305, hash: SHA256, sign: None, enc_key_len: 32, fixed_iv_len: 12, explicit_nonce_len: 0 }\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_channel_with_more_confidential_label_from_public_node_with_privilege_ok ... ok\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_node_less_confidential_label_err ... ok\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create runtime for test\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Create test Node\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] Using ciphersuite TLS13_CHACHA20_POLY1305_SHA256\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Start test Node instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] NodeId(1): cannot write to Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(1): channel_create(Label { confidentiality_tags: [Tag { tag: Some(GrpcTag(GrpcTag { authorization_bearer_token_hmac: [1, 1, 1] })) }], integrity_tags: [] }) -> Err(ErrPermissionDenied)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: []\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] Not resuming\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node TestNode(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::tests] Stop runtime..done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\ntest tests::create_node_same_label_ok ... ok\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] TLS1.3 encrypted extensions: [ServerNameAck]\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::hs] ALPN protocol is None\r\n[2020-09-10T20:47:15Z DEBUG rustls::client::tls13] Ticket saved\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] flushed 57 bytes\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] read 57 bytes\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] parsed 2 headers\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::conn] incoming body is empty\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] Handling HTTP request; request size: 0 bytes, label: Label { confidentiality_tags: [], integrity_tags: [] }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::http] Inject the request into the Oak Node\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 2\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 9961052006950199576 maps to Channel 2 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 1791300487480186213 maps to Channel 2 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((9961052006950199576, 1791300487480186213))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] create new Channel object with ID 3\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 14881253896451425314 maps to Channel 3 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 3680328595663417379 maps to Channel 3 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_create(Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok((14881253896451425314, 3680328595663417379))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(9961052006950199576, Message { bytes: [10, 1, 47, 18, 3, 103, 101, 116, 34, 15, 10, 13, 111, 97, 107, 45, 108, 97, 98, 101, 108, 45, 98, 105, 110, 34, 22, 10, 4, 104, 111, 115, 116, 18, 14, 108, 111, 99, 97, 108, 104, 111, 115, 116, 58, 50, 53, 50, 55], handles: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(9961052006950199576, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(11080559098524874464, Message { bytes: [10, 9, 9, 101, 149, 61, 122, 77, 250, 219, 24, 18, 9, 9, 34, 132, 98, 22, 147, 213, 132, 206], handles: [1791300487480186213, 14881253896451425314] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(11080559098524874464, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): wait_on_channels: thread Thread { id: ThreadId(19), name: None } re-woken\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9961052006950199576)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 2 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9961052006950199576) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(1791300487480186213)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(6330247248751785548)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(1791300487480186213) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(14881253896451425314)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 7424337281389512419 maps to Channel 2 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(0): new ABI handle 12021382004607466382 maps to Channel 3 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(6330247248751785548) -> Ok(Some(Message { bytes: [10, 9, 9, 101, 149, 61, 122, 77, 250, 219, 24, 18, 9, 9, 34, 132, 98, 22, 147, 213, 132, 206], handles: [7424337281389512419, 12021382004607466382] }))\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(12021382004607466382, Message { bytes: [16, 200, 1], handles: [] })\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_write(12021382004607466382, ...) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(14881253896451425314) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] Generating response for runtime 0 and reader 3680328595663417379.\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): wait_on_channels(count=1) -> Ok([ReadReady])\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(3680328595663417379)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_read(3680328595663417379) -> Ok(Some(Message { bytes: [16, 200, 1], handles: [] }))\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] flushed 75 bytes\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] read 75 bytes\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::io] parsed 2 headers\r\n[2020-09-10T20:47:15Z DEBUG hyper::proto::h1::conn] incoming body is empty\r\n[2020-09-10T20:47:15Z DEBUG hyper::client::pool] pooling idle connection for (\"https\", localhost:2527)\r\n[2020-09-10T20:47:15Z DEBUG hyper::server::shutdown] signal received, starting graceful shutdown\r\n[2020-09-10T20:47:15Z DEBUG rustls::session] Sending warning alert CloseNotify\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::http] HTTP server pseudo-node terminated with Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 7424337281389512419 => Channel 2 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 12021382004607466382 => Channel 3 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 3680328595663417379 => Channel 3 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 1291357445641949921 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 6330247248751785548 => Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 11080559098524874464 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 7424337281389512419 => Channel 2 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 12021382004607466382 => Channel 3 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 3680328595663417379 => Channel 3 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 1291357445641949921 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 6330247248751785548 => Channel 1 READ\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 11080559098524874464 => Channel 1 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=2, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=2, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 3 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=3, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=3, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 1 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=1, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=1, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::http::tests::test_https_server_does_not_terminate_after_a_bad_request ... ok\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export has incorrect function signature: Signature { params: [], return_type: None }\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9571224628934017081)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9571224628934017081) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 3120311075007141291 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 3120311075007141291 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_starting_module_with_wrong_signature_2_fails ... ok\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export has incorrect function signature: Signature { params: [I64, I32, I32, I32, I32, I32, I32], return_type: Some(I32) }\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9155368652591746135)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9155368652591746135) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 9360967439716317465 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 9360967439716317465 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_starting_module_with_wrong_signature_fails ... ok\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::wasm] entrypoint 'oak_main' export validated\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] Wasm module SHA-256 hash: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 4251925570913386371 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] NodeId(0): start node instance NodeId(1) with privilege NodePrivilege { can_declassify_confidentiality_tags: {Tag { tag: Some(WebAssemblyModuleTag(WebAssemblyModuleTag { web_assembly_module_hash_sha_256: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115] })) }}, can_endorse_integrity_tags: {Tag { tag: Some(WebAssemblyModuleTag(WebAssemblyModuleTag { web_assembly_module_hash_sha_256: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115] })) }} }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(5932870466176810717)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] test: running entrypoint 'oak_main'\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(5932870466176810717) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 490605741076439707 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(1) handle 4251925570913386371 => Channel 0 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping node NodeId(1) ...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node test(1)...\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export not found\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_other_main' export not found\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(244963718117020840)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_other_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(244963718117020840) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9092960943208256323)\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(9092960943208256323) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 16698263373585990938 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime::node::wasm] entrypoint 'oak_main' export validated\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 16096544177349775597 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 16698263373585990938 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 16096544177349775597 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] Wasm module SHA-256 hash: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115]\r\ntest node::wasm::tests::wasm_starting_module_missing_an_export_fails ... ok\r\ntest node::wasm::tests::wasm_starting_module_with_wrong_export_fails ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): new ABI handle 5475638980536319391 maps to Channel 0 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] NodeId(0): start node instance NodeId(1) with privilege NodePrivilege { can_declassify_confidentiality_tags: {Tag { tag: Some(WebAssemblyModuleTag(WebAssemblyModuleTag { web_assembly_module_hash_sha_256: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115] })) }}, can_endorse_integrity_tags: {Tag { tag: Some(WebAssemblyModuleTag(WebAssemblyModuleTag { web_assembly_module_hash_sha_256: [26, 161, 141, 245, 243, 77, 146, 104, 219, 10, 136, 14, 100, 21, 76, 8, 240, 205, 105, 95, 72, 171, 239, 72, 123, 153, 124, 126, 132, 34, 230, 115] })) }} }\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] test: running entrypoint 'oak_main'\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Ok(())\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(14154969170481411718)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(14154969170481411718) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 10859620126471780718 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(1) handle 5475638980536319391 => Channel 0 READ\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping node NodeId(1) ...\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node test(1)...\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] entrypoint 'oak_main' export has incorrect function signature: Signature { params: [I64], return_type: Some(I32) }\r\n[2020-09-10T20:47:15Z WARN  oak_runtime::node::wasm] could not validate entrypoint: ErrInvalidArgs\r\n[2020-09-10T20:47:15Z WARN  oak_runtime] could not create node: IncorrectWebAssemblyModuleName\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): node_create(NodeConfiguration { name: \"test\", config_type: Some(WasmConfig(WebAssemblyConfiguration { wasm_module_name: \"oak_module\", wasm_entrypoint_name: \"oak_main\" })) }, Label { confidentiality_tags: [], integrity_tags: [] }) -> Err(ErrInvalidArgs)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(580815582432483639)\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::proxy] NodeId(0): channel_close(580815582432483639) -> Ok(())\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 1767916906041923217 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 1767916906041923217 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_starting_module_with_wrong_signature_3_fails ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] test: entrypoint 'oak_main' completed\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: [4251925570913386371]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node test(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping node NodeId(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 490605741076439707 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_verify_module_signature_succeeds ... ok\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::node::wasm] test: entrypoint 'oak_main' completed\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] NodeId(1): remove_node_id() found open handles on exit: [5475638980536319391]\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] join thread for node test(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping node NodeId(1)...done\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] stopping runtime instance\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime] waking waiters on NodeId(0) handle 10859620126471780718 => Channel 0 WRITE\r\n[2020-09-10T20:47:15Z INFO  oak_runtime] Runtime instance dropped\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] last writer for channel 0 gone, wake waiters\r\n[2020-09-10T20:47:15Z DEBUG oak_runtime::channel] dropping Channel object Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\n[2020-09-10T20:47:15Z ERROR oak_runtime::channel] Couldn't send ChannelDestroyed event, since the runtime has been dropped. Channel { id=0, #readers=0, #writers=0, label=Label { confidentiality_tags: [], integrity_tags: [] } }\r\ntest node::wasm::tests::wasm_starting_minimal_module_succeeds ... ok\r\n\r\n```\r\n\r\n</p>\r\n</summary>\r\n\r\n_Originally posted by @tiziano88 in https://github.com/project-oak/oak/pull/1461#issuecomment-690563634_\n\n### Code Patch:\n@@ -30,7 +30,11 @@ struct HttpServerTester {\n }\n \n impl HttpServerTester {\n-    fn new(port: u32, with_simulator_thread: bool) -> HttpServerTester {\n+    /// Create a new runtime and test server.\n+    /// port: The port on which the server is started\n+    /// number_of_requests: the number of requests the server expects to receive. This parameter is\n+    /// passed to `oak_node_simulator` to set up the Oak node that servers the requests.\n+    fn new(port: u32, number_of_requests: i32) -> HttpServerTester {\n         let runtime = create_runtime();\n         let server_node = create_server_node(port);\n         let (init_receiver, invocation_receiver) = create_communication_channel(&runtime);\n@@ -45,14 +49,10 @@ impl HttpServerTester {\n \n         // Simulate an Oak node that responds with 200 (OK) to every request it receives\n         // TODO(#1186): Use tokio instead of spawning a thread.\n-        let oak_node_simulator_thread = if with_simulator_thread {\n-            let runtime_proxy = runtime.clone();\n-            Some(std::thread::spawn(move || {\n-                oak_node_simulator(&runtime_proxy, invocation_receiver);\n-            }))\n-        } else {\n-            None\n-        };\n+        let runtime_proxy = runtime.clone();\n+        let oak_node_simulator_thread = Some(std::thread::spawn(move || {\n+            oak_node_simulator(number_of_requests, &runtime_proxy, invocation_receiver);\n+        }));\n \n         HttpServerTester {\n             runtime,\n@@ -80,14 +80,24 @@ impl HttpServerTester {\n     }\n }\n \n+fn init_logger() {\n+    // Ignore the result. We don't want to panic if the logger cannot be initialized, or is being\n+    // initialized more than once. Also, if the logger is not initialized, we cannot log an\n+    // error!\n+    let _res = env_logger::builder().is_test(true).try_init();\n+}\n+\n #[tokio::test]\n async fn test_https_server_can_serve_https_requests() {\n+    init_logger();\n+\n     // Start a runtime with an HTTP server node, and a thread simulating an Oak node to respond to\n     // HTTP requests.\n-    let mut http_server_tester = HttpServerTester::new(2525, true);\n+    let mut http_server_tester = HttpServerTester::new(2525, 1);\n+    let client_with_valid_tls = create_client(LOCAL_CA);\n \n     // Send an HTTPS request, and check that response has StatusCode::OK\n-    let resp = send_request(\"https://localhost:2525\", LOCAL_CA).await;\n+    let resp = send_request(client_with_valid_tls, \"https://localhost:2525\").await;\n     assert!(resp.is_ok());\n     assert_eq!(\n         resp.unwrap().status(),\n@@ -100,14 +110,17 @@ async fn test_https_server_can_serve_https_requests() {\n \n #[tokio::test]\n async fn test_https_server_cannot_serve_http_requests() {\n+    init_logger();\n+\n     // Start a runtime with an HTTP server node. The HTTP server in this case rejects the requests,\n     // and would not send anything to the Oak node. Creating a thread to simulate the Oak node will\n     // result in the thread being blocked for ever. So, we set up the test without an\n     // oak-node-simulator thread.\n-    let mut http_server_tester = HttpServerTester::new(2526, false);\n+    let mut http_server_tester = HttpServerTester::new(2526, 0);\n+    let client_with_valid_tls = create_client(LOCAL_CA);\n \n     // Send an HTTP request, and check that the server responds with an error\n-    let resp = send_request(\"http://localhost:2526\", LOCAL_CA).await;\n+    let resp = send_request(client_with_valid_tls, \"http://localhost:2526\").await;\n     assert!(resp.is_err());\n \n     // Stop the runtime and the servers\n@@ -116,14 +129,25 @@ async fn test_https_server_cannot_serve_http_requests() {\n \n #[tokio::test]\n async fn test_https_server_does_not_terminate_after_a_bad_request() {\n-    let mut http_server_tester = HttpServerTester::new(2527, true);\n+    init_logger();\n+\n+    // Start a runtime with an HTTP server node, and a thread simulating an Oak node to respond to\n+    // HTTP requests.\n+    let mut http_server_tester = HttpServerTester::new(2527, 2);\n+    let client_with_valid_tls = create_client(LOCAL_CA);\n+    let client_with_invalid_tls = create_client(GCP_CA);\n+\n+    // Send a valid request, making sure that the server is started\n+    let resp = send_request(client_with_valid_tls.clone(), \"https://localhost:2527\").await;\n+    assert!(resp.is_ok());\n \n     // Send an HTTPS request with invalid certificate, and check that the server responds with error\n-    let resp = send_request(\"https://localhost:2527\", GCP_CA).await;\n+    let resp = send_request(client_with_invalid_tls, \"https://localhost:2527\").await;\n     assert!(resp.is_err());\n \n-    // Send a second request, and check that the server is alive and responsive\n-    let resp = send_request(\"https://localhost:2527\", LOCAL_CA).await;\n+    // Send another valid request, and check that the server is alive and responsive\n+    // let client_with_valid_tls = create_client(LOCAL_CA);\n+    let resp = send_request(client_with_valid_tls, \"https://localhost:2527\").await;\n     assert!(resp.is_ok());\n \n     // Stop the runtime and the servers\n@@ -197,35 +221,40 @@ fn create_communication_channel(runtime: &RuntimeProxy) -> (oak_abi::Handle, oak\n     (init_receiver, invocation_receiver)\n }\n \n-fn oak_node_simulator(runtime: &RuntimeProxy, invocation_receiver: oak_abi::Handle) {\n+// Simulate an Oak node that responds to the specified number of requests. Since the Oak node blocks\n+// until a new request is received, we need to know the number of requests that the test sends\n+// beforehand. Alternatively, the node simulator could listen for incoming requests in an infinite\n+// loop and have a termination mechanism that could be signalled from outside. However, specifying\n+// the exact number of expected requests is more appropriate for testing purposes.\n+fn oak_node_simulator(\n+    number_of_requests: i32,\n+    runtime: &RuntimeProxy,\n+    invocation_receiver: oak_abi::Handle,\n+) {\n     // Get invocation message that contains the response_writer handle.\n     let invocation_receiver = Receiver::<HttpInvocation>::new(ReadHandle {\n         handle: invocation_receiver,\n     });\n-    let invocation = invocation_receiver.receive(runtime).unwrap();\n-    let resp = HttpResponse {\n-        body: vec![],\n-        status: http::status::StatusCode::OK.as_u16() as i32,\n-        headers: hashmap! {},\n-    };\n-    invocation\n-        .sender\n-        .expect(\"Empty sender on invocation.\")\n-        .send(resp, runtime)\n-        .unwrap();\n-}\n \n-async fn send_request(\n-    uri: &str,\n-    ca_path: &str,\n-) -> Result<http::response::Response<hyper::Body>, hyper::Error> {\n-    // Send a request, and wait for the response\n-    let label = oak_abi::label::Label::public_untrusted();\n-    let mut label_bytes = vec![];\n-    if let Err(err) = label.encode(&mut label_bytes) {\n-        panic!(\"Failed to encode label: {}\", err);\n+    for _counter in 0..number_of_requests {\n+        let invocation = invocation_receiver.receive(runtime).unwrap();\n+        let resp = HttpResponse {\n+            body: vec![],\n+            status: http::status::StatusCode::OK.as_u16() as i32,\n+            headers: hashmap! {},\n+        };\n+        invocation\n+            .sender\n+            .expect(\"Empty sender on invocation.\")\n+            .send(resp, runtime)\n+            .unwrap();\n     }\n+}\n \n+// Build a TLS client, using the given CA store\n+fn create_client(\n+    ca_path: &str,\n+) -> hyper::client::Client<hyper_rustls::HttpsConnector<hyper::client::HttpConnector>> {\n     let ca_file =\n         fs::File::open(ca_path).unwrap_or_else(|e| panic!(\"failed to open {}: {}\", ca_path, e));\n     let mut ca = io::BufReader::new(ca_file);\n@@ -241,15 +270,47 @@ async fn send_request(\n     // Join the above part into an HTTPS connector.\n     let https = hyper_rustls::HttpsConnector::from((http, tls));\n \n-    let client: hyper::client::Client<_, hyper::Body> =\n-        hyper::client::Client::builder().build(https);\n+    hyper::client::Client::builder().build(https)\n+}\n \n-    let request = hyper::Request::builder()\n-        .method(http::Method::GET)\n-        .uri(uri)\n-        .header(oak_abi::OAK_LABEL_HTTP_PROTOBUF_KEY, label_bytes)\n-        .body(hyper::Body::empty())\n-        .unwrap();\n+async fn send_request(\n+    client: hyper::client::Client<hyper_rustls::HttpsConnector<hyper::client::HttpConnector>>,\n+    uri: &str,\n+) -> Result<http::response::Response<hyper::Body>, hyper::Error> {\n+    // Send a request, and wait for the response\n+    let label = oak_abi::label::Label::public_untrusted();\n+    let mut label_bytes = vec![];\n+    if let Err(err) = label.encode(&mut label_bytes) {\n+        panic!(\"Failed to encode label: {}\", err);\n+    }\n \n-    client.request(request).await\n+    // The client thread may start sending the requests before the server is up. In this case, the\n+    // request will be rejected with a \"ConnectError\". To make the tests are stable, we need to\n+    // retry sending the requests until the server is up. To distinguish between these cases and\n+    // actual errors (e.g., errors due to invalid TLS certificates), we need to check the cause of\n+    // the error.\n+    loop {\n+        let request = hyper::Request::builder()\n+            .method(http::Method::GET)\n+            .uri(uri)\n+            .header(oak_abi::OAK_LABEL_HTTP_PROTOBUF_KEY, label_bytes.clone())\n+            .body(hyper::Body::empty())\n+            .unwrap();\n+\n+        match client.request(request).await {\n+            Ok(reps) => return Ok(reps),\n+            Err(error) => {\n+                // We cannot access the cause of the error, so we need to check the string instead.\n+                let error_str = format!(\"{:?}\", error);\n+                // If the cause is `ConnectError` (https://github.com/hyperium/hyper/blob/66fc127c8d4f81aed9300c9d0f13246b8206067a/src/client/connect/http.rs#L392)\n+                // it means that a connection to the server cannot be made. Retry sending the\n+                // request in this case.\n+                if error_str.contains(\"ConnectError\") {\n+                    continue;\n+                } else {\n+                    return Err(error);\n+                }\n+            }\n+        }\n+    }\n }\n"}
{"id": 85, "ground_truth": "Concurrency", "predicted": "Network", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nmark message as seen on imap in this case.\r\nspams log with events even though the jobs can not be executed because the device is offline.\r\n\r\nproposed solution: don't run the mark msg as read on imap job if offline\n\n### Code Patch:\n@@ -17,6 +17,7 @@\n - `get_connectivity_html()` returns HTML as non-scalable #3213\n - add update-serial to `DC_EVENT_WEBXDC_STATUS_UPDATE` #3215\n - Speed up message receiving via IMAP a bit #3225\n+- mark messages as seen on IMAP in batches #3223\n \n \n ## 1.77.0\n\n@@ -24,7 +24,7 @@ use crate::download::DownloadState;\n use crate::ephemeral::{stock_ephemeral_timer_changed, Timer as EphemeralTimer};\n use crate::events::EventType;\n use crate::headerdef::{HeaderDef, HeaderDefMap};\n-use crate::job::{self, Action};\n+use crate::imap::markseen_on_imap_table;\n use crate::location;\n use crate::log::LogExt;\n use crate::message::{\n@@ -339,16 +339,7 @@ pub(crate) async fn dc_receive_imf_inner(\n                 .await?;\n         } else if !mime_parser.mdn_reports.is_empty() && mime_parser.has_chat_version() {\n             // This is a Delta Chat MDN. Mark as read.\n-            job::add(\n-                context,\n-                job::Job::new(\n-                    Action::MarkseenMsgOnImap,\n-                    insert_msg_id.to_u32(),\n-                    Params::new(),\n-                    0,\n-                ),\n-            )\n-            .await?;\n+            markseen_on_imap_table(context, rfc724_mid).await?;\n         }\n     }\n \n@@ -2300,6 +2291,7 @@ mod tests {\n     use crate::chat::{get_chat_msgs, ChatItem, ChatVisibility};\n     use crate::chatlist::Chatlist;\n     use crate::constants::DC_GCL_NO_SPECIALS;\n+    use crate::imap::prefetch_should_download;\n     use crate::message::Message;\n     use crate::test_utils::{get_chat_msg, TestContext, TestContextManager};\n \n@@ -2883,7 +2875,7 @@ mod tests {\n \n         // Check that the ndn would be downloaded:\n         let headers = mailparse::parse_mail(raw_ndn).unwrap().headers;\n-        assert!(crate::imap::prefetch_should_download(\n+        assert!(prefetch_should_download(\n             &t,\n             &headers,\n             \"some-other-message-id\",\n\n@@ -7,6 +7,7 @@ use std::{\n     cmp,\n     cmp::max,\n     collections::{BTreeMap, BTreeSet},\n+    iter::Peekable,\n };\n \n use anyhow::{bail, format_err, Context as _, Result};\n@@ -31,14 +32,13 @@ use crate::dc_receive_imf::{\n use crate::dc_tools::dc_create_id;\n use crate::events::EventType;\n use crate::headerdef::{HeaderDef, HeaderDefMap};\n-use crate::job::{self, Action};\n+use crate::job;\n use crate::login_param::{\n     CertificateChecks, LoginParam, ServerAddress, ServerLoginParam, Socks5Config,\n };\n use crate::message::{self, Message, MessageState, MessengerMessage, MsgId, Viewtype};\n use crate::mimeparser;\n use crate::oauth2::dc_get_oauth2_access_token;\n-use crate::param::Params;\n use crate::provider::Socket;\n use crate::scheduler::connectivity::ConnectivityStore;\n use crate::scheduler::InterruptInfo;\n@@ -165,6 +165,67 @@ struct ImapConfig {\n     pub can_condstore: bool,\n }\n \n+struct UidGrouper<T: Iterator<Item = (i64, u32, String)>> {\n+    inner: Peekable<T>,\n+}\n+\n+impl<T, I> From<I> for UidGrouper<T>\n+where\n+    T: Iterator<Item = (i64, u32, String)>,\n+    I: IntoIterator<IntoIter = T>,\n+{\n+    fn from(inner: I) -> Self {\n+        Self {\n+            inner: inner.into_iter().peekable(),\n+        }\n+    }\n+}\n+\n+impl<T: Iterator<Item = (i64, u32, String)>> Iterator for UidGrouper<T> {\n+    // Tuple of folder, row IDs, and UID range as a string.\n+    type Item = (String, Vec<i64>, String);\n+\n+    fn next(&mut self) -> Option<Self::Item> {\n+        let (_, _, folder) = self.inner.peek().cloned()?;\n+\n+        let mut uid_set = String::new();\n+        let mut rowid_set = Vec::new();\n+\n+        while uid_set.len() < 1000 {\n+            // Construct a new range.\n+            if let Some((start_rowid, start_uid, _)) = self\n+                .inner\n+                .next_if(|(_, _, start_folder)| start_folder == &folder)\n+            {\n+                rowid_set.push(start_rowid);\n+                let mut end_uid = start_uid;\n+\n+                while let Some((next_rowid, next_uid, _)) =\n+                    self.inner.next_if(|(_, next_uid, next_folder)| {\n+                        next_folder == &folder && *next_uid == end_uid + 1\n+                    })\n+                {\n+                    end_uid = next_uid;\n+                    rowid_set.push(next_rowid);\n+                }\n+\n+                let uid_range = UidRange {\n+                    start: start_uid,\n+                    end: end_uid,\n+                };\n+                if !uid_set.is_empty() {\n+                    uid_set.push(',');\n+                }\n+                uid_set.push_str(&uid_range.to_string());\n+            } else {\n+                break;\n+            }\n+        }\n+\n+        Some((folder, rowid_set, uid_set))\n+    }\n+}\n+\n impl Imap {\n     /// Creates new disconnected IMAP client using the specific login parameters.\n     ///\n@@ -944,7 +1005,7 @@ impl Imap {\n     ///\n     /// This is the only place where messages are moved or deleted on the IMAP server.\n     async fn move_delete_messages(&mut self, context: &Context, folder: &str) -> Result<()> {\n-        let mut rows = context\n+        let rows = context\n             .sql\n             .query_map(\n                 \"SELECT id, uid, target FROM imap\n@@ -960,48 +1021,12 @@ impl Imap {\n                 },\n                 |rows| rows.collect::<Result<Vec<_>, _>>().map_err(Into::into),\n             )\n-            .await?\n-            .into_iter()\n-            .peekable();\n+            .await?;\n \n         self.prepare(context).await?;\n         self.select_folder(context, Some(folder)).await?;\n \n-        while let Some((_, _, target)) = rows.peek().cloned() {\n-            // Construct next request for the target folder.\n-            let mut uid_set = String::new();\n-            let mut rowid_set = Vec::new();\n-\n-            while uid_set.len() < 1000 {\n-                // Construct a new range.\n-                if let Some((start_rowid, start_uid, _)) =\n-                    rows.next_if(|(_, _, start_target)| start_target == &target)\n-                {\n-                    rowid_set.push(start_rowid);\n-                    let mut end_uid = start_uid;\n-\n-                    while let Some((next_rowid, next_uid, _)) =\n-                        rows.next_if(|(_, next_uid, next_target)| {\n-                            next_target == &target && *next_uid == end_uid + 1\n-                        })\n-                    {\n-                        end_uid = next_uid;\n-                        rowid_set.push(next_rowid);\n-                    }\n-\n-                    let uid_range = UidRange {\n-                        start: start_uid,\n-                        end: end_uid,\n-                    };\n-                    if !uid_set.is_empty() {\n-                        uid_set.push(',');\n-                    }\n-                    uid_set.push_str(&uid_range.to_string());\n-                } else {\n-                    break;\n-                }\n-            }\n-\n+        for (target, rowid_set, uid_set) in UidGrouper::from(rows) {\n             // Empty target folder name means messages should be deleted.\n             if target.is_empty() {\n                 self.delete_message_batch(context, &uid_set, rowid_set)\n@@ -1028,6 +1053,62 @@ impl Imap {\n         Ok(())\n     }\n \n+    /// Stores pending `\\Seen` flags for messages in `imap_markseen` table.\n+    pub(crate) async fn store_seen_flags_on_imap(&mut self, context: &Context) -> Result<()> {\n+        self.prepare(context).await?;\n+\n+        let rows = context\n+            .sql\n+            .query_map(\n+                \"SELECT imap.id, uid, folder FROM imap, imap_markseen\n+                 WHERE imap.id = imap_markseen.id AND target = folder\n+                 ORDER BY folder, uid\",\n+                [],\n+                |row| {\n+                    let rowid: i64 = row.get(0)?;\n+                    let uid: u32 = row.get(1)?;\n+                    let folder: String = row.get(2)?;\n+                    Ok((rowid, uid, folder))\n+                },\n+                |rows| rows.collect::<Result<Vec<_>, _>>().map_err(Into::into),\n+            )\n+            .await?;\n+\n+        for (folder, rowid_set, uid_set) in UidGrouper::from(rows) {\n+            self.select_folder(context, Some(&folder))\n+                .await\n+                .context(\"failed to select folder\")?;\n+\n+            if let Err(err) = self.add_flag_finalized_with_set(&uid_set, \"\\\\Seen\").await {\n+                warn!(\n+                    context,\n+                    \"Cannot mark messages {} in folder {} as seen, will retry later: {}.\",\n+                    uid_set,\n+                    folder,\n+                    err\n+                );\n+            } else {\n+                info!(\n+                    context,\n+                    \"Marked messages {} in folder {} as seen.\", uid_set, folder\n+                );\n+                context\n+                    .sql\n+                    .execute(\n+                        format!(\n+                            \"DELETE FROM imap_markseen WHERE id IN ({})\",\n+                            sql::repeat_vars(rowid_set.len())?\n+                        ),\n+                        rusqlite::params_from_iter(rowid_set),\n+                    )\n+                    .await\n+                    .context(\"cannot remove messages marked as seen from imap_markseen table\")?;\n+            }\n+        }\n+\n+        Ok(())\n+    }\n+\n     /// Synchronizes `\\Seen` flags using `CONDSTORE` extension.\n     pub(crate) async fn sync_seen_flags(&mut self, context: &Context, folder: &str) -> Result<()> {\n         if !self.config.can_condstore {\n@@ -1364,11 +1445,6 @@ impl Imap {\n     /// the flag, or other imap-errors, returns true as well.\n     ///\n     /// Returning error means that the operation can be retried.\n-    async fn add_flag_finalized(&mut self, server_uid: u32, flag: &str) -> Result<()> {\n-        let s = server_uid.to_string();\n-        self.add_flag_finalized_with_set(&s, flag).await\n-    }\n-\n     async fn add_flag_finalized_with_set(&mut self, uid_set: &str, flag: &str) -> Result<()> {\n         if self.should_reconnect() {\n             bail!(\"Can't set flag, should reconnect\");\n@@ -1386,7 +1462,7 @@ impl Imap {\n         Ok(())\n     }\n \n-    pub async fn prepare_imap_operation_on_msg(\n+    pub(crate) async fn prepare_imap_operation_on_msg(\n         &mut self,\n         context: &Context,\n         folder: &str,\n@@ -1426,32 +1502,6 @@ impl Imap {\n         }\n     }\n \n-    pub(crate) async fn set_seen(\n-        &mut self,\n-        context: &Context,\n-        folder: &str,\n-        uid: u32,\n-    ) -> ImapActionResult {\n-        if let Some(imapresult) = self\n-            .prepare_imap_operation_on_msg(context, folder, uid)\n-            .await\n-        {\n-            return imapresult;\n-        }\n-        // we are connected, and the folder is selected\n-        info!(context, \"Marking message {}/{} as seen...\", folder, uid,);\n-\n-        if let Err(err) = self.add_flag_finalized(uid, \"\\\\Seen\").await {\n-            warn!(\n-                context,\n-                \"Cannot mark message {} in folder {} as seen, ignoring: {}.\", uid, folder, err\n-            );\n-            ImapActionResult::Failed\n-        } else {\n-            ImapActionResult::Success\n-        }\n-    }\n-\n     pub async fn ensure_configured_folders(\n         &mut self,\n         context: &Context,\n@@ -1882,13 +1932,11 @@ pub(crate) async fn prefetch_should_download(\n     mut flags: impl Iterator<Item = Flag<'_>>,\n     show_emails: ShowEmails,\n ) -> Result<bool> {\n-    if let Some(msg_id) = message::rfc724_mid_exists(context, message_id).await? {\n-        // We know the Message-ID already, it must be a Bcc: to self.\n-        job::add(\n-            context,\n-            job::Job::new(Action::MarkseenMsgOnImap, msg_id.to_u32(), Params::new(), 0),\n-        )\n-        .await?;\n+    if message::rfc724_mid_exists(context, message_id)\n+        .await?\n+        .is_some()\n+    {\n+        markseen_on_imap_table(context, message_id).await?;\n         return Ok(false);\n     }\n \n@@ -2022,6 +2070,22 @@ async fn mark_seen_by_uid(\n     }\n }\n \n+/// Schedule marking the message as Seen on IMAP by adding all known IMAP messages corresponding to\n+/// the given Message-ID to `imap_markseen` table.\n+pub(crate) async fn markseen_on_imap_table(context: &Context, message_id: &str) -> Result<()> {\n+    context\n+        .sql\n+        .execute(\n+            \"INSERT OR IGNORE INTO imap_markseen (id)\n+             SELECT id FROM imap WHERE rfc724_mid=?\",\n+            paramsv![message_id],\n+        )\n+        .await?;\n+    context.interrupt_inbox(InterruptInfo::new(false)).await;\n+\n+    Ok(())\n+}\n+\n /// uid_next is the next unique identifier value from the last time we fetched a folder\n /// See <https://tools.ietf.org/html/rfc3501#section-2.3.1.1>\n /// This function is used to update our uid_next after fetching messages.\n\n@@ -13,7 +13,7 @@ use crate::contact::{normalize_name, Contact, ContactId, Modifier, Origin};\n use crate::context::Context;\n use crate::dc_tools::time;\n use crate::events::EventType;\n-use crate::imap::{Imap, ImapActionResult};\n+use crate::imap::Imap;\n use crate::location;\n use crate::log::LogExt;\n use crate::message::{Message, MsgId};\n@@ -86,7 +86,6 @@ pub enum Action {\n     // Jobs in the INBOX-thread, range from DC_IMAP_THREAD..DC_IMAP_THREAD+999\n     Housekeeping = 105, // low priority ...\n     FetchExistingMsgs = 110,\n-    MarkseenMsgOnImap = 130,\n \n     // this is user initiated so it should have a fairly high priority\n     UpdateRecentQuota = 140,\n@@ -123,7 +122,6 @@ impl From<Action> for Thread {\n             Housekeeping => Thread::Imap,\n             FetchExistingMsgs => Thread::Imap,\n             ResyncFolders => Thread::Imap,\n-            MarkseenMsgOnImap => Thread::Imap,\n             UpdateRecentQuota => Thread::Imap,\n             DownloadMsg => Thread::Imap,\n \n@@ -403,67 +401,6 @@ impl Job {\n             Status::Finished(Ok(()))\n         }\n     }\n-\n-    async fn markseen_msg_on_imap(&mut self, context: &Context, imap: &mut Imap) -> Status {\n-        if let Err(err) = imap.prepare(context).await {\n-            warn!(context, \"could not connect: {:?}\", err);\n-            return Status::RetryLater;\n-        }\n-\n-        let msg = job_try!(Message::load_from_db(context, MsgId::new(self.foreign_id)).await);\n-        let row = job_try!(\n-            context\n-                .sql\n-                .query_row_optional(\n-                    \"SELECT uid, folder FROM imap\n-                    WHERE rfc724_mid=? AND folder=target\n-                    ORDER BY uid ASC\n-                    LIMIT 1\",\n-                    paramsv![msg.rfc724_mid],\n-                    |row| {\n-                        let uid: u32 = row.get(0)?;\n-                        let folder: String = row.get(1)?;\n-                        Ok((uid, folder))\n-                    }\n-                )\n-                .await\n-        );\n-        if let Some((server_uid, server_folder)) = row {\n-            let result = imap.set_seen(context, &server_folder, server_uid).await;\n-            match result {\n-                ImapActionResult::RetryLater => return Status::RetryLater,\n-                ImapActionResult::Success | ImapActionResult::Failed => {}\n-            }\n-        } else {\n-            info!(\n-                context,\n-                \"Can't mark the message {} as seen on IMAP because there is no known UID\",\n-                msg.rfc724_mid\n-            );\n-        }\n-\n-        // XXX we send MDN even in case of failure to mark the messages as seen, e.g. if it was\n-        // already deleted on the server by another device. The job will not be retried so locally\n-        // there is no risk of double-sending MDNs.\n-        //\n-        // Read receipts for system messages are never sent. These messages have no place to\n-        // display received read receipt anyway.  And since their text is locally generated,\n-        // quoting them is dangerous as it may contain contact names. E.g., for original message\n-        // \"Group left by me\", a read receipt will quote \"Group left by <name>\", and the name can\n-        // be a display name stored in address book rather than the name sent in the From field by\n-        // the user.\n-        if msg.param.get_bool(Param::WantsMdn).unwrap_or_default() && !msg.is_system_message() {\n-            let mdns_enabled = job_try!(context.get_config_bool(Config::MdnsEnabled).await);\n-            if mdns_enabled {\n-                if let Err(err) = send_mdn(context, &msg).await {\n-                    warn!(context, \"could not send out mdn for {}: {}\", msg.id, err);\n-                    return Status::Finished(Err(err));\n-                }\n-            }\n-        }\n-\n-        Status::Finished(Ok(()))\n-    }\n }\n \n /// Delete all pending jobs with the given action.\n@@ -660,7 +597,6 @@ async fn perform_job_action(\n             location::job_maybe_send_locations_ended(context, job).await\n         }\n         Action::ResyncFolders => job.resync_folders(context, connection.inbox()).await,\n-        Action::MarkseenMsgOnImap => job.markseen_msg_on_imap(context, connection.inbox()).await,\n         Action::FetchExistingMsgs => job.fetch_existing_msgs(context, connection.inbox()).await,\n         Action::Housekeeping => {\n             sql::housekeeping(context).await.ok_or_log(context);\n@@ -698,13 +634,13 @@ fn get_backoff_time_offset(tries: u32, action: Action) -> i64 {\n     }\n }\n \n-async fn send_mdn(context: &Context, msg: &Message) -> Result<()> {\n+pub(crate) async fn send_mdn(context: &Context, msg_id: MsgId, from_id: ContactId) -> Result<()> {\n     let mut param = Params::new();\n-    param.set(Param::MsgId, msg.id.to_u32().to_string());\n+    param.set(Param::MsgId, msg_id.to_u32().to_string());\n \n     add(\n         context,\n-        Job::new(Action::SendMdn, msg.from_id.to_u32(), param, 0),\n+        Job::new(Action::SendMdn, from_id.to_u32(), param, 0),\n     )\n     .await?;\n \n@@ -732,7 +668,6 @@ pub async fn add(context: &Context, job: Job) -> Result<()> {\n             Action::Unknown => unreachable!(),\n             Action::Housekeeping\n             | Action::ResyncFolders\n-            | Action::MarkseenMsgOnImap\n             | Action::FetchExistingMsgs\n             | Action::UpdateRecentQuota\n             | Action::DownloadMsg => {\n\n@@ -9,6 +9,7 @@ use rusqlite::types::ValueRef;\n use serde::{Deserialize, Serialize};\n \n use crate::chat::{self, Chat, ChatId};\n+use crate::config::Config;\n use crate::constants::{\n     Blocked, Chattype, VideochatType, DC_CHAT_ID_TRASH, DC_DESIRED_TEXT_LEN, DC_MSG_ID_LAST_SPECIAL,\n };\n@@ -21,6 +22,7 @@ use crate::dc_tools::{\n use crate::download::DownloadState;\n use crate::ephemeral::{start_ephemeral_timers_msgids, Timer as EphemeralTimer};\n use crate::events::EventType;\n+use crate::imap::markseen_on_imap_table;\n use crate::job::{self, Action};\n use crate::log::LogExt;\n use crate::mimeparser::{parse_message_id, FailureReport, SystemMessage};\n@@ -1294,6 +1296,9 @@ pub async fn markseen_msgs(context: &Context, msg_ids: Vec<MsgId>) -> Result<()>\n                     m.chat_id AS chat_id,\n                     m.state AS state,\n                     m.ephemeral_timer AS ephemeral_timer,\n+                    m.param AS param,\n+                    m.from_id AS from_id,\n+                    m.rfc724_mid AS rfc724_mid,\n                     c.blocked AS blocked\n                  FROM msgs m LEFT JOIN chats c ON c.id=m.chat_id\n                  WHERE m.id IN ({}) AND m.chat_id>9\",\n@@ -1304,12 +1309,18 @@ pub async fn markseen_msgs(context: &Context, msg_ids: Vec<MsgId>) -> Result<()>\n                 let id: MsgId = row.get(\"id\")?;\n                 let chat_id: ChatId = row.get(\"chat_id\")?;\n                 let state: MessageState = row.get(\"state\")?;\n+                let param: Params = row.get::<_, String>(\"param\")?.parse().unwrap_or_default();\n+                let from_id: ContactId = row.get(\"from_id\")?;\n+                let rfc724_mid: String = row.get(\"rfc724_mid\")?;\n                 let blocked: Option<Blocked> = row.get(\"blocked\")?;\n                 let ephemeral_timer: EphemeralTimer = row.get(\"ephemeral_timer\")?;\n                 Ok((\n                     id,\n                     chat_id,\n                     state,\n+                    param,\n+                    from_id,\n+                    rfc724_mid,\n                     blocked.unwrap_or_default(),\n                     ephemeral_timer,\n                 ))\n@@ -1318,30 +1329,52 @@ pub async fn markseen_msgs(context: &Context, msg_ids: Vec<MsgId>) -> Result<()>\n         )\n         .await?;\n \n-    if msgs\n-        .iter()\n-        .any(|(_id, _chat_id, _state, _blocked, ephemeral_timer)| {\n+    if msgs.iter().any(\n+        |(_id, _chat_id, _state, _param, _from_id, _rfc724_mid, _blocked, ephemeral_timer)| {\n             *ephemeral_timer != EphemeralTimer::Disabled\n-        })\n-    {\n+        },\n+    ) {\n         start_ephemeral_timers_msgids(context, &msg_ids)\n             .await\n             .context(\"failed to start ephemeral timers\")?;\n     }\n \n     let mut updated_chat_ids = BTreeSet::new();\n-    for (id, curr_chat_id, curr_state, curr_blocked, _curr_ephemeral_timer) in msgs.into_iter() {\n+    for (\n+        id,\n+        curr_chat_id,\n+        curr_state,\n+        curr_param,\n+        curr_from_id,\n+        curr_rfc724_mid,\n+        curr_blocked,\n+        _curr_ephemeral_timer,\n+    ) in msgs.into_iter()\n+    {\n         if curr_blocked == Blocked::Not\n             && (curr_state == MessageState::InFresh || curr_state == MessageState::InNoticed)\n         {\n             update_msg_state(context, id, MessageState::InSeen).await?;\n             info!(context, \"Seen message {}.\", id);\n \n-            job::add(\n-                context,\n-                job::Job::new(Action::MarkseenMsgOnImap, id.to_u32(), Params::new(), 0),\n-            )\n-            .await?;\n+            markseen_on_imap_table(context, &curr_rfc724_mid).await?;\n+\n+            // Read receipts for system messages are never sent. These messages have no place to\n+            // display received read receipt anyway.  And since their text is locally generated,\n+            // quoting them is dangerous as it may contain contact names. E.g., for original message\n+            // \"Group left by me\", a read receipt will quote \"Group left by <name>\", and the name can\n+            // be a display name stored in address book rather than the name sent in the From field by\n+            // the user.\n+            if curr_param.get_bool(Param::WantsMdn).unwrap_or_default()\n+                && curr_param.get_cmd() == SystemMessage::Unknown\n+            {\n+                let mdns_enabled = context.get_config_bool(Config::MdnsEnabled).await?;\n+                if mdns_enabled {\n+                    if let Err(err) = job::send_mdn(context, id, curr_from_id).await {\n+                        warn!(context, \"could not send out mdn for {}: {}\", id, err);\n+                    }\n+                }\n+            }\n             updated_chat_ids.insert(curr_chat_id);\n         }\n     }\n\n@@ -168,6 +168,16 @@ async fn fetch_idle(ctx: &Context, connection: &mut Imap, folder: Config) -> Int\n                 return connection.fake_idle(ctx, Some(watch_folder)).await;\n             }\n \n+            if folder == Config::ConfiguredInboxFolder {\n+                if let Err(err) = connection\n+                    .store_seen_flags_on_imap(ctx)\n+                    .await\n+                    .context(\"store_seen_flags_on_imap failed\")\n+                {\n+                    warn!(ctx, \"{:#}\", err);\n+                }\n+            }\n+\n             // Fetch the watched folder.\n             if let Err(err) = connection.fetch_move_delete(ctx, &watch_folder).await {\n                 connection.trigger_reconnect(ctx).await;\n\n@@ -181,6 +181,7 @@ impl Sql {\n                      PRAGMA secure_delete=on;\n                      PRAGMA busy_timeout = {};\n                      PRAGMA temp_store=memory; -- Avoid SQLITE_IOERR_GETTEMPPATH errors on Android\n+                     PRAGMA foreign_keys=on;\n                      \",\n                     Duration::from_secs(10).as_millis()\n                 ))?;\n\n@@ -613,6 +613,17 @@ CREATE INDEX smtp_messageid ON imap(rfc724_mid);\n         sql.execute_migration(\"DROP TABLE IF EXISTS backup_blobs;\", 88)\n             .await?;\n     }\n+    if dbversion < 89 {\n+        info!(context, \"[migration] v89\");\n+        sql.execute_migration(\n+            r#\"CREATE TABLE imap_markseen (\n+              id INTEGER,\n+              FOREIGN KEY(id) REFERENCES imap(id) ON DELETE CASCADE\n+            );\"#,\n+            89,\n+        )\n+        .await?;\n+    }\n \n     Ok((\n         recalc_fingerprints,\n"}
{"id": 86, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/metalbear-co/mirrord/actions/runs/5066110644/jobs/9095518984?pr=1451\r\nI think it's similar to the #1451 just need to do same in flask\r\napplication_1_Application__PythonFlaskHTTP\r\n```\r\nthread 'mirroring_with_http::application_1_Application__PythonFlaskHTTP' panicked at 'Timeout 60s expired', /Users/runner/.cargo/registry/src/index.crates.io-6f17d22bba15001f/rstest-0.17.0/src/timeout.rs:42:21\r\nstack backtrace:\r\n   0: rust_begin_unwind\r\n             at /rustc/478cbb42b730ba4739351b72ce2aa928e78e2f81/library/std/src/panicking.rs:577:5\r\n   1: core::panicking::panic_fmt\r\n             at /rustc/478cbb42b730ba4739351b72ce2aa928e78e2f81/library/core/src/panicking.rs:67:14\r\n   2: rstest::timeout::execute_with_timeout_async::{{closure}}\r\n   3: http_mirroring::mirroring_with_http::application_1_Application__PythonFlaskHTTP::{{closure}}\r\n   4: <core::pin::Pin<P> as core::future::future::Future>::poll\r\n   5: tokio::runtime::park::CachedParkThread::block_on::{{closure}}\r\n   6: tokio::runtime::park::CachedParkThread::block_on\r\n   7: tokio::runtime::context::BlockingRegionGuard::block_on\r\n   8: tokio::runtime::scheduler::multi_thread::MultiThread::block_on\r\n   9: tokio::runtime::runtime::Runtime::block_on\r\n  10: http_mirroring::mirroring_with_http::application_1_Application__PythonFlaskHTTP\r\n  11: http_mirroring::mirroring_with_http::application_1_Application__PythonFlaskHTTP::{{closure}}\r\n  12: core::ops::function::FnOnce::call_once\r\n  13: core::ops::function::FnOnce::call_once\r\n             at /rustc/478cbb42b730ba4739351b72ce2aa928e78e2f81/library/core/src/ops/function.rs:250:5\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\n```\n\n### Code Patch:\n@@ -0,0 +1 @@\n+Change locks in test process to be async to avoid deadlocks\n\\ No newline at end of file\n\n@@ -96,6 +96,6 @@ async fn bash_script(dylib_path: &Path) {\n     // and that's okay - it's either file closing then process terminates or process terminates.\n     // which closes the whole session in the agent\n \n-    test_process.assert_no_error_in_stdout();\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stdout().await;\n+    test_process.assert_no_error_in_stderr().await;\n }\n\n@@ -1,11 +1,6 @@\n use std::{\n-    assert_matches::assert_matches,\n-    cmp::min,\n-    collections::HashMap,\n-    fmt::Debug,\n-    path::PathBuf,\n-    process::Stdio,\n-    sync::{Arc, Mutex},\n+    assert_matches::assert_matches, cmp::min, collections::HashMap, fmt::Debug, path::PathBuf,\n+    process::Stdio, sync::Arc,\n };\n \n use actix_codec::Framed;\n@@ -25,6 +20,7 @@ use tokio::{\n     io::{AsyncReadExt, AsyncWriteExt, BufReader},\n     net::{TcpListener, TcpStream},\n     process::{Child, Command},\n+    sync::Mutex,\n };\n \n /// Configuration for [`Application::RustOutgoingTcp`] and [`Application::RustOutgoingUdp`].\n@@ -46,23 +42,23 @@ pub struct TestProcess {\n }\n \n impl TestProcess {\n-    pub fn get_stdout(&self) -> String {\n-        self.stdout.lock().unwrap().clone()\n+    pub async fn get_stdout(&self) -> String {\n+        self.stdout.lock().await.clone()\n     }\n \n-    pub fn get_stderr(&self) -> String {\n-        self.stderr.lock().unwrap().clone()\n+    pub async fn get_stderr(&self) -> String {\n+        self.stderr.lock().await.clone()\n     }\n \n-    pub fn assert_log_level(&self, stderr: bool, level: &str) {\n+    pub async fn assert_log_level(&self, stderr: bool, level: &str) {\n         if stderr {\n-            assert!(!self.stderr.lock().unwrap().contains(level));\n+            assert!(!self.stderr.lock().await.contains(level));\n         } else {\n-            assert!(!self.stdout.lock().unwrap().contains(level));\n+            assert!(!self.stdout.lock().await.contains(level));\n         }\n     }\n \n-    fn from_child(mut child: Child) -> TestProcess {\n+    async fn from_child(mut child: Child) -> TestProcess {\n         let stderr_data = Arc::new(Mutex::new(String::new()));\n         let stdout_data = Arc::new(Mutex::new(String::new()));\n         let child_stderr = child.stderr.take().unwrap();\n@@ -82,7 +78,7 @@ impl TestProcess {\n                 let string = String::from_utf8_lossy(&buf[..n]);\n                 eprintln!(\"stderr {} {pid}: {}\", format_time(), string);\n                 {\n-                    stderr_data_reader.lock().unwrap().push_str(&string);\n+                    stderr_data_reader.lock().await.push_str(&string);\n                 }\n             }\n         });\n@@ -97,7 +93,7 @@ impl TestProcess {\n                 let string = String::from_utf8_lossy(&buf[..n]);\n                 print!(\"stdout {} {pid}: {}\", format_time(), string);\n                 {\n-                    stdout_data_reader.lock().unwrap().push_str(&string);\n+                    stdout_data_reader.lock().await.push_str(&string);\n                 }\n             }\n         });\n@@ -125,28 +121,28 @@ impl TestProcess {\n             .spawn()\n             .unwrap();\n         println!(\"Started application.\");\n-        TestProcess::from_child(child)\n+        TestProcess::from_child(child).await\n     }\n \n-    pub fn assert_stdout_contains(&self, string: &str) {\n-        assert!(self.stdout.lock().unwrap().contains(string));\n+    pub async fn assert_stdout_contains(&self, string: &str) {\n+        assert!(self.stdout.lock().await.contains(string));\n     }\n \n-    pub fn assert_stderr_contains(&self, string: &str) {\n-        assert!(self.stderr.lock().unwrap().contains(string));\n+    pub async fn assert_stderr_contains(&self, string: &str) {\n+        assert!(self.stderr.lock().await.contains(string));\n     }\n \n-    pub fn assert_no_error_in_stdout(&self) {\n+    pub async fn assert_no_error_in_stdout(&self) {\n         assert!(!self\n             .error_capture\n-            .is_match(&self.stdout.lock().unwrap())\n+            .is_match(&self.stdout.lock().await)\n             .unwrap());\n     }\n \n-    pub fn assert_no_error_in_stderr(&self) {\n+    pub async fn assert_no_error_in_stderr(&self) {\n         assert!(!self\n             .error_capture\n-            .is_match(&self.stderr.lock().unwrap())\n+            .is_match(&self.stderr.lock().await)\n             .unwrap());\n     }\n \n\n@@ -57,6 +57,6 @@ async fn test_dns_resolve(\n     .unwrap();\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n-    test_process.assert_no_error_in_stdout();\n+    test_process.assert_no_error_in_stderr().await;\n+    test_process.assert_no_error_in_stdout().await;\n }\n\n@@ -42,8 +42,8 @@ async fn self_open(dylib_path: &PathBuf) {\n     assert!(layer_connection.is_ended().await);\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n-    test_process.assert_no_error_in_stdout();\n+    test_process.assert_no_error_in_stderr().await;\n+    test_process.assert_no_error_in_stdout().await;\n }\n \n /// Verify that if the user's app is trying to read out of mirrord's temp bin dir for some messed up\n@@ -84,12 +84,12 @@ async fn read_from_mirrord_bin(dylib_path: &PathBuf) {\n     assert!(layer_connection.is_ended().await);\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n-    test_process.assert_no_error_in_stdout();\n+    test_process.assert_no_error_in_stderr().await;\n+    test_process.assert_no_error_in_stdout().await;\n \n     // We read the contents from <TMPDIR>/<OUR-FILE> even though the app tried to read from\n     // <TMPDIR>/mirrord-bin/<TMPDIR>/<OUR-FILE>.\n-    test_process.assert_stdout_contains(contents);\n+    test_process.assert_stdout_contains(contents).await;\n }\n \n /// Verifies `pwrite` - if opening a file in write mode and writing to it at an offset of zero\n@@ -206,7 +206,7 @@ async fn pwrite(\n     }\n     // Assert all clear\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n \n     // Assert that fwrite flushed correclty\n     let data = std::fs::read(\"/tmp/test_file2.txt\").unwrap();\n@@ -276,7 +276,7 @@ async fn node_close(\n \n     // Assert all clear\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n #[rstest]\n@@ -337,7 +337,7 @@ async fn go_stat(\n         .await\n         .unwrap();\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n #[rstest]\n@@ -461,7 +461,7 @@ async fn go_dir(\n     layer_connection.expect_file_close(fd).await;\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n #[rstest]\n@@ -553,7 +553,7 @@ async fn go_dir_on_linux(\n     layer_connection.expect_file_close(fd).await;\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n /// Test that the bypass works for reading dirs with Go.\n@@ -590,7 +590,7 @@ async fn go_dir_bypass(\n     assert!(layer_connection.is_ended().await);\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n /// Test go file read and close.\n@@ -641,7 +641,7 @@ async fn read_go(\n \n     // Assert all clear\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n /// Test go file write.\n@@ -670,7 +670,7 @@ async fn write_go(\n \n     // Assert all clear\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n /// Test go file lseek.\n@@ -703,7 +703,7 @@ async fn lseek_go(\n \n     // Assert all clear\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n /// Test go file access.\n@@ -730,5 +730,5 @@ async fn faccessat_go(\n \n     // Assert all clear\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stderr().await;\n }\n\n@@ -30,6 +30,6 @@ async fn fork(dylib_path: &PathBuf) {\n     assert!(connection_from_child_process.is_ended().await);\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n-    test_process.assert_no_error_in_stdout();\n+    test_process.assert_no_error_in_stderr().await;\n+    test_process.assert_no_error_in_stdout().await;\n }\n\n@@ -65,12 +65,20 @@ async fn mirroring_with_http(\n         .await;\n \n     test_process.wait().await;\n-    test_process.assert_stdout_contains(\"GET: Request completed\");\n-    test_process.assert_stdout_contains(\"POST: Request completed\");\n-    test_process.assert_stdout_contains(\"PUT: Request completed\");\n-    test_process.assert_stdout_contains(\"DELETE: Request completed\");\n-    test_process.assert_no_error_in_stdout();\n-    test_process.assert_no_error_in_stderr();\n+    test_process\n+        .assert_stdout_contains(\"GET: Request completed\")\n+        .await;\n+    test_process\n+        .assert_stdout_contains(\"POST: Request completed\")\n+        .await;\n+    test_process\n+        .assert_stdout_contains(\"PUT: Request completed\")\n+        .await;\n+    test_process\n+        .assert_stdout_contains(\"DELETE: Request completed\")\n+        .await;\n+    test_process.assert_no_error_in_stdout().await;\n+    test_process.assert_no_error_in_stderr().await;\n }\n \n /// Run the http mirroring test only on MacOS, because of a known crash on Linux.\n\n@@ -33,6 +33,6 @@ async fn ignore_ports(\n     // Make sure no listen request was made.\n     assert!(layer_connection.is_ended().await);\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n-    test_process.assert_no_error_in_stdout();\n+    test_process.assert_no_error_in_stderr().await;\n+    test_process.assert_no_error_in_stdout().await;\n }\n\n@@ -40,6 +40,10 @@ async fn test_issue1054(\n     layer_connection.send_close(new_connection_id).await;\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_stdout_contains(\"test issue 1054: START\");\n-    test_process.assert_stdout_contains(\"test issue 1054: SUCCESS\");\n+    test_process\n+        .assert_stdout_contains(\"test issue 1054: START\")\n+        .await;\n+    test_process\n+        .assert_stdout_contains(\"test issue 1054: SUCCESS\")\n+        .await;\n }\n\n@@ -32,6 +32,10 @@ async fn test_issue1123(\n     println!(\"Application subscribed to port, sending tcp messages.\");\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_stdout_contains(\"test issue 1123: START\");\n-    test_process.assert_stdout_contains(\"test issue 1123: SUCCESS\");\n+    test_process\n+        .assert_stdout_contains(\"test issue 1123: START\")\n+        .await;\n+    test_process\n+        .assert_stdout_contains(\"test issue 1123: SUCCESS\")\n+        .await;\n }\n\n@@ -77,8 +77,12 @@ async fn test_issue1458(\n         .unwrap();\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_stdout_contains(\"test issue 1458: START\");\n-    test_process.assert_stdout_contains(\"test issue 1458: SUCCESS\");\n+    test_process\n+        .assert_stdout_contains(\"test issue 1458: START\")\n+        .await;\n+    test_process\n+        .assert_stdout_contains(\"test issue 1458: SUCCESS\")\n+        .await;\n }\n \n /// Verify that we don't intercept UDP packets when `sendto` address' port is not `53`.\n@@ -108,6 +112,10 @@ async fn test_issue1458_port_not_53(\n     assert!(layer_connection.codec.try_next().await.unwrap().is_none());\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_stdout_contains(\"test issue 1458 port not 53: START\");\n-    test_process.assert_stdout_contains(\"test issue 1458 port not 53: SUCCESS\");\n+    test_process\n+        .assert_stdout_contains(\"test issue 1458 port not 53: START\")\n+        .await;\n+    test_process\n+        .assert_stdout_contains(\"test issue 1458 port not 53: SUCCESS\")\n+        .await;\n }\n\n@@ -28,8 +28,8 @@ async fn test_issue834(\n         .await;\n \n     test_process.wait().await;\n-    test_process.assert_stdout_contains(\"okay\");\n+    test_process.assert_stdout_contains(\"okay\").await;\n \n-    test_process.assert_no_error_in_stdout();\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stdout().await;\n+    test_process.assert_no_error_in_stderr().await;\n }\n\n@@ -60,6 +60,6 @@ async fn listen_ports(\n     }\n     assert!(layer_connection.is_ended().await);\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n-    test_process.assert_no_error_in_stdout();\n+    test_process.assert_no_error_in_stderr().await;\n+    test_process.assert_no_error_in_stdout().await;\n }\n\n@@ -61,6 +61,6 @@ async fn recv_from(\n     .unwrap();\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n-    test_process.assert_no_error_in_stdout();\n+    test_process.assert_no_error_in_stderr().await;\n+    test_process.assert_no_error_in_stdout().await;\n }\n\n@@ -35,6 +35,6 @@ async fn self_connect(dylib_path: &PathBuf) {\n     }\n     assert!(layer_connection.is_ended().await);\n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stderr();\n-    test_process.assert_no_error_in_stdout();\n+    test_process.assert_no_error_in_stderr().await;\n+    test_process.assert_no_error_in_stdout().await;\n }\n\n@@ -41,7 +41,8 @@ async fn tmp_dir_read_locally(dylib_path: &Path) {\n     test_process.wait().await;\n     assert!(!test_process\n         .get_stdout()\n+        .await\n         .contains(\"No such file or directory\"));\n-    test_process.assert_no_error_in_stdout();\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stdout().await;\n+    test_process.assert_no_error_in_stderr().await;\n }\n\n@@ -68,6 +68,6 @@ async fn node_spawn(dylib_path: &PathBuf) {\n     };\n \n     test_process.wait_assert_success().await;\n-    test_process.assert_no_error_in_stdout();\n-    test_process.assert_no_error_in_stderr();\n+    test_process.assert_no_error_in_stdout().await;\n+    test_process.assert_no_error_in_stderr().await;\n }\n\n@@ -40,7 +40,7 @@ mod file_ops {\n         .await;\n         let res = process.child.wait().await.unwrap();\n         assert!(res.success());\n-        ops.assert(process);\n+        ops.assert(process).await;\n     }\n \n     #[cfg(target_os = \"macos\")]\n@@ -69,7 +69,7 @@ mod file_ops {\n         .await;\n         let res = process.child.wait().await.unwrap();\n         assert!(res.success());\n-        process.assert_python_fileops_stderr();\n+        process.assert_python_fileops_stderr().await;\n     }\n \n     #[rstest]\n@@ -102,7 +102,7 @@ mod file_ops {\n         .await;\n         let res = process.child.wait().await.unwrap();\n         assert!(res.success());\n-        process.assert_python_fileops_stderr();\n+        process.assert_python_fileops_stderr().await;\n     }\n \n     #[rstest]\n@@ -134,7 +134,7 @@ mod file_ops {\n         .await;\n         let res = process.child.wait().await.unwrap();\n         assert!(res.success());\n-        process.assert_python_fileops_stderr();\n+        process.assert_python_fileops_stderr().await;\n     }\n \n     // Currently fails due to Layer >> AddressConversion in ci for some reason\n\n@@ -49,18 +49,22 @@ mod http {\n                 None,\n             )\n             .await;\n-        process.wait_for_line(Duration::from_secs(120), \"daemon subscribed\");\n+        process\n+            .wait_for_line(Duration::from_secs(120), \"daemon subscribed\")\n+            .await;\n         send_requests(&url, false, Default::default()).await;\n-        process.wait_for_line(Duration::from_secs(10), \"GET\");\n-        process.wait_for_line(Duration::from_secs(10), \"POST\");\n-        process.wait_for_line(Duration::from_secs(10), \"PUT\");\n-        process.wait_for_line(Duration::from_secs(10), \"DELETE\");\n+        process.wait_for_line(Duration::from_secs(10), \"GET\").await;\n+        process.wait_for_line(Duration::from_secs(10), \"POST\").await;\n+        process.wait_for_line(Duration::from_secs(10), \"PUT\").await;\n+        process\n+            .wait_for_line(Duration::from_secs(10), \"DELETE\")\n+            .await;\n         timeout(Duration::from_secs(40), process.child.wait())\n             .await\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&process);\n+        application.assert(&process).await;\n     }\n \n     #[ignore] // TODO: create integration test instead.\n@@ -91,17 +95,21 @@ mod http {\n                 None,\n             )\n             .await;\n-        process.wait_for_line(Duration::from_secs(300), \"daemon subscribed\");\n+        process\n+            .wait_for_line(Duration::from_secs(300), \"daemon subscribed\")\n+            .await;\n         send_requests(&url, false, Default::default()).await;\n-        process.wait_for_line(Duration::from_secs(10), \"GET\");\n-        process.wait_for_line(Duration::from_secs(10), \"POST\");\n-        process.wait_for_line(Duration::from_secs(10), \"PUT\");\n-        process.wait_for_line(Duration::from_secs(10), \"DELETE\");\n+        process.wait_for_line(Duration::from_secs(10), \"GET\").await;\n+        process.wait_for_line(Duration::from_secs(10), \"POST\").await;\n+        process.wait_for_line(Duration::from_secs(10), \"PUT\").await;\n+        process\n+            .wait_for_line(Duration::from_secs(10), \"DELETE\")\n+            .await;\n         timeout(Duration::from_secs(40), process.child.wait())\n             .await\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&process);\n+        application.assert(&process).await;\n     }\n }\n\n@@ -17,7 +17,7 @@ mod utils {\n         net::Ipv4Addr,\n         path::PathBuf,\n         process::Stdio,\n-        sync::{Arc, Condvar, Mutex},\n+        sync::{Arc, Condvar},\n         time::Duration,\n     };\n \n@@ -42,7 +42,10 @@ mod utils {\n     use tokio::{\n         io::{AsyncReadExt, AsyncWriteExt, BufReader},\n         process::{Child, Command},\n-        sync::oneshot::{self, Sender},\n+        sync::{\n+            oneshot::{self, Sender},\n+            Mutex,\n+        },\n     };\n \n     const TEXT: &'static str = \"Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat. Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur. Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\";\n@@ -146,35 +149,35 @@ mod utils {\n     }\n \n     impl TestProcess {\n-        pub fn get_stdout(&self) -> String {\n-            self.stdout.lock().unwrap().clone()\n+        pub async fn get_stdout(&self) -> String {\n+            self.stdout.lock().await.clone()\n         }\n \n-        pub fn get_stderr(&self) -> String {\n-            self.stderr.lock().unwrap().clone()\n+        pub async fn get_stderr(&self) -> String {\n+            self.stderr.lock().await.clone()\n         }\n \n-        pub fn assert_log_level(&self, stderr: bool, level: &str) {\n+        pub async fn assert_log_level(&self, stderr: bool, level: &str) {\n             if stderr {\n-                assert!(!self.stderr.lock().unwrap().contains(level));\n+                assert!(!self.stderr.lock().await.contains(level));\n             } else {\n-                assert!(!self.stdout.lock().unwrap().contains(level));\n+                assert!(!self.stdout.lock().await.contains(level));\n             }\n         }\n \n-        pub fn assert_python_fileops_stderr(&self) {\n-            assert!(!self.stderr.lock().unwrap().contains(\"FAILED\"));\n+        pub async fn assert_python_fileops_stderr(&self) {\n+            assert!(!self.stderr.lock().await.contains(\"FAILED\"));\n         }\n \n         pub async fn wait_assert_success(self) {\n             let output = self.child.wait_with_output().await.unwrap();\n             assert!(output.status.success());\n         }\n \n-        pub fn wait_for_line(&self, timeout: Duration, line: &str) {\n+        pub async fn wait_for_line(&self, timeout: Duration, line: &str) {\n             let now = std::time::Instant::now();\n             while now.elapsed() < timeout {\n-                let stderr = self.get_stderr();\n+                let stderr = self.get_stderr().await;\n                 if stderr.contains(line) {\n                     return;\n                 }\n@@ -190,7 +193,7 @@ mod utils {\n             }\n         }\n \n-        pub fn from_child(mut child: Child, tempdir: TempDir) -> TestProcess {\n+        pub async fn from_child(mut child: Child, tempdir: TempDir) -> TestProcess {\n             let stderr_data = Arc::new(Mutex::new(String::new()));\n             let stdout_data = Arc::new(Mutex::new(String::new()));\n             let child_stderr = child.stderr.take().unwrap();\n@@ -211,7 +214,7 @@ mod utils {\n                     let string = String::from_utf8_lossy(&buf[..n]);\n                     eprintln!(\"stderr {} {pid}: {}\", format_time(), string);\n                     {\n-                        stderr_data_reader.lock().unwrap().push_str(&string);\n+                        stderr_data_reader.lock().await.push_str(&string);\n                     }\n                 }\n             });\n@@ -226,7 +229,7 @@ mod utils {\n                     let string = String::from_utf8_lossy(&buf[..n]);\n                     print!(\"stdout {} {pid}: {}\", format_time(), string);\n                     {\n-                        stdout_data_reader.lock().unwrap().push_str(&string);\n+                        stdout_data_reader.lock().await.push_str(&string);\n                     }\n                 }\n             });\n@@ -296,13 +299,13 @@ mod utils {\n             run_exec_with_target(self.get_cmd(), target, namespace, args, env).await\n         }\n \n-        pub fn assert(&self, process: &TestProcess) {\n+        pub async fn assert(&self, process: &TestProcess) {\n             match self {\n                 Application::PythonFastApiHTTP => {\n-                    process.assert_log_level(true, \"ERROR\");\n-                    process.assert_log_level(false, \"ERROR\");\n-                    process.assert_log_level(true, \"CRITICAL\");\n-                    process.assert_log_level(false, \"CRITICAL\");\n+                    process.assert_log_level(true, \"ERROR\").await;\n+                    process.assert_log_level(false, \"ERROR\").await;\n+                    process.assert_log_level(true, \"CRITICAL\").await;\n+                    process.assert_log_level(false, \"CRITICAL\").await;\n                 }\n                 _ => {}\n             }\n@@ -334,9 +337,9 @@ mod utils {\n         }\n \n         #[cfg(target_os = \"linux\")]\n-        pub fn assert(&self, process: TestProcess) {\n+        pub async fn assert(&self, process: TestProcess) {\n             match self {\n-                FileOps::Python => process.assert_python_fileops_stderr(),\n+                FileOps::Python => process.assert_python_fileops_stderr().await,\n                 _ => {}\n             }\n         }\n@@ -450,11 +453,11 @@ mod utils {\n             server.id().unwrap()\n         );\n         // We need to hold temp dir until the process is finished\n-        TestProcess::from_child(server, temp_dir)\n+        TestProcess::from_child(server, temp_dir).await\n     }\n \n     /// Runs `mirrord ls` command and asserts if the json matches the expected format\n-    pub fn run_ls(args: Option<Vec<&str>>, namespace: Option<&str>) -> TestProcess {\n+    pub async fn run_ls(args: Option<Vec<&str>>, namespace: Option<&str>) -> TestProcess {\n         let path = match option_env!(\"MIRRORD_TESTS_USE_BINARY\") {\n             None => env!(\"CARGO_BIN_FILE_MIRRORD\"),\n             Some(binary_path) => binary_path,\n@@ -479,7 +482,7 @@ mod utils {\n             \"executed mirrord with args {mirrord_args:?} pid {}\",\n             process.id().unwrap()\n         );\n-        TestProcess::from_child(process, temp_dir)\n+        TestProcess::from_child(process, temp_dir).await\n     }\n \n     #[fixture]\n@@ -494,7 +497,7 @@ mod utils {\n     struct ResourceGuard {\n         /// Used in the implementation of [`Drop`] for this struct to block until the background\n         /// task exits.\n-        task_finished: Arc<(Mutex<bool>, Condvar)>,\n+        task_finished: Arc<(std::sync::Mutex<bool>, Condvar)>,\n         /// Used to inform the background task whether this guard was dropped during panic.\n         panic_tx: Option<Sender<bool>>,\n     }\n@@ -511,7 +514,7 @@ mod utils {\n             api.create(&PostParams::default(), data).await?;\n \n             let (panic_tx, panic_rx) = oneshot::channel::<bool>();\n-            let task_finished = Arc::new((Mutex::new(false), Condvar::new()));\n+            let task_finished = Arc::new((std::sync::Mutex::new(false), Condvar::new()));\n             let finished = task_finished.clone();\n \n             tokio::spawn(async move {\n\n@@ -11,10 +11,10 @@ mod target {\n     #[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n     pub async fn mirrord_ls(#[future] service: KubeService) {\n         let service = service.await;\n-        let mut process = run_ls(None, None);\n+        let mut process = run_ls(None, None).await;\n         let res = process.child.wait().await.unwrap();\n         assert!(res.success());\n-        let stdout = process.get_stdout();\n+        let stdout = process.get_stdout().await;\n         let targets: Vec<String> = serde_json::from_str(&stdout).unwrap();\n         let re = Regex::new(r\"^(pod|deployment)/.+(/container/.+)?$\").unwrap();\n         targets\n\n@@ -24,7 +24,7 @@ mod targetless {\n         let mut process = app.run_targetless(None, None, None).await;\n         let res = process.child.wait().await.unwrap();\n         assert!(res.success());\n-        let stdout = process.get_stdout();\n+        let stdout = process.get_stdout().await;\n         assert!(stdout.contains(r#\"\"apiVersion\": \"v1\"\"#))\n     }\n }\n\n@@ -44,14 +44,16 @@ mod steal {\n             .run(&service.target, Some(&service.namespace), Some(flags), None)\n             .await;\n \n-        process.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        process\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n         send_requests(&url, true, Default::default()).await;\n         tokio::time::timeout(Duration::from_secs(40), process.child.wait())\n             .await\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&process);\n+        application.assert(&process).await;\n     }\n \n     #[cfg(target_os = \"linux\")]\n@@ -85,14 +87,16 @@ mod steal {\n             )\n             .await;\n \n-        process.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        process\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n         send_requests(&url, true, Default::default()).await;\n         tokio::time::timeout(Duration::from_secs(40), process.child.wait())\n             .await\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&process);\n+        application.assert(&process).await;\n     }\n \n     /// Test the app continues running with mirrord and traffic is no longer stolen after the app\n@@ -118,10 +122,14 @@ mod steal {\n             .await;\n \n         // Verify that we hooked the socket operations and the agent started stealing.\n-        process.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        process\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         // Wait for the test app to close the socket and tell us about it.\n-        process.wait_for_line(Duration::from_secs(40), \"Closed socket\");\n+        process\n+            .wait_for_line(Duration::from_secs(40), \"Closed socket\")\n+            .await;\n \n         // Flake-proofing the test:\n         // If we connect to the service between the time the test application had closed its socket\n@@ -206,12 +214,16 @@ mod steal {\n         let (addr, port) = get_service_host_and_port(kube_client.clone(), &service).await;\n \n         // Wait for the app to start listening for stolen data before connecting.\n-        process.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        process\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         let mut tcp_stream = TcpStream::connect((addr, port as u16)).unwrap();\n \n         // Wait for the test app to close the socket and tell us about it.\n-        process.wait_for_line(Duration::from_secs(40), \"Closed socket.\");\n+        process\n+            .wait_for_line(Duration::from_secs(40), \"Closed socket.\")\n+            .await;\n \n         const DATA: &[u8; 16] = b\"upper me please\\n\";\n \n@@ -278,7 +290,9 @@ mod steal {\n             )\n             .await;\n \n-        client.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        client\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         let mut headers = HeaderMap::default();\n         headers.insert(\"x-filter\", \"yes\".parse().unwrap());\n@@ -289,7 +303,7 @@ mod steal {\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&client);\n+        application.assert(&client).await;\n     }\n \n     #[rstest]\n@@ -318,7 +332,9 @@ mod steal {\n             )\n             .await;\n \n-        client.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        client\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         let mut headers = HeaderMap::default();\n         headers.insert(\"x-filter\", \"yes\".parse().unwrap());\n@@ -329,7 +345,7 @@ mod steal {\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&client);\n+        application.assert(&client).await;\n     }\n \n     #[rstest]\n@@ -358,7 +374,9 @@ mod steal {\n             )\n             .await;\n \n-        client.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        client\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         let headers = HeaderMap::default();\n         // Send a GET that should go through to remote\n@@ -384,7 +402,7 @@ mod steal {\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&client);\n+        application.assert(&client).await;\n     }\n \n     #[rstest]\n@@ -413,7 +431,9 @@ mod steal {\n             )\n             .await;\n \n-        mirrored_process.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        mirrored_process\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         // Send a GET that should be matched and stolen.\n         // And a DELETE that closes the app.\n@@ -449,7 +469,7 @@ mod steal {\n             .expect(\"Timed out waiting for mirrored_process!\")\n             .expect(\"mirrored_process failed!\");\n \n-        application.assert(&mirrored_process);\n+        application.assert(&mirrored_process).await;\n     }\n \n     /// To run on mac, first build universal binary: (from repo root) `scripts/build_fat_mac.sh`\n@@ -485,7 +505,9 @@ mod steal {\n             )\n             .await;\n \n-        mirrorded_process.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        mirrorded_process\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         // Send a GET that should be matched and stolen.\n         let client = reqwest::Client::new();\n@@ -517,7 +539,7 @@ mod steal {\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&mirrorded_process);\n+        application.assert(&mirrorded_process).await;\n     }\n \n     /// Test the case where running with `steal` set and an http header filter, but getting a\n@@ -551,7 +573,9 @@ mod steal {\n             )\n             .await;\n \n-        mirrorded_process.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        mirrorded_process\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         let addr = SocketAddr::new(host.trim().parse().unwrap(), port as u16);\n         let mut stream = TcpStream::connect(addr).unwrap();\n@@ -564,7 +588,7 @@ mod steal {\n         assert_eq!(&buf[8..], tcp_data); // The correct data was sent there and back.\n \n         // Verify the data was passed through and nothing was sent to the local app.\n-        let stdout_after = mirrorded_process.get_stdout();\n+        let stdout_after = mirrorded_process.get_stdout().await;\n         assert!(!stdout_after.contains(\"LOCAL APP GOT DATA\"));\n \n         // Send a DELETE that should be matched and thus stolen, closing the app.\n@@ -580,7 +604,7 @@ mod steal {\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&mirrorded_process);\n+        application.assert(&mirrorded_process).await;\n     }\n \n     /// Test the case where running with `steal` set and an http header filter, we get an HTTP\n@@ -621,7 +645,9 @@ mod steal {\n             )\n             .await;\n \n-        mirrorded_process.wait_for_line(Duration::from_secs(40), \"daemon subscribed\");\n+        mirrorded_process\n+            .wait_for_line(Duration::from_secs(40), \"daemon subscribed\")\n+            .await;\n \n         // Create a websocket connection to test the HTTP upgrade bypass.\n         let host = host.trim();\n@@ -649,7 +675,7 @@ mod steal {\n         assert_eq!(&read_message[8..], write_data); // The correct data was sent there and back.\n \n         // Verify the data was passed through and nothing was sent to the local app.\n-        let stdout_after = mirrorded_process.get_stdout();\n+        let stdout_after = mirrorded_process.get_stdout().await;\n         assert!(!stdout_after.contains(\"LOCAL APP GOT DATA\"));\n \n         // Send a DELETE that should be matched and thus stolen, closing the app.\n@@ -665,6 +691,6 @@ mod steal {\n             .unwrap()\n             .unwrap();\n \n-        application.assert(&mirrorded_process);\n+        application.assert(&mirrorded_process).await;\n     }\n }\n"}
{"id": 3, "ground_truth": "Network", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Describe the bug\r\n\r\nhttps://github.com/paradigmxyz/reth/actions/runs/5405202612/jobs/9820416442?pr=3455#step:8:789\r\n\r\n```console\r\n--- STDERR:              reth-network::it connect::test_incoming_node_id_blacklist ---\r\nthread 'connect::test_incoming_node_id_blacklist' panicked at 'called `Result::unwrap()` on an `Err` value: HTTPError(reqwest::Error { kind: Request, url: Url { scheme: \"http\", cannot_be_a_base: false, username: \"\", password: None, host: Some(Ipv4(127.0.0.1)), port: Some(44187), path: \"/\", query: None, fragment: None }, source: hyper::Error(Connect, ConnectError(\"tcp connect error\", Os { code: 111, kind: ConnectionRefused, message: \"Connection refused\" })) })', crates/net/network/tests/it/connect.rs:325:70\r\n```\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow the Code of Conduct\n\n### Code Patch:\n@@ -318,7 +318,7 @@ async fn test_incoming_node_id_blacklist() {\n \n         // instantiate geth and add ourselves as a peer\n         let temp_dir = tempfile::tempdir().unwrap().into_path();\n-        let geth = Geth::new().data_dir(temp_dir).disable_discovery().spawn();\n+        let geth = Geth::new().data_dir(temp_dir).disable_discovery().authrpc_port(0).spawn();\n         let geth_endpoint = SocketAddr::new([127, 0, 0, 1].into(), geth.port());\n         let provider = Provider::<Http>::try_from(format!(\"http://{geth_endpoint}\")).unwrap();\n \n@@ -371,7 +371,7 @@ async fn test_incoming_connect_with_single_geth() {\n \n         // instantiate geth and add ourselves as a peer\n         let temp_dir = tempfile::tempdir().unwrap().into_path();\n-        let geth = Geth::new().data_dir(temp_dir).disable_discovery().spawn();\n+        let geth = Geth::new().data_dir(temp_dir).disable_discovery().authrpc_port(0).spawn();\n         let geth_endpoint = SocketAddr::new([127, 0, 0, 1].into(), geth.port());\n         let provider = Provider::<Http>::try_from(format!(\"http://{geth_endpoint}\")).unwrap();\n \n@@ -429,7 +429,7 @@ async fn test_outgoing_connect_with_single_geth() {\n \n         // instantiate geth and add ourselves as a peer\n         let temp_dir = tempfile::tempdir().unwrap().into_path();\n-        let geth = Geth::new().disable_discovery().data_dir(temp_dir).spawn();\n+        let geth = Geth::new().disable_discovery().data_dir(temp_dir).authrpc_port(0).spawn();\n \n         let geth_p2p_port = geth.p2p_port().unwrap();\n         let geth_socket = SocketAddr::new([127, 0, 0, 1].into(), geth_p2p_port);\n@@ -474,7 +474,7 @@ async fn test_geth_disconnect() {\n \n         // instantiate geth and add ourselves as a peer\n         let temp_dir = tempfile::tempdir().unwrap().into_path();\n-        let geth = Geth::new().disable_discovery().data_dir(temp_dir).spawn();\n+        let geth = Geth::new().disable_discovery().data_dir(temp_dir).authrpc_port(0).spawn();\n \n         let geth_p2p_port = geth.p2p_port().unwrap();\n         let geth_socket = SocketAddr::new([127, 0, 0, 1].into(), geth_p2p_port);\n"}
{"id": 49, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis seems to be flaky, and unrelated to the PR:\nhttps://drone-auto-casper-network.casperlabs.io/casper-network/casper-node/6206/2/4\n\n### Code Patch:\n@@ -9,7 +9,7 @@ use std::{\n     time::Instant,\n };\n \n-use tracing::{error, trace};\n+use tracing::{error, trace, warn};\n \n use casper_hashing::Digest;\n use casper_types::bytesrepr::{self, FromBytes, ToBytes};\n@@ -96,11 +96,12 @@ where\n                             current = next;\n                         }\n                         None => {\n-                            panic!(\n+                            warn!(\n                                 \"No trie value at key: {:?} (reading from key: {:?})\",\n                                 pointer.hash(),\n                                 key\n                             );\n+                            return Ok(ReadResult::NotFound);\n                         }\n                     },\n                     None => {\n@@ -117,11 +118,12 @@ where\n                             current = next;\n                         }\n                         None => {\n-                            panic!(\n+                            warn!(\n                                 \"No trie value at key: {:?} (reading from key: {:?})\",\n                                 pointer.hash(),\n                                 key\n                             );\n+                            return Ok(ReadResult::NotFound);\n                         }\n                     }\n                 } else {\n@@ -192,11 +194,12 @@ where\n                 let next = match store.get(txn, pointer.hash())? {\n                     Some(next) => next,\n                     None => {\n-                        panic!(\n-                            \"No trie value at key: {:?} (reading from key: {:?})\",\n+                        warn!(\n+                            \"No trie value at key: {:?} (reading from path: {:?})\",\n                             pointer.hash(),\n-                            key\n+                            path\n                         );\n+                        return Ok(ReadResult::NotFound);\n                     }\n                 };\n                 depth += 1;\n@@ -216,11 +219,12 @@ where\n                 let next = match store.get(txn, pointer.hash())? {\n                     Some(next) => next,\n                     None => {\n-                        panic!(\n-                            \"No trie value at key: {:?} (reading from key: {:?})\",\n+                        warn!(\n+                            \"No trie value at key: {:?} (reading from path: {:?})\",\n                             pointer.hash(),\n-                            key\n+                            path\n                         );\n+                        return Ok(ReadResult::NotFound);\n                     }\n                 };\n                 depth += affix.len();\n@@ -410,7 +414,7 @@ fn scan<K, V, T, S, E>(\n     store: &S,\n     key_bytes: &[u8],\n     root: &Trie<K, V>,\n-) -> Result<TrieScan<K, V>, E>\n+) -> Result<Option<TrieScan<K, V>>, E>\n where\n     K: ToBytes + FromBytes + Clone,\n     V: ToBytes + FromBytes + Clone,\n@@ -428,7 +432,7 @@ where\n     loop {\n         match current {\n             leaf @ Trie::Leaf { .. } => {\n-                return Ok(TrieScan::new(leaf, acc));\n+                return Ok(Some(TrieScan::new(leaf, acc)));\n             }\n             Trie::Node { pointer_block } => {\n                 let index = {\n@@ -443,7 +447,7 @@ where\n                 let pointer = match maybe_pointer {\n                     Some(pointer) => pointer,\n                     None => {\n-                        return Ok(TrieScan::new(Trie::Node { pointer_block }, acc));\n+                        return Ok(Some(TrieScan::new(Trie::Node { pointer_block }, acc)));\n                     }\n                 };\n                 match store.get(txn, pointer.hash())? {\n@@ -453,18 +457,19 @@ where\n                         acc.push((index, Trie::Node { pointer_block }))\n                     }\n                     None => {\n-                        panic!(\n+                        warn!(\n                             \"No trie value at key: {:?} (reading from path: {:?})\",\n                             pointer.hash(),\n                             path\n                         );\n+                        return Ok(None);\n                     }\n                 }\n             }\n             Trie::Extension { affix, pointer } => {\n                 let sub_path = &path[depth..depth + affix.len()];\n                 if sub_path != affix.as_slice() {\n-                    return Ok(TrieScan::new(Trie::Extension { affix, pointer }, acc));\n+                    return Ok(Some(TrieScan::new(Trie::Extension { affix, pointer }, acc)));\n                 }\n                 match store.get(txn, pointer.hash())? {\n                     Some(next) => {\n@@ -477,11 +482,12 @@ where\n                         acc.push((index, Trie::Extension { affix, pointer }))\n                     }\n                     None => {\n-                        panic!(\n+                        warn!(\n                             \"No trie value at key: {:?} (reading from path: {:?})\",\n                             pointer.hash(),\n                             path\n                         );\n+                        return Ok(None);\n                     }\n                 }\n             }\n@@ -519,7 +525,10 @@ where\n \n     let key_bytes = key_to_delete.to_bytes()?;\n     let TrieScan { tip, mut parents } =\n-        scan::<_, _, _, _, E>(correlation_id, txn, store, &key_bytes, &root_trie)?;\n+        match scan::<_, _, _, _, E>(correlation_id, txn, store, &key_bytes, &root_trie)? {\n+            Some(trie_scan) => trie_scan,\n+            None => return Ok(DeleteResult::DoesNotExist),\n+        };\n \n     // Check that tip is a leaf\n     match tip {\n@@ -967,7 +976,13 @@ where\n             };\n             let path: Vec<u8> = key.to_bytes()?;\n             let TrieScan { tip, parents } =\n-                scan::<K, V, T, S, E>(correlation_id, txn, store, &path, &current_root)?;\n+                match scan::<K, V, T, S, E>(correlation_id, txn, store, &path, &current_root)? {\n+                    Some(trie_scan) => trie_scan,\n+                    // If we are scanning the trie and it's not complete under the given root, then\n+                    // in the context of a write we must consider this root to \"not exist\".\n+                    // This can happen when a trie is being sync'd or is incomplete.\n+                    None => return Ok(WriteResult::RootNotFound),\n+                };\n             let new_elements: Vec<(Digest, Trie<K, V>)> = match tip {\n                 // If the \"tip\" is the same as the new leaf, then the leaf\n                 // is already in the Trie.\n\n@@ -32,7 +32,8 @@ where\n         store,\n         key,\n         &root,\n-    )?;\n+    )?\n+    .expect(\"trie scan returned no leaf\");\n \n     for (index, parent) in parents.into_iter().rev() {\n         let expected_tip_hash = {\n"}
{"id": 18, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n## Issue summary\r\n\r\nThe RPC test for `Filecoin.EthGetBlockByNumber` fails randomly. Investigate and fix this. Until then, the test is ignored.\r\n\r\n## Other information and links\r\n\r\n<!-- Add any other context or screenshots about the issue here. -->\r\n\r\n<!-- Thank you 🙏 -->\r\n\n\n### Code Patch:\n@@ -35,6 +35,10 @@\n \n ### Fixed\n \n+- [#4603](https://github.com/ChainSafe/forest/pull/4603) Fixed incorrect\n+  deserialisation in `Filecoin.EthGetBlockByNumber` and\n+  `Filecoin.EthGetBlockByHash` RPC methods.\n+\n ## Forest 0.19.2 \"Eagle\"\n \n Non-mandatory release that includes a fix for the Prometheus-incompatible\n\n@@ -9,7 +9,3 @@\n !Filecoin.StateCall\n # CustomCheckFailed in Forest: https://github.com/ChainSafe/forest/issues/4446\n !Filecoin.StateCirculatingSupply\n-# CustomCheckFailed in Forest: https://github.com/ChainSafe/forest/issues/4584\n-!Filecoin.EthGetBlockByHash\n-# CustomCheckFailed in Forest: https://github.com/ChainSafe/forest/issues/4585\n-!Filecoin.EthGetBlockByNumber\n\n@@ -19,7 +19,3 @@\n !Filecoin.StateCirculatingSupply\n # The estimation is inaccurate only for offline RPC server, to be investigated: https://github.com/ChainSafe/forest/issues/4555\n !Filecoin.EthEstimateGas\n-# CustomCheckFailed in Forest: https://github.com/ChainSafe/forest/issues/4584\n-!Filecoin.EthGetBlockByHash\n-# CustomCheckFailed in Forest: https://github.com/ChainSafe/forest/issues/4585\n-!Filecoin.EthGetBlockByNumber\n\n@@ -35,7 +35,8 @@ use crate::shim::trace::{CallReturn, ExecutionEvent};\n use crate::shim::{clock::ChainEpoch, state_tree::StateTree};\n use crate::utils::db::BlockstoreExt as _;\n use anyhow::{bail, Result};\n-use bytes::Buf;\n+use cbor4ii::core::dec::Decode as _;\n+use cbor4ii::core::Value;\n use cid::Cid;\n use fvm_ipld_blockstore::Blockstore;\n use fvm_ipld_encoding::{RawBytes, CBOR, DAG_CBOR, IPLD_RAW};\n@@ -806,10 +807,10 @@ fn encode_as_abi_helper(param1: u64, param2: u64, data: &[u8]) -> Vec<u8> {\n fn decode_payload(payload: &fvm_ipld_encoding::RawBytes, codec: u64) -> Result<EthBytes> {\n     match codec {\n         DAG_CBOR | CBOR => {\n-            let result: Result<Vec<u8>, _> = serde_ipld_dagcbor::de::from_reader(payload.reader());\n-            match result {\n-                Ok(buffer) => Ok(EthBytes(buffer)),\n-                Err(err) => bail!(\"decode_payload: failed to decode cbor payload: {err}\"),\n+            let mut reader = cbor4ii::core::utils::SliceReader::new(payload.bytes());\n+            match Value::decode(&mut reader) {\n+                Ok(Value::Bytes(bytes)) => Ok(EthBytes(bytes)),\n+                _ => bail!(\"failed to read params byte array\"),\n             }\n         }\n         IPLD_RAW => Ok(EthBytes(payload.to_vec())),\n"}
{"id": 33, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nLooks like the new `fuzz_threaded_connections` is flaky, it can hit an unwrap on the first read_event, which shouldn't fail, but CI managed to make it. This isn't an immediate correctness concern, however, as that erroring isnt a problem, just surprising.\n\n### Code Patch:\n@@ -2334,7 +2334,7 @@ mod tests {\n \t\t\t\t\tlet addr_b = NetAddress::IPv4{addr: [127, 0, 0, 1], port: 1001};\n \t\t\t\t\tlet initial_data = peers[1].new_outbound_connection(id_a, fd_b.clone(), Some(addr_a.clone())).unwrap();\n \t\t\t\t\tpeers[0].new_inbound_connection(fd_a.clone(), Some(addr_b.clone())).unwrap();\n-\t\t\t\t\tassert_eq!(peers[0].read_event(&mut fd_a, &initial_data).unwrap(), false);\n+\t\t\t\t\tif peers[0].read_event(&mut fd_a, &initial_data).is_err() { break; }\n \n \t\t\t\t\twhile start_time.elapsed() < std::time::Duration::from_secs(1) {\n \t\t\t\t\t\tpeers[0].process_events();\n@@ -2364,8 +2364,10 @@ mod tests {\n \t\t\t\t\t\t\t\t},\n \t\t\t\t\t\t\t});\n \n-\t\t\t\t\t\tpeers[0].timer_tick_occurred();\n-\t\t\t\t\t\tpeers[1].timer_tick_occurred();\n+\t\t\t\t\t\tif ctr % 2 == 0 {\n+\t\t\t\t\t\t\tpeers[0].timer_tick_occurred();\n+\t\t\t\t\t\t\tpeers[1].timer_tick_occurred();\n+\t\t\t\t\t\t}\n \t\t\t\t\t}\n \n \t\t\t\t\tpeers[0].socket_disconnected(&fd_a);\n"}
{"id": 43, "ground_truth": "Network", "predicted": "Time", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n```shell\r\nfailures:\r\n    api::http_service::test_http_service_tls_server_mutual_tls\r\n    servers::http::http_query_handlers::test_http_service_tls_server_mutual_tls\r\n```\r\n\r\nhttps://github.com/datafuselabs/databend/actions/runs/3057687821/jobs/4933098585\n\n### Code Patch:\n@@ -0,0 +1,77 @@\n+# Test Certs\n+\n+We use certs inside `cfssl` for tls testing.\n+\n+Please use `./gencert.sh generate_keys` to refresh all the certs.\n+\n+## Current Used\n+\n+The next refresh time will be `Sep 15 05:03:00 2023 GMT`\n+\n+```shell\n+:) openssl x509 -noout -text -in ./tests/certs/tls/cfssl/server/server.pem\n+Certificate:\n+    Data:\n+        Version: 3 (0x2)\n+        Serial Number:\n+            6c:35:77:ed:6d:6f:49:be:80:3d:96:ce:00:d0:d2:96:21:68:ba:9c\n+        Signature Algorithm: sha256WithRSAEncryption\n+        Issuer: C = US, ST = LA, L = CA, O = Datafuselabs, OU = Databend, CN = Databend\n+        Validity\n+            Not Before: Sep 15 05:03:00 2022 GMT\n+            Not After : Sep 15 05:03:00 2023 GMT\n+        Subject: C = US, ST = CA, L = San Francisco, CN = Databend Server\n+        Subject Public Key Info:\n+            Public Key Algorithm: rsaEncryption\n+                RSA Public-Key: (2048 bit)\n+                Modulus:\n+                    00:af:95:7d:ac:14:90:33:47:f4:2d:6e:8b:8d:70:\n+                    30:d0:a6:a4:f5:67:65:87:f2:e5:8d:4b:bf:ae:4e:\n+                    79:00:fd:2c:24:49:ea:2d:07:18:dd:67:cd:76:c3:\n+                    0e:d8:f4:5f:21:26:f9:3e:db:6a:ba:3f:f3:4e:a3:\n+                    4e:f9:eb:d7:1c:8d:45:90:2e:bb:1d:55:67:36:ec:\n+                    11:86:96:f9:59:29:1e:9f:92:9b:87:95:d5:80:67:\n+                    a8:d5:88:21:d8:85:83:83:ce:8f:56:5d:95:0e:54:\n+                    dd:64:9a:72:c4:45:b1:2b:63:83:57:7e:fc:6d:1d:\n+                    a9:6f:ff:f2:ca:54:30:bd:09:ff:e2:2c:30:02:8a:\n+                    4c:f1:bf:a0:b7:2e:e7:fc:13:8f:1c:a1:ab:6e:98:\n+                    2f:98:cb:be:6e:9a:fb:27:4d:86:bd:ec:ab:0f:c2:\n+                    f9:24:10:8c:33:26:e6:08:10:02:b7:bb:de:ea:02:\n+                    ab:af:4f:6c:98:88:f5:9d:ec:b5:c5:db:f0:44:79:\n+                    d7:4a:43:ab:c4:e5:65:57:79:3b:3d:a2:ac:9e:7c:\n+                    30:ee:25:18:b9:fe:9c:11:2b:87:ee:a5:20:05:21:\n+                    7b:20:31:b2:f7:4b:8e:77:d8:d1:39:47:4d:34:7d:\n+                    5f:c0:55:da:10:3e:be:e9:e7:96:bd:c3:d7:21:cf:\n+                    ee:cb\n+                Exponent: 65537 (0x10001)\n+        X509v3 extensions:\n+            X509v3 Key Usage: critical\n+                Digital Signature, Key Encipherment\n+            X509v3 Extended Key Usage:\n+                TLS Web Server Authentication, TLS Web Client Authentication\n+            X509v3 Basic Constraints: critical\n+                CA:FALSE\n+            X509v3 Subject Key Identifier:\n+                94:82:81:4A:8F:87:11:AF:25:63:16:86:AA:39:C5:15:1D:30:E4:06\n+            X509v3 Authority Key Identifier:\n+                keyid:9F:80:F4:B3:13:48:27:02:C5:FD:21:DC:AB:E9:23:1E:C6:73:DF:5F\n+\n+            X509v3 Subject Alternative Name:\n+                DNS:localhost, IP Address:127.0.0.1, IP Address:0.0.0.0, IP Address:0:0:0:0:0:0:0:1\n+    Signature Algorithm: sha256WithRSAEncryption\n+         83:15:bf:21:a0:1b:c8:9d:7e:50:67:c7:84:4e:d7:f4:e7:61:\n+         1c:25:4a:0a:0c:86:82:60:e4:00:58:b9:f0:8d:a1:51:e6:c1:\n+         12:47:67:e8:3a:6b:2f:ba:b8:c0:e5:29:94:23:2d:bd:76:01:\n+         40:b3:62:12:12:8d:42:94:5b:ad:ce:18:c1:4d:d5:fb:60:a4:\n+         45:7c:64:b6:fb:f7:99:b8:39:07:1c:c2:f9:8a:a5:56:9f:08:\n+         d9:5c:76:a6:c1:f2:e3:41:d2:f6:41:96:ec:70:91:d3:ae:3d:\n+         b2:3b:c6:b5:f5:f4:46:dd:b5:9c:36:4c:1e:ae:a7:df:65:43:\n+         ea:14:50:19:1d:cf:7e:44:24:41:71:58:8a:20:b5:0d:a3:96:\n+         14:69:75:ba:23:7a:cb:64:7f:ef:5c:53:4b:59:6c:d9:0a:2e:\n+         b7:2f:65:f3:53:82:ed:94:e2:60:bd:4c:d9:e5:2a:06:39:4b:\n+         a7:c7:f5:d7:9e:62:f0:86:85:48:66:9a:a5:c2:23:27:7d:cd:\n+         05:51:20:90:f0:ac:d4:42:cf:4c:36:77:85:0c:98:93:2c:87:\n+         c6:36:90:ad:d8:4d:ca:00:a7:75:31:56:d8:45:e1:d6:71:0f:\n+         bf:22:23:9e:47:46:f2:4b:b4:d0:9e:77:52:69:49:88:dc:06:\n+         8f:2c:d9:3e\n+```\n\n@@ -1,27 +1,27 @@\n -----BEGIN RSA PRIVATE KEY-----\n-MIIEpQIBAAKCAQEAzUAkmwWs/u6siRwLax0N2KqXlLrAYzEUSeXhQA3yxBT+Hj9h\n-bUJauXGVZtvpbY3DaGcbCdzrAig28+u9wh+z9zZa3jzJXAszpqzjuacKw1y7lflu\n-YB8taFTTk4gN+oMr6Zjph9HP1iz9XY97g36x36urZRZsCRlVR+OSR+u/K9ElS4H7\n-28uQkzqCvpdcoUd5B2f8LL20qZrBWS7/4oOq2HGuI/AjjmHG8yXjesMdA7q+Bk+1\n-FkmigZedAQqvo6DVwzrRt8BjnVoYGK7r25dYxBqHyhZAL633p+aB8YYB6Smf0+Nc\n-5bgeObtbUspro8GVUPrI9h8UFYuKr5JJbtBO9QIDAQABAoIBAQDBoA4oVE1j3cLU\n-Qamf7bO/PwdqXlbZ/FXYdM1vAPmQ8wDbhg0h5mPx5236r+Dg4ciC+Clwv4DP8yIM\n-L8s+k2vR/evxFUUb2NFsHlNiM2wa6qNoEvuGIENU1hkWZ1ZQimflocRExbCcqqqG\n-FbJ+VQn2KFuzLFjUqJpkDBJWWNZqvNxiG3lah7I8JKh8/blAgBL/egukvR8CuLUs\n-WkTIi33b5CNPzpA3QzhWqcft7TInwj6sOi8fqfr28lqGpfCwLgXak0aY+PGQFTIN\n-EfVtZJkVamko6+Q8ysXeKA3imMWlAhZ3Lyz0h6+pUaLNfTEd3/YYDGgiccfNVBY6\n-5L//FiYNAoGBAPRlEZfGvGmrqe/V32El/y3j1gGXCpIJND8JOdWQoTivRwyVGuIf\n-MtMdPWW8k0axn+VRuth5wdGEANO8LjZ++TXhowC/ygDpBBIas/IyubhI4ca0RYPH\n-hJwIJHLFWbChu0GMCtSnyaJtCovw/rOVEyYrZNug5sNLqSIyrUQ8qRaXAoGBANb/\n-Odj0PVjIyYbfmZWV983b88DPaPFgo352Ejhdrzk7jBUDjEq5AJpYyZyDiYVMqH2O\n-07ZZAeP1ozPH3QbUcBSuH5PNBVw+q5HzlC8XC8kgZ8crsujiN66bx3VmbArL7l5O\n-+kY7VjqBKacAToN2LrfKpBVcodldm/OVQBqqjWRTAoGANFHGuGVVRJH5aVyx24xU\n-2rAhymlZW/YYdz40YWH7AbWUUj9BaBMhwExMLpF2PfYcsu0SQPGf6oiF3M42XnhE\n-RaKuUKS+S/+WXYHV8A2n8Km5m95P22xFGVos410YXFXjTp76Wpgo9Fq/nUHUZQGO\n-/ebgSpxhioGSDb4nlaoe0LUCgYEAjzBs1pHUSc1wauOA5p966yASt9drjVdPjGoW\n-uHOsv/Fo2pYCWcHdJO79V8BDVwQ3M2hsmIWvPKPcjiAOlP3np95QKZ/icg1LN2mC\n-x9oRf2KVu5DfFzuS2jfmH8qXKpmnqCfxh9T9GPntQ8xwfh23/7B2l8OaB6rQsuo0\n-R45PnQECgYEAlcJr8oM4CLOV0gQXM6g1JGneToCehfEacm5bdbhqu8aG2jYD4Oqt\n-oWgolLCZ+SHfnrUU8ko9OWJFVepBzrl/f8FT+K7vg6gHWOJq35zL0erCzchjHmLe\n-/WHXaEXa01L/XTDhW79OME9I0pt7HcF+4BSte5q8ma1cFW7vZt2TZuc=\n+MIIEpAIBAAKCAQEAwwwBqKYVYSp2XUOmgKhmmEqpiF3NsNwGMPVLASEvr70yN+0m\n+WV7SMYnkCgf48OV96THeUGWSgzBvz931SYuYsj+9VMNSyYy8Fi9KQf5Z/+UWs087\n+q7/MRoJJ8oNwRiILpG+VIocvD/9ELUm31xt6yavvDfvWnuKN9zGbYc0odA9g338X\n+/xhdA91unHffJfTDPAy+b/PSI0dL8y5UEN01uoDPYMeErwFAEha4uD7RB1iL78m2\n+UYjr1M3r3J0Wbnc6ziVQvw5Vl6u1Qq79AOIA95G165owTOM/2y7i/gX/QciCFN1+\n+kWJBoD3FqMHZvu+yaol4ixah4Wsf23yjLOSJoQIDAQABAoIBACnyFRE91m81AIuC\n+FvaoQJgTxm3LLocQakxxFBZEGstCduBMIekqWS3KJ754hHLaB7/0t4i2r1GDQyiC\n+5VNRsB71UjQ+ZTlvCKwEs4WJu7UqgT1CluSStM2dC2YaGjUa8UIFtR7dz6MzFGhg\n+6CZchIWGcjyhokA9HKbChlX8Fu0OWVjFA6oRSCgMN5r3lAI93JP1h8GCT7NUDBJt\n+FjkJggkGnq/WZDC8smfxvy5COIbJQpUfkx/sFSx8kBSYbO14rNfwWFF+SFYvFqEe\n+lJZaNCnuf5/3jpBdSD4Li4c1xqeYv8ZVca/F4XTBzwZ+IhREOyz6PgICWGSfrWxS\n+vGU3CqECgYEA07st0l0jzpBScKQccEoGslJCeI3lJlIxTUp1bqM0SQ6aNhwb35p0\n+LP82yXg235c85BlWG8xuA/NsUzgd0d1fXZdFOg+xrZOvMMk+abZSiCQCigbjDSSO\n+XH6oRzNwqVLbyqV/F38WV3JGjH79QDg4wL3g4AntXMYt1oKH+fXeHrUCgYEA69PP\n+I5L4nv4W2/t/BCsgVBYI+r0lt5fpNr17BMbZSxO2gZxYdE7pzEJ/Ubvh5Nmt/otl\n+ZSCshdvBWGT8yb7M6iwQgi5aHdE2AlhusRAYWDz4PRqtNdTRQzSkXlkJQl050KBs\n+0wPF/T1rE2CHYKFM/J+XW8v0Aao5JPKCXJCVJr0CgYAdljxy9vz38obyopwuvMlu\n+xbvwKLPqxPWKvWrniPrXcqjuAsOKXYT6fLdlM91HJVUm8rtrqwd8mdl7T44rM2oO\n+JANSd7r2bHrAsH5+OzIRQCbyrr+vqHoURsaxHGkT4DZCfRnjaKjwk1F0dJJjFPgp\n+Rt9Tr09x9WeWYCssLMVbOQKBgQCTFYEM8/6i02EYhQxcYjWAnFmvYUdj8ZhgLt/K\n+Q+eE3t+miQfWSt2hKxxTsvYaEPZnWzO7VhykREKbyiir/5e4uhD10Mkd7b1cyyEG\n+yD4W0rgVDE1Z1MlLvEqEGc4fmRuVFmmpVqIm5yagaEUnLJNiwHrboyIl47BObf6G\n+kxqjlQKBgQCskCqTVP3xo48p+lhWHsDJcAldlWUfBxkENb2S8Wlb3IDUwdLQ9FFQ\n+HxR0Vlvc7PAWFGyLXBSVGCUOKagzyUNKBZ4FLwW3Rx02jJ6fhp3RLKVBXpV3ZPH4\n+86y2qFar+Jg99CcxJo08fiFXFzW2K5W6FbKLaUauThwLSEaGD5ZoPg==\n -----END RSA PRIVATE KEY-----\n\n@@ -2,16 +2,16 @@\n MIICyzCCAbMCAQAwZDELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkxBMQswCQYDVQQH\n EwJDQTEVMBMGA1UEChMMRGF0YWZ1c2VsYWJzMREwDwYDVQQLEwhEYXRhYmVuZDER\n MA8GA1UEAxMIRGF0YWJlbmQwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIB\n-AQDNQCSbBaz+7qyJHAtrHQ3YqpeUusBjMRRJ5eFADfLEFP4eP2FtQlq5cZVm2+lt\n-jcNoZxsJ3OsCKDbz673CH7P3NlrePMlcCzOmrOO5pwrDXLuV+W5gHy1oVNOTiA36\n-gyvpmOmH0c/WLP1dj3uDfrHfq6tlFmwJGVVH45JH678r0SVLgfvby5CTOoK+l1yh\n-R3kHZ/wsvbSpmsFZLv/ig6rYca4j8COOYcbzJeN6wx0Dur4GT7UWSaKBl50BCq+j\n-oNXDOtG3wGOdWhgYruvbl1jEGofKFkAvrfen5oHxhgHpKZ/T41zluB45u1tSymuj\n-wZVQ+sj2HxQVi4qvkklu0E71AgMBAAGgIjAgBgkqhkiG9w0BCQ4xEzARMA8GA1Ud\n-EwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAImyi96e2y75q00a+cCeQOHS\n-UWPhrx0SztVhclArfo58rNWPGCpLMW9VCmH25/GfNLb9kZeA/KeONoHAGZvWc0nz\n-dg6mhYRzyyxAslK+2rcHXl0Jldthj6Pu3IfiH4DBmq/06C76jbWJrAwUwNIJw3JS\n-znaOUN55c0KJw7vi5b1UsfKgSmYOQyJkFzHZIE57nxboe0NrshfNhZ4Aq21DxpO+\n-rYURiFeSY9k/AAoQHHPhAkCnjxcP7UWIr3WY+HuqpLM3iPwwiiRjJyP7UNL+QIDG\n-b86ZPqxEGoDZEuheep0U+tOTq9+Zw3kLeVVS8lbLluS57k1w5/d9fC3PqFyxYAw=\n+AQDDDAGophVhKnZdQ6aAqGaYSqmIXc2w3AYw9UsBIS+vvTI37SZZXtIxieQKB/jw\n+5X3pMd5QZZKDMG/P3fVJi5iyP71Uw1LJjLwWL0pB/ln/5RazTzurv8xGgknyg3BG\n+Igukb5Uihy8P/0QtSbfXG3rJq+8N+9ae4o33MZthzSh0D2Dffxf/GF0D3W6cd98l\n+9MM8DL5v89IjR0vzLlQQ3TW6gM9gx4SvAUASFri4PtEHWIvvybZRiOvUzevcnRZu\n+dzrOJVC/DlWXq7VCrv0A4gD3kbXrmjBM4z/bLuL+Bf9ByIIU3X6RYkGgPcWowdm+\n+77JqiXiLFqHhax/bfKMs5ImhAgMBAAGgIjAgBgkqhkiG9w0BCQ4xEzARMA8GA1Ud\n+EwEB/wQFMAMBAf8wDQYJKoZIhvcNAQELBQADggEBAAu3r1y8jIgvUfbVuKHCH1UW\n+LpnJb3HNbq32U5Lm/aKknIv6JoJNE6D7bhxv4pwgASYcm2Lf3nA76SB3Lckg3QyC\n+EglQJ+VZTV31EPsABfFX7qey8fOI39kcLdmDFXmC+QgEXt0Mce1ELe1A6SyuEVYQ\n+IZAqvdLBaFanJANFojdJ3Wh1lrtXAmLB0tFxSHHFSGYB/yxRn2QSidPsA2uyWNg+\n+YEEzW2YHxwQL7mnLx86RgIhWPmtoZM88iU4C2zRwVA+IlN65GcOwEqa9Vx6ayBEZ\n+E9WIUONIGtO+1LfXn+ltNrHCtEkaZQfKjKKrj/oWl4MGD2oFfaUoOWwHyUUH8g4=\n -----END CERTIFICATE REQUEST-----\n\n@@ -1,22 +1,22 @@\n -----BEGIN CERTIFICATE-----\n-MIIDmDCCAoCgAwIBAgIUWCCVRPY7vHuxiqZ3wWYhQBAG7j0wDQYJKoZIhvcNAQEL\n+MIIDmDCCAoCgAwIBAgIUK96ZP6R+qjYEouefLtYYgjTWA/QwDQYJKoZIhvcNAQEL\n BQAwZDELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkxBMQswCQYDVQQHEwJDQTEVMBMG\n A1UEChMMRGF0YWZ1c2VsYWJzMREwDwYDVQQLEwhEYXRhYmVuZDERMA8GA1UEAxMI\n-RGF0YWJlbmQwHhcNMjEwOTE1MDMxNjAwWhcNMzEwOTEzMDMxNjAwWjBkMQswCQYD\n+RGF0YWJlbmQwHhcNMjIwOTE1MDUwMzAwWhcNMzIwOTEyMDUwMzAwWjBkMQswCQYD\n VQQGEwJVUzELMAkGA1UECBMCTEExCzAJBgNVBAcTAkNBMRUwEwYDVQQKEwxEYXRh\n ZnVzZWxhYnMxETAPBgNVBAsTCERhdGFiZW5kMREwDwYDVQQDEwhEYXRhYmVuZDCC\n-ASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAM1AJJsFrP7urIkcC2sdDdiq\n-l5S6wGMxFEnl4UAN8sQU/h4/YW1CWrlxlWbb6W2Nw2hnGwnc6wIoNvPrvcIfs/c2\n-Wt48yVwLM6as47mnCsNcu5X5bmAfLWhU05OIDfqDK+mY6YfRz9Ys/V2Pe4N+sd+r\n-q2UWbAkZVUfjkkfrvyvRJUuB+9vLkJM6gr6XXKFHeQdn/Cy9tKmawVku/+KDqthx\n-riPwI45hxvMl43rDHQO6vgZPtRZJooGXnQEKr6Og1cM60bfAY51aGBiu69uXWMQa\n-h8oWQC+t96fmgfGGAekpn9PjXOW4Hjm7W1LKa6PBlVD6yPYfFBWLiq+SSW7QTvUC\n+ASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAMMMAaimFWEqdl1DpoCoZphK\n+qYhdzbDcBjD1SwEhL6+9MjftJlle0jGJ5AoH+PDlfekx3lBlkoMwb8/d9UmLmLI/\n+vVTDUsmMvBYvSkH+Wf/lFrNPO6u/zEaCSfKDcEYiC6RvlSKHLw//RC1Jt9cbesmr\n+7w371p7ijfcxm2HNKHQPYN9/F/8YXQPdbpx33yX0wzwMvm/z0iNHS/MuVBDdNbqA\n+z2DHhK8BQBIWuLg+0QdYi+/JtlGI69TN69ydFm53Os4lUL8OVZertUKu/QDiAPeR\n+teuaMEzjP9su4v4F/0HIghTdfpFiQaA9xajB2b7vsmqJeIsWoeFrH9t8oyzkiaEC\n AwEAAaNCMEAwDgYDVR0PAQH/BAQDAgEGMA8GA1UdEwEB/wQFMAMBAf8wHQYDVR0O\n-BBYEFL/9lS5s895y6sXdY5wM5Q72/nnbMA0GCSqGSIb3DQEBCwUAA4IBAQAgjkHD\n-Bzhdd8qAmCet/157fPM+x+hVSxZrPLIY3xc3GILT05TbFBrAs7r8yZypiIRerrWD\n-6PyqKGV0r2QAOPO3LCMhkQ4o/OfjoyJxz+ddHClrN03Vp22NVB0jESb9BodFQsUg\n-LaLpmmlKZRpCnYgXj44QEa8mkAaUJIIHnpKw2SjjdFH6gI29ozwBKqan5B9HGN30\n-uh1XKDB0We9EBxlfQFuYSRHDp7OsmRO9fVoTEMYY2JnuKe/oSo+dP1VxAraeXaYn\n-BDtOPSYTD1f7ZNbvJQycsIA+G24Se5lh/+RN8v9xQ6nOM+2FofueCkXr4s6tk8mK\n-BlD/+QcstCie8ZhV\n+BBYEFJ+A9LMTSCcCxf0h3KvpIx7Gc99fMA0GCSqGSIb3DQEBCwUAA4IBAQBc2qFI\n+ld91XtwRgfqvtc8ukTov5szuB23nm/fC0G/Z0IhaLkDK7+WWAQcnGKUV4pZAUzzk\n+SRqDiRzrmy7xQD5CkJpBwmDPT4bi9gASgc9aEkxxx5kKxFu0cCS7LcqgHrWg1/4b\n+oeCwQa2tjeGyLN/NFssUmcVEF+NNI7881CZbd4AI7dtKzysFUDrCN+ROj0cT+fZg\n+X0k3QvTbfeeM029v+r7GXX/tNoCOsk63twF0sYy4joHKBACQC89PJ+99PGNBksaH\n+4AzYUUCJt8bI8y59J3bSuKY3ZvHoTtEow60FA9d0GOdkWRZCXOV5R2zYCdLevZ0t\n+6SnlDzXGQnIbb4B+\n -----END CERTIFICATE-----\n\n@@ -1,28 +1,28 @@\n -----BEGIN PRIVATE KEY-----\n-MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDNQCSbBaz+7qyJ\n-HAtrHQ3YqpeUusBjMRRJ5eFADfLEFP4eP2FtQlq5cZVm2+ltjcNoZxsJ3OsCKDbz\n-673CH7P3NlrePMlcCzOmrOO5pwrDXLuV+W5gHy1oVNOTiA36gyvpmOmH0c/WLP1d\n-j3uDfrHfq6tlFmwJGVVH45JH678r0SVLgfvby5CTOoK+l1yhR3kHZ/wsvbSpmsFZ\n-Lv/ig6rYca4j8COOYcbzJeN6wx0Dur4GT7UWSaKBl50BCq+joNXDOtG3wGOdWhgY\n-ruvbl1jEGofKFkAvrfen5oHxhgHpKZ/T41zluB45u1tSymujwZVQ+sj2HxQVi4qv\n-kklu0E71AgMBAAECggEBAMGgDihUTWPdwtRBqZ/ts78/B2peVtn8Vdh0zW8A+ZDz\n-ANuGDSHmY/Hnbfqv4ODhyIL4KXC/gM/zIgwvyz6Ta9H96/EVRRvY0WweU2IzbBrq\n-o2gS+4YgQ1TWGRZnVlCKZ+WhxETFsJyqqoYVsn5VCfYoW7MsWNSommQMElZY1mq8\n-3GIbeVqHsjwkqHz9uUCAEv96C6S9HwK4tSxaRMiLfdvkI0/OkDdDOFapx+3tMifC\n-Pqw6Lx+p+vbyWoal8LAuBdqTRpj48ZAVMg0R9W1kmRVqaSjr5DzKxd4oDeKYxaUC\n-FncvLPSHr6lRos19MR3f9hgMaCJxx81UFjrkv/8WJg0CgYEA9GURl8a8aaup79Xf\n-YSX/LePWAZcKkgk0Pwk51ZChOK9HDJUa4h8y0x09ZbyTRrGf5VG62HnB0YQA07wu\n-Nn75NeGjAL/KAOkEEhqz8jK5uEjhxrRFg8eEnAgkcsVZsKG7QYwK1KfJom0Ki/D+\n-s5UTJitk26Dmw0upIjKtRDypFpcCgYEA1v852PQ9WMjJht+ZlZX3zdvzwM9o8WCj\n-fnYSOF2vOTuMFQOMSrkAmljJnIOJhUyofY7TtlkB4/WjM8fdBtRwFK4fk80FXD6r\n-kfOULxcLySBnxyuy6OI3rpvHdWZsCsvuXk76RjtWOoEppwBOg3Yut8qkFVyh2V2b\n-85VAGqqNZFMCgYA0Uca4ZVVEkflpXLHbjFTasCHKaVlb9hh3PjRhYfsBtZRSP0Fo\n-EyHATEwukXY99hyy7RJA8Z/qiIXczjZeeERFoq5QpL5L/5ZdgdXwDafwqbmb3k/b\n-bEUZWizjXRhcVeNOnvpamCj0Wr+dQdRlAY795uBKnGGKgZINvieVqh7QtQKBgQCP\n-MGzWkdRJzXBq44Dmn3rrIBK312uNV0+Maha4c6y/8WjalgJZwd0k7v1XwENXBDcz\n-aGyYha88o9yOIA6U/een3lApn+JyDUs3aYLH2hF/YpW7kN8XO5LaN+Yfypcqmaeo\n-J/GH1P0Y+e1DzHB+Hbf/sHaXw5oHqtCy6jRHjk+dAQKBgQCVwmvygzgIs5XSBBcz\n-qDUkad5OgJ6F8Rpyblt1uGq7xobaNgPg6q2haCiUsJn5Id+etRTySj05YkVV6kHO\n-uX9/wVP4ru+DqAdY4mrfnMvR6sLNyGMeYt79YddoRdrTUv9dMOFbv04wT0jSm3sd\n-wX7gFK17mryZrVwVbu9m3ZNm5w==\n+MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDDDAGophVhKnZd\n+Q6aAqGaYSqmIXc2w3AYw9UsBIS+vvTI37SZZXtIxieQKB/jw5X3pMd5QZZKDMG/P\n+3fVJi5iyP71Uw1LJjLwWL0pB/ln/5RazTzurv8xGgknyg3BGIgukb5Uihy8P/0Qt\n+SbfXG3rJq+8N+9ae4o33MZthzSh0D2Dffxf/GF0D3W6cd98l9MM8DL5v89IjR0vz\n+LlQQ3TW6gM9gx4SvAUASFri4PtEHWIvvybZRiOvUzevcnRZudzrOJVC/DlWXq7VC\n+rv0A4gD3kbXrmjBM4z/bLuL+Bf9ByIIU3X6RYkGgPcWowdm+77JqiXiLFqHhax/b\n+fKMs5ImhAgMBAAECggEAKfIVET3WbzUAi4IW9qhAmBPGbcsuhxBqTHEUFkQay0J2\n+4Ewh6SpZLconvniEctoHv/S3iLavUYNDKILlU1GwHvVSND5lOW8IrASzhYm7tSqB\n+PUKW5JK0zZ0LZhoaNRrxQgW1Ht3PozMUaGDoJlyEhYZyPKGiQD0cpsKGVfwW7Q5Z\n+WMUDqhFIKAw3mveUAj3ck/WHwYJPs1QMEm0WOQmCCQaer9ZkMLyyZ/G/LkI4hslC\n+lR+TH+wVLHyQFJhs7Xis1/BYUX5IVi8WoR6Ullo0Ke5/n/eOkF1IPguLhzXGp5i/\n+xlVxr8XhdMHPBn4iFEQ7LPo+AgJYZJ+tbFK8ZTcKoQKBgQDTuy3SXSPOkFJwpBxw\n+SgayUkJ4jeUmUjFNSnVuozRJDpo2HBvfmnQs/zbJeDbflzzkGVYbzG4D82xTOB3R\n+3V9dl0U6D7Gtk68wyT5ptlKIJAKKBuMNJI5cfqhHM3CpUtvKpX8XfxZXckaMfv1A\n+ODjAveDgCe1cxi3Wgof59d4etQKBgQDr088jkvie/hbb+38EKyBUFgj6vSW3l+k2\n+vXsExtlLE7aBnFh0TunMQn9Ru+Hk2a3+i2VlIKyF28FYZPzJvszqLBCCLlod0TYC\n+WG6xEBhYPPg9Gq011NFDNKReWQlCXTnQoGzTA8X9PWsTYIdgoUz8n5dby/QBqjkk\n+8oJckJUmvQKBgB2WPHL2/PfyhvKinC68yW7Fu/Aos+rE9Yq9aueI+tdyqO4Cw4pd\n+hPp8t2Uz3UclVSbyu2urB3yZ2XtPjiszag4kA1J3uvZsesCwfn47MhFAJvKuv6+o\n+ehRGxrEcaRPgNkJ9GeNoqPCTUXR0kmMU+ClG31OvT3H1Z5ZgKywsxVs5AoGBAJMV\n+gQzz/qLTYRiFDFxiNYCcWa9hR2PxmGAu38pD54Te36aJB9ZK3aErHFOy9hoQ9mdb\n+M7tWHKREQpvKKKv/l7i6EPXQyR3tvVzLIQbIPhbSuBUMTVnUyUu8SoQZzh+ZG5UW\n+aalWoibnJqBoRScsk2LAetujIiXjsE5t/oaTGqOVAoGBAKyQKpNU/fGjjyn6WFYe\n+wMlwCV2VZR8HGQQ1vZLxaVvcgNTB0tD0UVAfFHRWW9zs8BYUbItcFJUYJQ4pqDPJ\n+Q0oFngUvBbdHHTaMnp+GndEspUFelXdk8fjzrLaoVqv4mD30JzEmjTx+IVcXNbYr\n+lboVsotpRq5OHAtIRoYPlmg+\n -----END PRIVATE KEY-----\n\n\n\n@@ -1,27 +1,27 @@\n -----BEGIN RSA PRIVATE KEY-----\n-MIIEpQIBAAKCAQEA3e4zQY9qZdF3T3EVxM9rbRSB1j0ekarDXzXphgtc8qlYuHnX\n-blAuWmxH7s9qaqNsb/tpgK/A2wLFJTOEz72UwAQELfMGPX1xl10fbdG4DVFzpRH9\n-ueO7pW9Ps7bJpRsc953BPbTfxM5g19TkQKVyCqUiq1vfv02ZnaFs2+5IofY1LNzi\n-YZQ5TA/d4ybvcuWbJqaZX+jpl+hYlEQsgA7z+e154wrH2MwiMV5D5Nza+Ly6AeJc\n-yWw8um5DdbPB0zdTBwp4ipc21wdnVC6IwmDcDiKDyJXLq+l5mpDHSxM6GbH7Ajmq\n-MScJYlmxJliBi1fU9jhw0I7tMI+uyWZe0p8UyQIDAQABAoIBAAEpWP1T2dO0l/JD\n-fkCscTx96fwMD9flU+Zwqss3zgG7HyrAAH+kTcnNXUcDsOPj/RJM941j3heHWS7L\n-qTtJG7kxjDANYI/70xdetmZlvZfcQEWx5WkOpxHDlmAzabTCgpFNMxC2nKE0D0+x\n-3ov0Ebe6i4boETC4IECx8pboFVs7oyayPwomztss1R4HxBaNYT8qBQnaILZ14tLd\n-hjrdgrsU/oKo6+ojaTjltEn9DeklH726ytAJ5yr7exeXQupIf11nFr3MvDXqgSJJ\n-xQ2JLvPvbOshR+u81Kx/MfpIZzg9bkMoZIVOI9shvDJm10Vrj+0YjMzSKv73V40a\n-9A3ob50CgYEA6J2m5190zLwuExjpPiRSz8ouawHqtsZAZ8E0H1FBzs1NGlFA9F3f\n-JlLvLHH/EXUcVxIb32F9cBBY+/uJ+QOHEpCZ43v0fr5il54T1S1OZyq372wM8PxD\n-4aq+M8f51GtPUwDuglIxQjudr5bBKQcFbAZ6vRfT6Czh4Xj4W6VOAusCgYEA9D2P\n-vYKcD0e3xykdgsucpaMspeE6O1YPeNjGF1dMU8CcMwveS5lIhDjMPzwgSb0R6uHx\n-X62OXYr/lExfJPPzr75/Zt3ETCKPITgKxgNbPBOlppsUYoJN9nq+EgkoshNngYBu\n-eXL93hXflfvI2XEKgFQkxtmYE9dkRePU4+8A0hsCgYEAqGAlRek0lFXKTAyGGHMW\n-dk3014Qlr3NFf1S8M8cPiW/V1aznBXtFYWo6scM6TBqnDueOwVzB/XH31z22G0dQ\n-aEG/Aojg6zVIDFNiK7aFv+uOBIIDxBP11s1OrBCeaAHdA0nzWuW/vh+E/mpzGw14\n-AT+XbHRo8PA7+IclzsCS/U0CgYEA5yMfVcQ19kvzwtnJIM+aJjK+/RihqCewqrfS\n-96HZDOWZrMFNlROxydeKi0lWcRIp07QRXm977FCvwGNqIylu82F/ZhT3QOnvtAs6\n-PtxMShV5zaqzZupT56+PErhyN9wfNzIY2nyVfcgLeP/3ure8rWd0Cm8/+X78WRvc\n-i/zSNQMCgYEAoe/1v5B8wDZ8LhXg5hp6Ifvres80TrnbA7zenwJjC9gdEWh+l2Zg\n-Iy843RbC6PV4kxd2je/mVeNyo4MAvakD5D1IamRaz4ahds4lYlLbi0BLw2HckfRf\n-UbvGa+K7k3ufpogbvdw+/H4RVEYzHco3xlkgqtyYnhjB94BW4M5lmKo=\n+MIIEpAIBAAKCAQEA0ZsXElnV5HMIrR7TR/QZvf2YNoL7lpRDdbycGijvL3W7CI0K\n+XnZYIanFQ3NjpiRNfq4RQLB7XMc0AKeLzy/JYayYQ15/ZXABbUhiBGJ7ByS492nS\n+PZNDDK3i76BKvSbhSLGJz8NwWElKvXh8CSDDteQyTvNpLLUW+cWOSGb8wB4L7VqU\n+RRjIGNP6cYakCMcVzbNfEFunZ1PS9XV1Vjuh8b4pDDRWT/N+ry2Rq05x69eOqX0m\n+YWgyKiCuYRy6jzbYt1WWlNkLt7majJU7nZZgsdnfQ8Yn98ZXwrVPOmc1+JRBTUFg\n+TuZQXHRDaKrzEN4Uh6LfoYr+nY3u1gD/ZNFouwIDAQABAoIBAEgf3L2EyhP0/VxM\n+yhjtRtpYY8MGKTo0RE5EG1bpFPy4vyX/Gj1M7aCPwYAnOXfW8A+SJ/+heqF/Lka3\n+YGVnpR0F/dLYtm+aLfODv66QkJLxsO/5qs1jWeQIBZm9B1q1ATo5VRl3v7WxapVt\n+2whtDMsO/byrB1rJ6WEC+KWzQm98hyX7pBAHoAYDlB+Oj4v8Yvg6108ka+N6FQow\n+G/bZiUfUM9uuPichs1jFYY+s672UMDAVRL5AoQwKjuA2TGwrl0fPp6Mdj3VnqugM\n+GKH1nTPZDWnQYUayNZm5veTUK3ZdRr8z1dHXNYRwnDXfQiHLHmIO2wF4ukHzIit+\n+axgXHBECgYEA/yyOQ2vF55PR4QfJ+7Oe8IZ0MOqOyjgweivEXIT8U7dQQUEJ705J\n+mJp2sxWuw9ZwUI+dLnOJJuHZJlp2aNcaDs8fy9lLh770A0tkC9g0tXr9WBnXqwDz\n+g03Uc4LJr47Zv8ddjqnx/goqAvGaR/GBwkWOTA8RYJOCHVomV2nVT9MCgYEA0kjG\n+evgAIaCtesRDAgHyc3EGRC6inIBW756TNI0G89m3O1t7qYZ2KXvoU5nI05Lg1kWt\n+X9cD71CMVkiuvDgBJC9x6EMMb6y5GZ0KoJkvaVVIMv1pRVaYYt5kSQUGMu9yf6fk\n+39l1D6HVF2Bmrs+zUkdhCwmSicSli92t7CTH2nkCgYEAusp84XG1D04v5tBy1OOp\n+x1xFFcG+AN2c7vG27hsTMp0QDJR1l03xpdn8Zh/LLOR+lxpp7yT4ExRkhyTVZO2A\n+z7JjDi/kkFwB959qfxknjGmZprJUI4Owvj9s85mb1f11/GMOY26YDGSYWEtPqrZp\n+TDLqdVbaW53fM2lYg8ifM60CgYB0mTvM5YSVINXwc7DABTpdfnc+CSnLY3JRDXmM\n+MwJ+5eKuvAw2tMMCP51T9bqgbhnTVfFLahrZcK1QDp8s6FThrpkXc04LAhfktuo0\n+oJvkaRBOjar4coquj6fFwU5toc24MO08ngRFtbNFcgOdvLcu0Tti9lzVnT857aJJ\n+hR936QKBgQD64aFhNH8rQso+iIIuk1vXECp9GuyG30jILvC3S65zCOZVWoMt75Iq\n+FJCeHBg3coWFzmivIINergNWfiRi4Q11flYJ2uMDAbeSKUAPAeq8qGHuQjbNXf00\n+Ej91ObRMXoGZ9qJAK1n94en3+lLkgWLyz/XgtaGmHrlMX/llI65JmQ==\n -----END RSA PRIVATE KEY-----\n\n@@ -1,16 +1,16 @@\n -----BEGIN CERTIFICATE REQUEST-----\n MIICkTCCAXkCAQAwTDELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQH\n Ew1TYW4gRnJhbmNpc2NvMRgwFgYDVQQDEw9EYXRhYmVuZCBDbGllbnQwggEiMA0G\n-CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDd7jNBj2pl0XdPcRXEz2ttFIHWPR6R\n-qsNfNemGC1zyqVi4edduUC5abEfuz2pqo2xv+2mAr8DbAsUlM4TPvZTABAQt8wY9\n-fXGXXR9t0bgNUXOlEf2547ulb0+ztsmlGxz3ncE9tN/EzmDX1ORApXIKpSKrW9+/\n-TZmdoWzb7kih9jUs3OJhlDlMD93jJu9y5Zsmpplf6OmX6FiURCyADvP57XnjCsfY\n-zCIxXkPk3Nr4vLoB4lzJbDy6bkN1s8HTN1MHCniKlzbXB2dULojCYNwOIoPIlcur\n-6XmakMdLEzoZsfsCOaoxJwliWbEmWIGLV9T2OHDQju0wj67JZl7SnxTJAgMBAAGg\n-ADANBgkqhkiG9w0BAQsFAAOCAQEA10/7Atc5g6OAAOo9mdSbUYDLEdYoKo+cu4u3\n-7l80JnbLsbohbiqT8wj4ho8mEU9oXC+7pbB1/+LRidRq0KuOTHhNL2kEuMwYCVxC\n-wJbVAZuaHWnQFCQ43l1LoQAcVKHv0kf3TU1N+rhvU9kz1ek0tu2yUpIUn84YRoLI\n-x+ovYt3Li2U6E/X7NADIa17+V4COJ1kyjc5u+eFcQqS3RpxsJ1IpiUJ2HAPaQccI\n-7jYCFO2LhSZgi+Nlewgmde8mmA665keH2shE3wU06hOaXr0DdlJYGsICMxjpr3nZ\n-GlmOelwijttgyiXFnW88ztuyIEPETpm+aRK0ykbRu7FsYbFn3w==\n+CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDRmxcSWdXkcwitHtNH9Bm9/Zg2gvuW\n+lEN1vJwaKO8vdbsIjQpedlghqcVDc2OmJE1+rhFAsHtcxzQAp4vPL8lhrJhDXn9l\n+cAFtSGIEYnsHJLj3adI9k0MMreLvoEq9JuFIsYnPw3BYSUq9eHwJIMO15DJO82ks\n+tRb5xY5IZvzAHgvtWpRFGMgY0/pxhqQIxxXNs18QW6dnU9L1dXVWO6HxvikMNFZP\n+836vLZGrTnHr146pfSZhaDIqIK5hHLqPNti3VZaU2Qu3uZqMlTudlmCx2d9Dxif3\n+xlfCtU86ZzX4lEFNQWBO5lBcdENoqvMQ3hSHot+hiv6dje7WAP9k0Wi7AgMBAAGg\n+ADANBgkqhkiG9w0BAQsFAAOCAQEAS1p4VSh69WQwba0PzkHjvy6tewGBY00cl/b4\n+YdihFj0/FONbU1dc5yRE3IcaakfaTPL6CxgkmRgmzcxWA6qqDyT1A5pOdetJZQjK\n+OqM8HF+0gF22GyOzffKkTOtPQyACbKKMQH6J7hLsmIlHsf9hzG+rG2z+zL2IDwH4\n+IHqjnFb7UlGG3EkwvtwPCQWIpF44jxXpggPsf4GdbI+kR5stGF5xZmld9maU+mal\n+bLlDfnv4NkSAseLF3B2BBmalkWOYzX/aA42Z/aQiAwx3YlYD7TgKDc4RbzA7hTmJ\n+vUfZdnw9HJbGoc5jX0vFiZ0Nu4emlA+8pHKKUTXjauF4fHJqaw==\n -----END CERTIFICATE REQUEST-----\n\n@@ -1,22 +1,22 @@\n -----BEGIN CERTIFICATE-----\n-MIIDszCCApugAwIBAgIUHXFJaIQlMT1jHL32WbMkPl2noLUwDQYJKoZIhvcNAQEL\n+MIIDszCCApugAwIBAgIUGV4nSX7CAy/YoO77CyZ+dzm2TUAwDQYJKoZIhvcNAQEL\n BQAwZDELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkxBMQswCQYDVQQHEwJDQTEVMBMG\n A1UEChMMRGF0YWZ1c2VsYWJzMREwDwYDVQQLEwhEYXRhYmVuZDERMA8GA1UEAxMI\n-RGF0YWJlbmQwHhcNMjEwOTE1MDMxNjAwWhcNMjIwOTE1MDMxNjAwWjBMMQswCQYD\n+RGF0YWJlbmQwHhcNMjIwOTE1MDUwMzAwWhcNMjMwOTE1MDUwMzAwWjBMMQswCQYD\n VQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGDAW\n BgNVBAMTD0RhdGFiZW5kIENsaWVudDCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC\n-AQoCggEBAN3uM0GPamXRd09xFcTPa20UgdY9HpGqw1816YYLXPKpWLh5125QLlps\n-R+7PamqjbG/7aYCvwNsCxSUzhM+9lMAEBC3zBj19cZddH23RuA1Rc6UR/bnju6Vv\n-T7O2yaUbHPedwT2038TOYNfU5EClcgqlIqtb379NmZ2hbNvuSKH2NSzc4mGUOUwP\n-3eMm73LlmyammV/o6ZfoWJRELIAO8/nteeMKx9jMIjFeQ+Tc2vi8ugHiXMlsPLpu\n-Q3WzwdM3UwcKeIqXNtcHZ1QuiMJg3A4ig8iVy6vpeZqQx0sTOhmx+wI5qjEnCWJZ\n-sSZYgYtX1PY4cNCO7TCPrslmXtKfFMkCAwEAAaN1MHMwDgYDVR0PAQH/BAQDAgWg\n-MBMGA1UdJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYEFC9l\n-GS5diOxSppXfidKfRQZ6Tk6WMB8GA1UdIwQYMBaAFL/9lS5s895y6sXdY5wM5Q72\n-/nnbMA0GCSqGSIb3DQEBCwUAA4IBAQAvVtVZVmRzATiWcIZEt8TgD+fo0nBqqheA\n-TzMvx9sk4vo4RBSfbRVPXfVZUtlw7KP6kLx8xhT432BYnmb4LllHrjVeWk+4VUmp\n-I5IhxedsAazSM0j9ak21YeFt2E5LB0oIfvA5QsL/2smqmVhm5POwxhArYfb3XDak\n-51OUpS+fcgZTEK8Qtq6hAbkWja5pOvNkTfIz49Gn+LRSjhecaik3U9Sx//caAVhn\n-12xbSTghqQ/8q1M4ogEI6qaSLW5SzfsVPBRot1dMqO9ydexKyOxwNUl2MthYF9Bh\n-iz+PWvN3i1mrf7BjbgOcF8rfXcSLP6NEhDMWc7p1s+Lp5cguLc0b\n+AQoCggEBANGbFxJZ1eRzCK0e00f0Gb39mDaC+5aUQ3W8nBoo7y91uwiNCl52WCGp\n+xUNzY6YkTX6uEUCwe1zHNACni88vyWGsmENef2VwAW1IYgRiewckuPdp0j2TQwyt\n+4u+gSr0m4Uixic/DcFhJSr14fAkgw7XkMk7zaSy1FvnFjkhm/MAeC+1alEUYyBjT\n++nGGpAjHFc2zXxBbp2dT0vV1dVY7ofG+KQw0Vk/zfq8tkatOcevXjql9JmFoMiog\n+rmEcuo822LdVlpTZC7e5moyVO52WYLHZ30PGJ/fGV8K1TzpnNfiUQU1BYE7mUFx0\n+Q2iq8xDeFIei36GK/p2N7tYA/2TRaLsCAwEAAaN1MHMwDgYDVR0PAQH/BAQDAgWg\n+MBMGA1UdJQQMMAoGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAwHQYDVR0OBBYEFIlR\n+ctw7oTNqeBJYqpqbXlbciaMnMB8GA1UdIwQYMBaAFJ+A9LMTSCcCxf0h3KvpIx7G\n+c99fMA0GCSqGSIb3DQEBCwUAA4IBAQCzeuCgBDkdQ0KPb1qyThMseCsrvPJPX/JJ\n+SmJVkgStGrIO/HgwlzsTYYfXjCKr7PudsAO1/IufBYfkHaiGpw4xvOFdHBuMwoRo\n+c8ocXe4TW1jopN+HlFMC3OoOymKHH5FSb+xyktCtx/2zLc7wlGm7vwEHKSgkLsjF\n+MhTX6spt7NJxiIwip9AHxEDuP8b6Vq4E9/h3lwoiIXdUKODnd9PMn4duWucAGNDO\n+Cnt3skUuYhB1E5n9QVQDvUZsftSozLmcPsde8OKorDzPu+T9Q8lkoR8c2faHAevd\n+J/5AlPEejkS/FRRlTaEjUxCKqk6avwGjq2f1KDI3p0P+C/fv88CS\n -----END CERTIFICATE-----\n\n@@ -1,28 +1,28 @@\n -----BEGIN PRIVATE KEY-----\n-MIIEvwIBADANBgkqhkiG9w0BAQEFAASCBKkwggSlAgEAAoIBAQDd7jNBj2pl0XdP\n-cRXEz2ttFIHWPR6RqsNfNemGC1zyqVi4edduUC5abEfuz2pqo2xv+2mAr8DbAsUl\n-M4TPvZTABAQt8wY9fXGXXR9t0bgNUXOlEf2547ulb0+ztsmlGxz3ncE9tN/EzmDX\n-1ORApXIKpSKrW9+/TZmdoWzb7kih9jUs3OJhlDlMD93jJu9y5Zsmpplf6OmX6FiU\n-RCyADvP57XnjCsfYzCIxXkPk3Nr4vLoB4lzJbDy6bkN1s8HTN1MHCniKlzbXB2dU\n-LojCYNwOIoPIlcur6XmakMdLEzoZsfsCOaoxJwliWbEmWIGLV9T2OHDQju0wj67J\n-Zl7SnxTJAgMBAAECggEAASlY/VPZ07SX8kN+QKxxPH3p/AwP1+VT5nCqyzfOAbsf\n-KsAAf6RNyc1dRwOw4+P9Ekz3jWPeF4dZLsupO0kbuTGMMA1gj/vTF162ZmW9l9xA\n-RbHlaQ6nEcOWYDNptMKCkU0zELacoTQPT7Hei/QRt7qLhugRMLggQLHylugVWzuj\n-JrI/CibO2yzVHgfEFo1hPyoFCdogtnXi0t2GOt2CuxT+gqjr6iNpOOW0Sf0N6SUf\n-vbrK0AnnKvt7F5dC6kh/XWcWvcy8NeqBIknFDYku8+9s6yFH67zUrH8x+khnOD1u\n-QyhkhU4j2yG8MmbXRWuP7RiMzNIq/vdXjRr0DehvnQKBgQDonabnX3TMvC4TGOk+\n-JFLPyi5rAeq2xkBnwTQfUUHOzU0aUUD0Xd8mUu8scf8RdRxXEhvfYX1wEFj7+4n5\n-A4cSkJnje/R+vmKXnhPVLU5nKrfvbAzw/EPhqr4zx/nUa09TAO6CUjFCO52vlsEp\n-BwVsBnq9F9PoLOHhePhbpU4C6wKBgQD0PY+9gpwPR7fHKR2Cy5yloyyl4To7Vg94\n-2MYXV0xTwJwzC95LmUiEOMw/PCBJvRHq4fFfrY5div+UTF8k8/Ovvn9m3cRMIo8h\n-OArGA1s8E6WmmxRigk32er4SCSiyE2eBgG55cv3eFd+V+8jZcQqAVCTG2ZgT12RF\n-49Tj7wDSGwKBgQCoYCVF6TSUVcpMDIYYcxZ2TfTXhCWvc0V/VLwzxw+Jb9XVrOcF\n-e0VhajqxwzpMGqcO547BXMH9cffXPbYbR1BoQb8CiODrNUgMU2IrtoW/644EggPE\n-E/XWzU6sEJ5oAd0DSfNa5b++H4T+anMbDXgBP5dsdGjw8Dv4hyXOwJL9TQKBgQDn\n-Ix9VxDX2S/PC2ckgz5omMr79GKGoJ7Cqt9L3odkM5ZmswU2VE7HJ14qLSVZxEinT\n-tBFeb3vsUK/AY2ojKW7zYX9mFPdA6e+0Czo+3ExKFXnNqrNm6lPnr48SuHI33B83\n-MhjafJV9yAt4//e6t7ytZ3QKbz/5fvxZG9yL/NI1AwKBgQCh7/W/kHzANnwuFeDm\n-Gnoh++t6zzROudsDvN6fAmML2B0RaH6XZmAjLzjdFsLo9XiTF3aN7+ZV43KjgwC9\n-qQPkPUhqZFrPhqF2ziViUtuLQEvDYdyR9F9Ru8Zr4ruTe5+miBu93D78fhFURjMd\n-yjfGWSCq3JieGMH3gFbgzmWYqg==\n+MIIEvgIBADANBgkqhkiG9w0BAQEFAASCBKgwggSkAgEAAoIBAQDRmxcSWdXkcwit\n+HtNH9Bm9/Zg2gvuWlEN1vJwaKO8vdbsIjQpedlghqcVDc2OmJE1+rhFAsHtcxzQA\n+p4vPL8lhrJhDXn9lcAFtSGIEYnsHJLj3adI9k0MMreLvoEq9JuFIsYnPw3BYSUq9\n+eHwJIMO15DJO82kstRb5xY5IZvzAHgvtWpRFGMgY0/pxhqQIxxXNs18QW6dnU9L1\n+dXVWO6HxvikMNFZP836vLZGrTnHr146pfSZhaDIqIK5hHLqPNti3VZaU2Qu3uZqM\n+lTudlmCx2d9Dxif3xlfCtU86ZzX4lEFNQWBO5lBcdENoqvMQ3hSHot+hiv6dje7W\n+AP9k0Wi7AgMBAAECggEASB/cvYTKE/T9XEzKGO1G2lhjwwYpOjRETkQbVukU/Li/\n+Jf8aPUztoI/BgCc5d9bwD5In/6F6oX8uRrdgZWelHQX90ti2b5ot84O/rpCQkvGw\n+7/mqzWNZ5AgFmb0HWrUBOjlVGXe/tbFqlW3bCG0Myw79vKsHWsnpYQL4pbNCb3yH\n+JfukEAegBgOUH46Pi/xi+DrXTyRr43oVCjAb9tmJR9Qz264+JyGzWMVhj6zrvZQw\n+MBVEvkChDAqO4DZMbCuXR8+nox2PdWeq6AwYofWdM9kNadBhRrI1mbm95NQrdl1G\n+vzPV0dc1hHCcNd9CIcseYg7bAXi6QfMiK35rGBccEQKBgQD/LI5Da8Xnk9HhB8n7\n+s57whnQw6o7KODB6K8RchPxTt1BBQQnvTkmYmnazFa7D1nBQj50uc4km4dkmWnZo\n+1xoOzx/L2UuHvvQDS2QL2DS1ev1YGderAPODTdRzgsmvjtm/x12OqfH+CioC8ZpH\n+8YHCRY5MDxFgk4IdWiZXadVP0wKBgQDSSMZ6+AAhoK16xEMCAfJzcQZELqKcgFbv\n+npM0jQbz2bc7W3uphnYpe+hTmcjTkuDWRa1f1wPvUIxWSK68OAEkL3HoQwxvrLkZ\n+nQqgmS9pVUgy/WlFVphi3mRJBQYy73J/p+Tf2XUPodUXYGauz7NSR2ELCZKJxKWL\n+3a3sJMfaeQKBgQC6ynzhcbUPTi/m0HLU46nHXEUVwb4A3Zzu8bbuGxMynRAMlHWX\n+TfGl2fxmH8ss5H6XGmnvJPgTFGSHJNVk7YDPsmMOL+SQXAH3n2p/GSeMaZmmslQj\n+g7C+P2zzmZvV/XX8Yw5jbpgMZJhYS0+qtmlMMup1Vtpbnd8zaViDyJ8zrQKBgHSZ\n+O8zlhJUg1fBzsMAFOl1+dz4JKctjclENeYwzAn7l4q68DDa0wwI/nVP1uqBuGdNV\n+8UtqGtlwrVAOnyzoVOGumRdzTgsCF+S26jSgm+RpEE6Nqvhyiq6Pp8XBTm2hzbgw\n+7TyeBEW1s0VyA528ty7RO2L2XNWdPzntokmFH3fpAoGBAPrhoWE0fytCyj6Igi6T\n+W9cQKn0a7IbfSMgu8LdLrnMI5lVagy3vkioUkJ4cGDdyhYXOaK8gg16uA1Z+JGLh\n+DXV+Vgna4wMBt5IpQA8B6ryoYe5CNs1d/TQSP3U5tExegZn2okArWf3h6ff6UuSB\n+YvLP9eC1oaYeuUxf+WUjrkmZ\n -----END PRIVATE KEY-----\n\n@@ -1,28 +1,28 @@\n -----BEGIN PRIVATE KEY-----\n-MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQDTaWpJ5eXge60K\n-UbouHmLXVLUsSgU8Ah3korsmUK7wnUAvxfk6yzIcXCljdy98s9u2CgjOMIQp3Hgi\n-JwK9E3nhbTFWBDl8iUApnUyjd8RRG3YhBTLTqWVm8r2SJmN+i4b/v9T5bStzlVnh\n-Z1naW8o71GnkICeCti77ngTT77XIiUzXPD0Xcrf0TSMNR+0ycxb0Ue2RQmS4bibD\n-XjRvGw6v02r2ISGJJfpYA0vUorfOsHp+BO7fEpfvlqJjRhYNriH8JHuIFyaoG0Nd\n-3uFGvsClFH/HvQVoBXvPKzQmcPKNAmkfYL9+AN8C+wBjCwCq7guNfEb/BD52FT0H\n-oRQUku8jAgMBAAECggEAUxj8Tk+vK61xNr0pETegYkNJcz8lD5FFbrkIFudf2/Pz\n-ZtPKQvHboIoZOp9+auXHVqZbD7DCQjIGhmYPDYK6RYFGAyll7y2DqtfJZ/eLktXY\n-eOIyOEA9O1nuaaPtN7BgP+aHMANZMPVjftXA27N4qkuz05dP8HDrxrs16pguO8QE\n-9L2Fh+xqa33yk/m/0JHBEc79n84y+hgCBj4j7XGaQVopUjZA1jV84Lbp+y668iep\n-PeH/UApB/D51hko93s/6AlzRAMwbMRxDuK12k86ZdO6JkyYBswN2Z6rA0VuUU/Jn\n-BAIJQN8iI9CpmSfq64QPC/pgxsaaeHiZzDNaUVkNAQKBgQD0WxYorX/UBc0s4/gu\n-tPUbqm/kLbF3QNX5JVHD4RB0fS9p/aIQXf7LRfQlF02k0Vu29sOhDohEv4eDqg4c\n-lxbKOX5sRNeK3+ifNqqZaLtVmwZYJA5+xppoAbhfM3sxepBu2kaqZXH2ULhLm2JT\n-iYFcF9xkFd9DjLc3KU/75wbc3QKBgQDdfHEwBt5a8t5uMSOpZhpEts02Zk+MZBYm\n-Ugq/n/RD0/zw8nVWXxXSTuu+4z+MJL0eG1ZSFmhvvoYytgp1FIF4psE3WyDNGgMM\n-451kKGB2xUkAgdFM0ldEWQtkurUkIzulxh/+K44+Nl3sBDmmQllXNe2sM3xr0LDX\n-tXsB3s47/wKBgQCdNY0IFrfK/QCIBRNDWYDoj7r1HsCybLA9XtlAIKipiXDbzCZc\n-SV0IArWUt21owJk1GYW7NNJPCLpy5xG1Ky+qY1sG5TOXUX8kD9OqZ0s/REHMq0X4\n-ImZsy86G0WRM7QHHj/8u2Z18T2qCmjGkdS8YxPEyIzBFEV8+ygF/m586sQKBgG3I\n-8j3xGaSRQCn0l42vO5QV3HB7lDCtp8EE56kGe0hnurggJoNSjHl7fyVWdThdOZTD\n-zizQSKIjGVfnnhVcvbR1S87Jj9+6ZgqshGaSEQauUbo7nuhqVrFdReV/Gqy3/if0\n-pgURx+cUgcesbHr55ljxNUcnGiRDBHVX0oohy643AoGAQKex9v2wZK+0eNPsbvIV\n-J+4RHYZPi0Q4TYOkBj84ncd66ZZQqzs9bpvJhLv4sotwTpViEZh5bU3zIqZ5Xd3E\n-+t5CYhAcu6sPldzhuqOzk5rMZMisxHRix6vYsySrTDO+U3u/8Vn+w4tEdzDbovzI\n-1J4Rh5iPOnk2nFxJQxz0Qqo=\n+MIIEvQIBADANBgkqhkiG9w0BAQEFAASCBKcwggSjAgEAAoIBAQCvlX2sFJAzR/Qt\n+bouNcDDQpqT1Z2WH8uWNS7+uTnkA/SwkSeotBxjdZ812ww7Y9F8hJvk+22q6P/NO\n+o07569ccjUWQLrsdVWc27BGGlvlZKR6fkpuHldWAZ6jViCHYhYODzo9WXZUOVN1k\n+mnLERbErY4NXfvxtHalv//LKVDC9Cf/iLDACikzxv6C3Luf8E48coatumC+Yy75u\n+mvsnTYa97KsPwvkkEIwzJuYIEAK3u97qAquvT2yYiPWd7LXF2/BEeddKQ6vE5WVX\n+eTs9oqyefDDuJRi5/pwRK4fupSAFIXsgMbL3S4532NE5R000fV/AVdoQPr7p55a9\n+w9chz+7LAgMBAAECggEAf/WRBaRixoyMncNvPl9ho92eYB2/N4lD02o9wfj8ZEUd\n+M0EOwUvnygsJCKD0pBXLciyl/jIHoL5UQZ3wQ2ClfvBRntb/lEwWEUePKifChkDa\n+SW6dm/0NACwLNbqk99etVZRmj4lZxH0Cb3vDYdhsWnOGV3B648Z3IeFEfkFG16xJ\n+0sMmWpdWu1bt5hFu6GxQ0CTAqyA5y2TYrd58ZEAsYc1HjXXVLM4pKIxekWau9fh6\n+Xen/njZUR11VpBKYnzfUzGT5ioPnmNS5UetNRuyCVzsUBsqIfoUF1Xg3G0RG+S00\n+N7IJMpBSqwywYJg/JBKXjCDoSUVcaiydF8fHTWzDEQKBgQDYWbSgkctynCGPR/Dv\n+Gm8O1f/GR3SirysKiEp6T9zU5iwx8U8+8CzbWfzOBojcHf3iCkNdbDqXou4R+ECp\n+RWmDYRt+A7qsR4u97jl1Yi3w8ko/onMFATrSRzWSh+MCqt7gpQTdQGT4EcHGVp4k\n+zXHhor+rNT/YbI2Oo+oHliTLcwKBgQDPwy+ry9tYUs+EtQnbWTUXfYmVRIzEoc+M\n+hsbMLZa8mxgjxdERmCxyrHFkZJMNrwTiC4VH1vAXdnvE2KZj6fpfGdlZ0dLaZ7MJ\n+b+cmM9KMUtdeFrgyV9/QW49lmERIEGzHguBJ9voc4TaRYKZs9foGv9fBXCcWC9ek\n+b0NyEjGpSQKBgHxG21k5JpLJWQXDH4VZu93b07cEKN2inajJLW8hHQs2xMBTi1/t\n++vZttKhsXIWEvOfxWUz69JVSREsSPaJ+euO7Dl3OqpZ36q/JfjwNj7JOm5P+Lu/7\n+CtzrhryLTj2kBa8pW6f0a4uMN7FE5EbjtL0WDd+9ENWzVYVQZwjAmJY9AoGAR6h+\n+rLZnNRN6177JY4plMXDvHzjBhKB6wXFK5+lhcDPeI5h6/6KI0hSAF6xIvBN9j6mB\n+ksGjlbQcBEzlNtjgriRwPvaLzyCtM7gQ9bAyEowQFDep2FNOssKhttzJ0EE6A6Y0\n+Tc6NfaB//axkOnisf99zPxHVuVjkS6bEkHeOAWECgYEAsFy+E2NJRkVHIpESHSDe\n+a9Uy5Pb+Ms1NsGkrRdZ7OYrj2XuSt6J0/3seBz9XDJ8oKHAwz8J6rVixw9mTINWU\n+fBSwn7yNXe8O43vo/w9sm6sNxNHOfvH77N33yq6t1FR3PuyDz387cOle3mNPkzgA\n+T47mFvqmSgH7dRBd8/+aCu8=\n -----END PRIVATE KEY-----\n\n@@ -1,27 +1,27 @@\n -----BEGIN RSA PRIVATE KEY-----\n-MIIEowIBAAKCAQEA02lqSeXl4HutClG6Lh5i11S1LEoFPAId5KK7JlCu8J1AL8X5\n-OssyHFwpY3cvfLPbtgoIzjCEKdx4IicCvRN54W0xVgQ5fIlAKZ1Mo3fEURt2IQUy\n-06llZvK9kiZjfouG/7/U+W0rc5VZ4WdZ2lvKO9Rp5CAngrYu+54E0++1yIlM1zw9\n-F3K39E0jDUftMnMW9FHtkUJkuG4mw140bxsOr9Nq9iEhiSX6WANL1KK3zrB6fgTu\n-3xKX75aiY0YWDa4h/CR7iBcmqBtDXd7hRr7ApRR/x70FaAV7zys0JnDyjQJpH2C/\n-fgDfAvsAYwsAqu4LjXxG/wQ+dhU9B6EUFJLvIwIDAQABAoIBAFMY/E5PryutcTa9\n-KRE3oGJDSXM/JQ+RRW65CBbnX9vz82bTykLx26CKGTqffmrlx1amWw+wwkIyBoZm\n-Dw2CukWBRgMpZe8tg6rXyWf3i5LV2HjiMjhAPTtZ7mmj7TewYD/mhzADWTD1Y37V\n-wNuzeKpLs9OXT/Bw68a7NeqYLjvEBPS9hYfsamt98pP5v9CRwRHO/Z/OMvoYAgY+\n-I+1xmkFaKVI2QNY1fOC26fsuuvInqT3h/1AKQfw+dYZKPd7P+gJc0QDMGzEcQ7it\n-dpPOmXTuiZMmAbMDdmeqwNFblFPyZwQCCUDfIiPQqZkn6uuEDwv6YMbGmnh4mcwz\n-WlFZDQECgYEA9FsWKK1/1AXNLOP4LrT1G6pv5C2xd0DV+SVRw+EQdH0vaf2iEF3+\n-y0X0JRdNpNFbtvbDoQ6IRL+Hg6oOHJcWyjl+bETXit/onzaqmWi7VZsGWCQOfsaa\n-aAG4XzN7MXqQbtpGqmVx9lC4S5tiU4mBXBfcZBXfQ4y3NylP++cG3N0CgYEA3Xxx\n-MAbeWvLebjEjqWYaRLbNNmZPjGQWJlIKv5/0Q9P88PJ1Vl8V0k7rvuM/jCS9HhtW\n-UhZob76GMrYKdRSBeKbBN1sgzRoDDOOdZChgdsVJAIHRTNJXRFkLZLq1JCM7pcYf\n-/iuOPjZd7AQ5pkJZVzXtrDN8a9Cw17V7Ad7OO/8CgYEAnTWNCBa3yv0AiAUTQ1mA\n-6I+69R7AsmywPV7ZQCCoqYlw28wmXEldCAK1lLdtaMCZNRmFuzTSTwi6cucRtSsv\n-qmNbBuUzl1F/JA/TqmdLP0RBzKtF+CJmbMvOhtFkTO0Bx4//LtmdfE9qgpoxpHUv\n-GMTxMiMwRRFfPsoBf5ufOrECgYBtyPI98RmkkUAp9JeNrzuUFdxwe5QwrafBBOep\n-BntIZ7q4ICaDUox5e38lVnU4XTmUw84s0EiiIxlX554VXL20dUvOyY/fumYKrIRm\n-khEGrlG6O57oalaxXUXlfxqst/4n9KYFEcfnFIHHrGx6+eZY8TVHJxokQwR1V9KK\n-IcuuNwKBgECnsfb9sGSvtHjT7G7yFSfuER2GT4tEOE2DpAY/OJ3HeumWUKs7PW6b\n-yYS7+LKLcE6VYhGYeW1N8yKmeV3dxPreQmIQHLurD5Xc4bqjs5OazGTIrMR0Yser\n-2LMkq0wzvlN7v/FZ/sOLRHcw26L8yNSeEYeYjzp5NpxcSUMc9EKq\n+MIIEowIBAAKCAQEAr5V9rBSQM0f0LW6LjXAw0Kak9Wdlh/LljUu/rk55AP0sJEnq\n+LQcY3WfNdsMO2PRfISb5Pttquj/zTqNO+evXHI1FkC67HVVnNuwRhpb5WSken5Kb\n+h5XVgGeo1Ygh2IWDg86PVl2VDlTdZJpyxEWxK2ODV378bR2pb//yylQwvQn/4iww\n+AopM8b+gty7n/BOPHKGrbpgvmMu+bpr7J02GveyrD8L5JBCMMybmCBACt7ve6gKr\n+r09smIj1ney1xdvwRHnXSkOrxOVlV3k7PaKsnnww7iUYuf6cESuH7qUgBSF7IDGy\n+90uOd9jROUdNNH1fwFXaED6+6eeWvcPXIc/uywIDAQABAoIBAH/1kQWkYsaMjJ3D\n+bz5fYaPdnmAdvzeJQ9NqPcH4/GRFHTNBDsFL58oLCQig9KQVy3Ispf4yB6C+VEGd\n+8ENgpX7wUZ7W/5RMFhFHjyonwoZA2klunZv9DQAsCzW6pPfXrVWUZo+JWcR9Am97\n+w2HYbFpzhldweuPGdyHhRH5BRtesSdLDJlqXVrtW7eYRbuhsUNAkwKsgOctk2K3e\n+fGRALGHNR4111SzOKSiMXpFmrvX4el3p/542VEddVaQSmJ831Mxk+YqD55jUuVHr\n+TUbsglc7FAbKiH6FBdV4NxtERvktNDeyCTKQUqsMsGCYPyQSl4wg6ElFXGosnRfH\n+x01swxECgYEA2Fm0oJHLcpwhj0fw7xpvDtX/xkd0oq8rCohKek/c1OYsMfFPPvAs\n+21n8zgaI3B394gpDXWw6l6LuEfhAqUVpg2EbfgO6rEeLve45dWIt8PJKP6JzBQE6\n+0kc1kofjAqre4KUE3UBk+BHBxlaeJM1x4aK/qzU/2GyNjqPqB5Yky3MCgYEAz8Mv\n+q8vbWFLPhLUJ21k1F32JlUSMxKHPjIbGzC2WvJsYI8XREZgscqxxZGSTDa8E4guF\n+R9bwF3Z7xNimY+n6XxnZWdHS2mezCW/nJjPSjFLXXha4Mlff0FuPZZhESBBsx4Lg\n+Sfb6HOE2kWCmbPX6Br/XwVwnFgvXpG9DchIxqUkCgYB8RttZOSaSyVkFwx+FWbvd\n+29O3BCjdop2oyS1vIR0LNsTAU4tf7fr2bbSobFyFhLzn8VlM+vSVUkRLEj2ifnrj\n+uw5dzqqWd+qvyX48DY+yTpuT/i7v+wrc64a8i049pAWvKVun9GuLjDexRORG47S9\n+Fg3fvRDVs1WFUGcIwJiWPQKBgEeofqy2ZzUTete+yWOKZTFw7x84wYSgesFxSufp\n+YXAz3iOYev+iiNIUgBesSLwTfY+pgZLBo5W0HARM5TbY4K4kcD72i88grTO4EPWw\n+MhKMEBQ3qdhTTrLCobbcydBBOgOmNE3OjX2gf/2sZDp4rH/fcz8R1blY5EumxJB3\n+jgFhAoGBALBcvhNjSUZFRyKREh0g3mvVMuT2/jLNTbBpK0XWezmK49l7kreidP97\n+Hgc/VwyfKChwMM/Ceq1YscPZkyDVlHwUsJ+8jV3vDuN76P8PbJurDcTRzn7x++zd\n+98qurdRUdz7sg89/O3DpXt5jT5M4AE+O5hb6pkoB+3UQXfP/mgrv\n -----END RSA PRIVATE KEY-----\n\n@@ -1,18 +1,18 @@\n -----BEGIN CERTIFICATE REQUEST-----\n MIIC1jCCAb4CAQAwTDELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkNBMRYwFAYDVQQH\n Ew1TYW4gRnJhbmNpc2NvMRgwFgYDVQQDEw9EYXRhYmVuZCBTZXJ2ZXIwggEiMA0G\n-CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQDTaWpJ5eXge60KUbouHmLXVLUsSgU8\n-Ah3korsmUK7wnUAvxfk6yzIcXCljdy98s9u2CgjOMIQp3HgiJwK9E3nhbTFWBDl8\n-iUApnUyjd8RRG3YhBTLTqWVm8r2SJmN+i4b/v9T5bStzlVnhZ1naW8o71GnkICeC\n-ti77ngTT77XIiUzXPD0Xcrf0TSMNR+0ycxb0Ue2RQmS4bibDXjRvGw6v02r2ISGJ\n-JfpYA0vUorfOsHp+BO7fEpfvlqJjRhYNriH8JHuIFyaoG0Nd3uFGvsClFH/HvQVo\n-BXvPKzQmcPKNAmkfYL9+AN8C+wBjCwCq7guNfEb/BD52FT0HoRQUku8jAgMBAAGg\n+CSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCvlX2sFJAzR/QtbouNcDDQpqT1Z2WH\n+8uWNS7+uTnkA/SwkSeotBxjdZ812ww7Y9F8hJvk+22q6P/NOo07569ccjUWQLrsd\n+VWc27BGGlvlZKR6fkpuHldWAZ6jViCHYhYODzo9WXZUOVN1kmnLERbErY4NXfvxt\n+Halv//LKVDC9Cf/iLDACikzxv6C3Luf8E48coatumC+Yy75umvsnTYa97KsPwvkk\n+EIwzJuYIEAK3u97qAquvT2yYiPWd7LXF2/BEeddKQ6vE5WVXeTs9oqyefDDuJRi5\n+/pwRK4fupSAFIXsgMbL3S4532NE5R000fV/AVdoQPr7p55a9w9chz+7LAgMBAAGg\n RTBDBgkqhkiG9w0BCQ4xNjA0MDIGA1UdEQQrMCmCCWxvY2FsaG9zdIcEfwAAAYcE\n-AAAAAIcQAAAAAAAAAAAAAAAAAAAAATANBgkqhkiG9w0BAQsFAAOCAQEAqEn8UOdB\n-tb+192MPIOsygzmg1C4eMXia0VSKSgJbXzY3DOnnf9JALivxUinh+QJ8VtkECUE5\n-6V4fWSVJqXclqfdVdczocn63SwIeFOZ24ECzZoZix2g5Xhf+GQaXJsbXnS6kjoFy\n-riz8FsSBtYdK+cxmupfbRZO5n6+0kESI9eP6c6vowggqPmAO58lVbD2esL38rUn5\n-vgIBgHoTtir/dCq2qtWcMxGIr2Eq+i2caAFtxmroj99rdfTnrivt4KItL3GuuBxo\n-tzVotvrYNk4ncbwfUNwB52Ky4sCAevdIvNGqEzNg8j+yhJybm2H58gU7S5W4eQX0\n-gCw9ILKceRdqJg==\n+AAAAAIcQAAAAAAAAAAAAAAAAAAAAATANBgkqhkiG9w0BAQsFAAOCAQEAM86KQs0h\n+9MBP6fiOHjAWFBQMXT4JRinN3jH2asf3J8IxaPd4WwkltzpiOWYfwQ4boCdEFfpG\n+IIxdYMXrJnCoE62yQ4PwbEIMaq3W/ZaZWLOpqpjRTwNq6QxTp2JBSWotarg/OnxC\n+/3rL5tJvhsHk4BjVTCcJWNtxGIa2ooDnPa1uO1ppEOsP7Tb9yur8xl/Li0kuA2Tn\n+VlAKHUC0QuVWrNpbasRU9B+whuHeKey0KOEnGTEF97K4wDiEb4uXqrLU9M3kHw7x\n+eRKT6g4C8znly9LCGuSnBLiDJ+uFQirb94aYRs1S8EfZTxK3XNu9d43fFALuuG1y\n+YKINyhxQVmb2gA==\n -----END CERTIFICATE REQUEST-----\n\n@@ -1,24 +1,24 @@\n -----BEGIN CERTIFICATE-----\n-MIID8zCCAtugAwIBAgIUQAh/QNWvc8T7TzUcQAYy5KSxl54wDQYJKoZIhvcNAQEL\n+MIID8zCCAtugAwIBAgIUbDV37W1vSb6APZbOANDSliFoupwwDQYJKoZIhvcNAQEL\n BQAwZDELMAkGA1UEBhMCVVMxCzAJBgNVBAgTAkxBMQswCQYDVQQHEwJDQTEVMBMG\n A1UEChMMRGF0YWZ1c2VsYWJzMREwDwYDVQQLEwhEYXRhYmVuZDERMA8GA1UEAxMI\n-RGF0YWJlbmQwHhcNMjEwOTE1MDMxNjAwWhcNMjIwOTE1MDMxNjAwWjBMMQswCQYD\n+RGF0YWJlbmQwHhcNMjIwOTE1MDUwMzAwWhcNMjMwOTE1MDUwMzAwWjBMMQswCQYD\n VQQGEwJVUzELMAkGA1UECBMCQ0ExFjAUBgNVBAcTDVNhbiBGcmFuY2lzY28xGDAW\n BgNVBAMTD0RhdGFiZW5kIFNlcnZlcjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCC\n-AQoCggEBANNpaknl5eB7rQpRui4eYtdUtSxKBTwCHeSiuyZQrvCdQC/F+TrLMhxc\n-KWN3L3yz27YKCM4whCnceCInAr0TeeFtMVYEOXyJQCmdTKN3xFEbdiEFMtOpZWby\n-vZImY36Lhv+/1PltK3OVWeFnWdpbyjvUaeQgJ4K2LvueBNPvtciJTNc8PRdyt/RN\n-Iw1H7TJzFvRR7ZFCZLhuJsNeNG8bDq/TavYhIYkl+lgDS9Sit86wen4E7t8Sl++W\n-omNGFg2uIfwke4gXJqgbQ13e4Ua+wKUUf8e9BWgFe88rNCZw8o0CaR9gv34A3wL7\n-AGMLAKruC418Rv8EPnYVPQehFBSS7yMCAwEAAaOBtDCBsTAOBgNVHQ8BAf8EBAMC\n+AQoCggEBAK+VfawUkDNH9C1ui41wMNCmpPVnZYfy5Y1Lv65OeQD9LCRJ6i0HGN1n\n+zXbDDtj0XyEm+T7baro/806jTvnr1xyNRZAuux1VZzbsEYaW+VkpHp+Sm4eV1YBn\n+qNWIIdiFg4POj1ZdlQ5U3WSacsRFsStjg1d+/G0dqW//8spUML0J/+IsMAKKTPG/\n+oLcu5/wTjxyhq26YL5jLvm6a+ydNhr3sqw/C+SQQjDMm5ggQAre73uoCq69PbJiI\n+9Z3stcXb8ER510pDq8TlZVd5Oz2irJ58MO4lGLn+nBErh+6lIAUheyAxsvdLjnfY\n+0TlHTTR9X8BV2hA+vunnlr3D1yHP7ssCAwEAAaOBtDCBsTAOBgNVHQ8BAf8EBAMC\n BaAwHQYDVR0lBBYwFAYIKwYBBQUHAwEGCCsGAQUFBwMCMAwGA1UdEwEB/wQCMAAw\n-HQYDVR0OBBYEFFZEmnGyOJKNdLptXg6gm5VQ3g+LMB8GA1UdIwQYMBaAFL/9lS5s\n-895y6sXdY5wM5Q72/nnbMDIGA1UdEQQrMCmCCWxvY2FsaG9zdIcEfwAAAYcEAAAA\n-AIcQAAAAAAAAAAAAAAAAAAAAATANBgkqhkiG9w0BAQsFAAOCAQEArwzAl0Ob2E5A\n-/9AHpEzqXJWGCD//mGxZThNg8tjGF2VP0yYMr3sS6UJoMvgXudzklXshLoihjber\n-3+qcJe8gggQBzPTiDB/Y/bL7yjG1NMMd8zPdhtGvMMirigoV6+6Bx3AoBYgK4pY5\n-pMfXNBUb9Sda7sn1zsEexZg1qpqXywGpe12sdsJAynpQbIuvKWjckxr+dN2r+fYe\n-DicPNJcv2zu3LMCVpQAkARE1YPG+QPOP6vQIbew02W/T4oYH4suldLPRji+53ISG\n-8hOujHQTuVwXYngmIyRVtN2qjSXwMS1lbE0Qh3SVV5WvmSaVPadmsdbUlMeRGoFF\n-8VhklIORGQ==\n+HQYDVR0OBBYEFJSCgUqPhxGvJWMWhqo5xRUdMOQGMB8GA1UdIwQYMBaAFJ+A9LMT\n+SCcCxf0h3KvpIx7Gc99fMDIGA1UdEQQrMCmCCWxvY2FsaG9zdIcEfwAAAYcEAAAA\n+AIcQAAAAAAAAAAAAAAAAAAAAATANBgkqhkiG9w0BAQsFAAOCAQEAgxW/IaAbyJ1+\n+UGfHhE7X9OdhHCVKCgyGgmDkAFi58I2hUebBEkdn6DprL7q4wOUplCMtvXYBQLNi\n+EhKNQpRbrc4YwU3V+2CkRXxktvv3mbg5BxzC+YqlVp8I2Vx2psHy40HS9kGW7HCR\n+0649sjvGtfX0Rt21nDZMHq6n32VD6hRQGR3PfkQkQXFYiiC1DaOWFGl1uiN6y2R/\n+71xTS1ls2Qouty9l81OC7ZTiYL1M2eUqBjlLp8f1155i8IaFSGaapcIjJ33NBVEg\n+kPCs1ELPTDZ3hQyYkyyHxjaQrdhNygCndTFW2EXh1nEPvyIjnkdG8ku00J53UmlJ\n+iNwGjyzZPg==\n -----END CERTIFICATE-----\n"}
{"id": 38, "ground_truth": "I/O", "predicted": "Environment", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe `text\\split-batch.yaml` fails with 25 pixel difference, and the `image\\tile-repeat-prim-or-decompose.yaml` crashes, because we run out of descriptors. The fail was introduced in https://github.com/szeged/webrender/commit/1e81eaf8fa8ddbed1f8beda20a18bfc8787c7e80, where we started using the `max_image_array_layers` limit from gfx-hal. Previously we had a constant `2048` value there. This only affects the Vulkan backend. \n\n### Code Patch:\n@@ -783,6 +783,7 @@ impl hal::PhysicalDevice<Backend> for PhysicalDevice {\n             max_image_2d_size: limits.max_image_dimension2_d,\n             max_image_3d_size: limits.max_image_dimension3_d,\n             max_image_cube_size: limits.max_image_dimension_cube,\n+            max_image_array_layers: limits.max_image_array_layers as _,\n             max_texel_elements: limits.max_texel_buffer_elements as _,\n             max_patch_size: limits.max_tessellation_patch_size as PatchSize,\n             max_viewports: limits.max_viewports as _,\n"}
{"id": 71, "ground_truth": "Time", "predicted": "Time", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Summary**\r\n\r\nhttps://github.com/datafuselabs/databend/actions/runs/8168665641/job/22331179267?pr=14855\r\n\r\n```\r\n   FAIL [   0.257s] databend-meta::it grpc::metasrv_grpc_kv_read_v1::test_kv_read_v1_on_follower\r\n```\r\n\r\n```\r\n                             expire_at: Some(\r\n<                                1709712582,\r\n>                                1709712583,\r\n                             ),\r\n\r\n```\n\n### Code Patch:\n@@ -153,21 +153,36 @@ async fn test_streamed_mget(client: &Arc<ClientHandle>, now_sec: u64) -> anyhow:\n         }))\n         .await?;\n \n-    let got = strm.map_err(|e| e.to_string()).collect::<Vec<_>>().await;\n-    assert_eq!(\n-        vec![\n-            Ok(pb::StreamItem::new(\n-                s(\"a\"),\n-                Some(pb::SeqV::with_meta(\n-                    1,\n-                    Some(KvMeta::new_expire(now_sec + 10)),\n-                    b(\"a\")\n-                ))\n-            )),\n-            Ok(pb::StreamItem::new(s(\"b\"), None)),\n-        ],\n-        got\n-    );\n+    let mut got = strm.map_err(|e| e.to_string()).collect::<Vec<_>>().await;\n+    assert_eq!(2, got.len());\n+\n+    let v1 = got.remove(0);\n+    let v2 = got.remove(0);\n+\n+    // check v1\n+    {\n+        let Ok(pb::StreamItem {\n+            key,\n+            value: Some(seq_v),\n+        }) = v1\n+        else {\n+            panic!(\"expecting Some(seq_v): but: {v1:?}\");\n+        };\n+\n+        assert_eq!(s(\"a\"), key);\n+        assert_eq!(1, seq_v.seq);\n+        assert_eq!(b(\"a\"), seq_v.data);\n+        // check meta\n+        {\n+            let KvMeta { expire_at } = seq_v.meta.unwrap();\n+            let want = now_sec + 10;\n+            assert!((want..want + 3).contains(&expire_at.unwrap()));\n+        }\n+    }\n+\n+    // check v2\n+    assert_eq!(v2, Ok(pb::StreamItem::new(s(\"b\"), None)));\n+\n     Ok(())\n }\n \n"}
{"id": 48, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nIt appears that after version 0.32, executing the GraphQL query `estimateGasPrice(10)` immediately after starting fuel-core will often throw the error below, making tests very flaky on [`fuels-ts`](https://github.com/FuelLabs/fuels-ts).\r\n\r\n```console\r\nFailed to estimate gas price for block, algorithm not yet set: 10\r\n```\r\n\r\n\r\n\r\nQuoting suspicions below:\r\n\r\n> @MitchTurner: I believe this is a race condition where the graphql services task is started before the gas price service is fully initialized. Each is run in a separate thread and we don't have any extra synchronization to make sure one is started before the other.\r\n\r\n> @Voxelot: hmm we need the graphql service up and running early otherwise the service may look unhealthy if other tasks take a while to initialize. We could split the graphql request handling from the health endpoint so they could be initialized at different times? (edited) \r\n\n\n### Code Patch:\n@@ -6,6 +6,9 @@ and this project adheres to [Semantic Versioning](http://semver.org/).\n \n ## [Unreleased]\n \n+### Changed\n+- [2113](https://github.com/FuelLabs/fuel-core/pull/2113): Modify the way the gas price service and shared algo is initialized to have some default value based on best guess instead of `None`, and initialize service before graphql.\n+\n ## [Version 0.34.0]\n \n ### Added\n\n@@ -6,7 +6,6 @@ use fuel_core_gas_price_service::{\n use fuel_core_producer::block_producer::gas_price::GasPriceProvider as ProducerGasPriceProvider;\n use fuel_core_txpool::{\n     ports::GasPriceProvider as TxPoolGasPriceProvider,\n-    Error as TxPoolError,\n     Result as TxPoolResult,\n };\n use fuel_core_types::fuel_types::BlockHeight;\n@@ -54,7 +53,7 @@ impl<A> FuelGasPriceProvider<A>\n where\n     A: GasPriceAlgorithm + Send + Sync,\n {\n-    async fn next_gas_price(&self) -> Option<u64> {\n+    async fn next_gas_price(&self) -> u64 {\n         self.algorithm.next_gas_price().await\n     }\n }\n@@ -65,9 +64,7 @@ where\n     A: GasPriceAlgorithm + Send + Sync,\n {\n     async fn next_gas_price(&self) -> anyhow::Result<u64> {\n-        self.next_gas_price()\n-            .await\n-            .ok_or(anyhow::anyhow!(\"No gas price available\"))\n+        Ok(self.next_gas_price().await)\n     }\n }\n \n@@ -77,11 +74,7 @@ where\n     A: GasPriceAlgorithm + Send + Sync,\n {\n     async fn next_gas_price(&self) -> TxPoolResult<u64> {\n-        self.next_gas_price()\n-            .await\n-            .ok_or(TxPoolError::GasPriceNotFound(\n-                \"Gas price not set yet\".to_string(),\n-            ))\n+        Ok(self.next_gas_price().await)\n     }\n }\n \n@@ -91,6 +84,6 @@ where\n     A: GasPriceAlgorithm + Send + Sync,\n {\n     async fn worst_case_gas_price(&self, height: BlockHeight) -> Option<u64> {\n-        self.algorithm.worst_case_gas_price(height).await\n+        Some(self.algorithm.worst_case_gas_price(height).await)\n     }\n }\n\n@@ -13,7 +13,7 @@ async fn gas_price__if_requested_block_height_is_latest_return_gas_price() {\n \n     // when\n     let expected_price = algo.next_gas_price();\n-    let actual_price = gas_price_provider.next_gas_price().await.unwrap();\n+    let actual_price = gas_price_provider.next_gas_price().await;\n \n     // then\n     assert_eq!(expected_price, actual_price);\n\n@@ -13,7 +13,7 @@ async fn gas_price__if_requested_block_height_is_latest_return_gas_price() {\n \n     // when\n     let expected_price = algo.next_gas_price();\n-    let actual_price = gas_price_provider.next_gas_price().await.unwrap();\n+    let actual_price = gas_price_provider.next_gas_price().await;\n \n     // then\n     assert_eq!(expected_price, actual_price);\n\n@@ -332,11 +332,9 @@ pub fn init_sub_services(\n     #[allow(unused_mut)]\n     // `FuelService` starts and shutdowns all sub-services in the `services` order\n     let mut services: SubServices = vec![\n-        // GraphQL should be shutdown first, so let's start it first.\n-        Box::new(graph_ql),\n+        Box::new(gas_price_service),\n         Box::new(txpool),\n         Box::new(consensus_parameters_provider_service),\n-        Box::new(gas_price_service),\n     ];\n \n     if let Some(poa) = poa {\n@@ -356,6 +354,7 @@ pub fn init_sub_services(\n         }\n     }\n \n+    services.push(Box::new(graph_ql));\n     services.push(Box::new(graphql_worker));\n \n     Ok((services, shared))\n\n@@ -16,6 +16,7 @@ use crate::{\n use fuel_core_gas_price_service::{\n     fuel_gas_price_updater::{\n         fuel_core_storage_adapter::{\n+            storage::GasPriceMetadata,\n             FuelL2BlockSource,\n             GasPriceSettingsProvider,\n         },\n@@ -88,7 +89,13 @@ impl InitializeTask {\n         gas_price_db: Database<GasPriceDatabase, RegularStage<GasPriceDatabase>>,\n         on_chain_db: Database<OnChain, RegularStage<OnChain>>,\n     ) -> anyhow::Result<Self> {\n-        let shared_algo = SharedGasPriceAlgo::new();\n+        let latest_block_height = on_chain_db\n+            .latest_height()\n+            .unwrap_or(genesis_block_height)\n+            .into();\n+        let default_metadata = get_default_metadata(&config, latest_block_height);\n+        let algo = get_best_algo(&gas_price_db, default_metadata)?;\n+        let shared_algo = SharedGasPriceAlgo::new_with_algorithm(algo);\n         let task = Self {\n             config,\n             genesis_block_height,\n@@ -102,6 +109,34 @@ impl InitializeTask {\n     }\n }\n \n+fn get_default_metadata(config: &Config, latest_block_height: u32) -> UpdaterMetadata {\n+    UpdaterMetadata::V0(V0Metadata {\n+        new_exec_price: config.starting_gas_price.max(config.min_gas_price),\n+        min_exec_gas_price: config.min_gas_price,\n+        exec_gas_price_change_percent: config.gas_price_change_percent,\n+        l2_block_height: latest_block_height,\n+        l2_block_fullness_threshold_percent: config.gas_price_threshold_percent,\n+    })\n+}\n+\n+fn get_best_algo(\n+    gas_price_db: &Database<GasPriceDatabase, RegularStage<GasPriceDatabase>>,\n+    default_metadata: UpdaterMetadata,\n+) -> anyhow::Result<Algorithm> {\n+    let best_metadata: UpdaterMetadata =\n+        if let Some(height) = gas_price_db.latest_height() {\n+            gas_price_db\n+                .storage::<GasPriceMetadata>()\n+                .get(&height)?\n+                .map(|m| m.into_owned())\n+                .unwrap_or(default_metadata)\n+        } else {\n+            default_metadata\n+        };\n+    let updater: AlgorithmUpdater = best_metadata.into();\n+    let algo = updater.algorithm();\n+    Ok(algo)\n+}\n #[async_trait::async_trait]\n impl RunnableService for InitializeTask {\n     const NAME: &'static str = \"GasPriceUpdater\";\n@@ -124,7 +159,7 @@ impl RunnableService for InitializeTask {\n             .unwrap_or(self.genesis_block_height);\n \n         let updater = get_synced_gas_price_updater(\n-            &self.config,\n+            self.config,\n             self.genesis_block_height,\n             self.settings,\n             self.gas_price_db,\n@@ -138,7 +173,7 @@ impl RunnableService for InitializeTask {\n }\n \n pub fn get_synced_gas_price_updater(\n-    config: &Config,\n+    config: Config,\n     genesis_block_height: BlockHeight,\n     settings: ConsensusParametersProvider,\n     mut gas_price_db: Database<GasPriceDatabase, RegularStage<GasPriceDatabase>>,\n@@ -158,14 +193,7 @@ pub fn get_synced_gas_price_updater(\n         first_run = true;\n         latest_block_height\n     };\n-\n-    let first_metadata = UpdaterMetadata::V0(V0Metadata {\n-        new_exec_price: config.starting_gas_price.max(config.min_gas_price),\n-        min_exec_gas_price: config.min_gas_price,\n-        exec_gas_price_change_percent: config.gas_price_change_percent,\n-        l2_block_height: latest_block_height,\n-        l2_block_fullness_threshold_percent: config.gas_price_threshold_percent,\n-    });\n+    let default_metadata = get_default_metadata(&config, latest_block_height);\n \n     if metadata_height > latest_block_height {\n         revert_gas_price_db_to_height(&mut gas_price_db, latest_block_height.into())?;\n@@ -183,7 +211,7 @@ pub fn get_synced_gas_price_updater(\n \n     if BlockHeight::from(latest_block_height) == genesis_block_height || first_run {\n         let updater = FuelGasPriceUpdater::new(\n-            first_metadata.into(),\n+            default_metadata.into(),\n             l2_block_source,\n             metadata_storage,\n         );\n\n@@ -79,7 +79,7 @@ where\n }\n \n #[derive(Debug, Default)]\n-pub struct SharedGasPriceAlgo<A>(Arc<RwLock<Option<A>>>);\n+pub struct SharedGasPriceAlgo<A>(Arc<RwLock<A>>);\n \n impl<A> Clone for SharedGasPriceAlgo<A> {\n     fn clone(&self) -> Self {\n@@ -91,34 +91,26 @@ impl<A> SharedGasPriceAlgo<A>\n where\n     A: Send + Sync,\n {\n-    pub fn new() -> Self {\n-        Self(Arc::new(RwLock::new(None)))\n-    }\n-\n     pub fn new_with_algorithm(algorithm: A) -> Self {\n-        Self(Arc::new(RwLock::new(Some(algorithm))))\n+        Self(Arc::new(RwLock::new(algorithm)))\n     }\n \n     pub async fn update(&mut self, new_algo: A) {\n         let mut write_lock = self.0.write().await;\n-        *write_lock = Some(new_algo);\n+        *write_lock = new_algo;\n     }\n }\n \n impl<A> SharedGasPriceAlgo<A>\n where\n     A: GasPriceAlgorithm + Send + Sync,\n {\n-    pub async fn next_gas_price(&self) -> Option<u64> {\n-        self.0.read().await.as_ref().map(|a| a.next_gas_price())\n+    pub async fn next_gas_price(&self) -> u64 {\n+        self.0.read().await.next_gas_price()\n     }\n \n-    pub async fn worst_case_gas_price(&self, block_height: BlockHeight) -> Option<u64> {\n-        self.0\n-            .read()\n-            .await\n-            .as_ref()\n-            .map(|a| a.worst_case_gas_price(block_height))\n+    pub async fn worst_case_gas_price(&self, block_height: BlockHeight) -> u64 {\n+        self.0.read().await.worst_case_gas_price(block_height)\n     }\n }\n \n@@ -236,16 +228,19 @@ mod tests {\n     async fn run__updates_gas_price() {\n         // given\n         let (price_sender, price_receiver) = mpsc::channel(1);\n+        let start_algo = TestAlgorithm { price: 50 };\n+        let expected_price = 100;\n         let updater = TestAlgorithmUpdater {\n-            start: TestAlgorithm { price: 0 },\n+            start: TestAlgorithm {\n+                price: expected_price,\n+            },\n             price_source: price_receiver,\n         };\n-        let shared_algo = SharedGasPriceAlgo::new();\n+        let shared_algo = SharedGasPriceAlgo::new_with_algorithm(start_algo);\n         let service = GasPriceService::new(0.into(), updater, shared_algo).await;\n         let watcher = StateWatcher::started();\n         let read_algo = service.next_block_algorithm();\n         let task = service.into_task(&watcher, ()).await.unwrap();\n-        let expected_price = 100;\n         let service = ServiceRunner::new(task);\n         service.start().unwrap();\n \n@@ -254,7 +249,7 @@ mod tests {\n         tokio::time::sleep(tokio::time::Duration::from_millis(10)).await;\n \n         // then\n-        let actual_price = read_algo.next_gas_price().await.unwrap();\n+        let actual_price = read_algo.next_gas_price().await;\n         assert_eq!(expected_price, actual_price);\n     }\n }\n"}
{"id": 51, "ground_truth": "Environment", "predicted": "Environment", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Describe the bug\n\nGithub CI for MacOS x86_64 is currently flaky with message:\r\n\r\n```\r\nWARNING: /Users/runner/hostedtoolcache/Java_Zulu_jdk/8.0.402-6/x64/jre/bin/java is loading libcrypto in an unsafe way\r\n```\r\n\r\nSome attempts have been made in #55 and #41, but it is still not resolved.\r\n\n\n### Steps to reproduce\n\nCheck the post-commit CI runs and occasionally it still fails with the above message.\n\n### Expected behavior\n\nThe CI for MacOS x86_64 should pass consistently.\n\n### Additional context\n\n_No response_\n\n### Code Patch:\n@@ -49,12 +49,13 @@ runs:\n         unzip $PROTO_ZIP\n         echo \"$HOME/d/protoc/bin\" >> $GITHUB_PATH\n         export PATH=$PATH:$HOME/d/protoc/bin\n-        # install openssl and setup DYLD_LIBRARY_PATH to work with libcrypto.dylib loading issues with x86_64 mac runners\n-        # see PR https://github.com/apache/arrow-datafusion-comet/pull/55 for more details\n+        # install openssl and setup DYLD_LIBRARY_PATH\n         brew install openssl\n-        OPENSSL_LIB_PATH=$(dirname `brew list openssl | grep 'lib/libcrypto.dylib'`)\n+        OPENSSL_LIB_PATH=`brew --prefix openssl`/lib\n         echo \"openssl lib path is: ${OPENSSL_LIB_PATH}\"\n-        export DYLD_LIBRARY_PATH=$OPENSSL_LIB_PATH:$DYLD_LIBRARY_PATH\n+        echo \"DYLD_LIBRARY_PATH=$OPENSSL_LIB_PATH:$DYLD_LIBRARY_PATH\" >> $GITHUB_ENV\n+        # output the current status of SIP for later debugging\n+        csrutil status || true\n \n     - name: Install JDK ${{inputs.jdk-version}}\n       uses: actions/setup-java@v4\n\n@@ -70,7 +70,7 @@ jobs:\n   macos-test:\n     strategy:\n       matrix:\n-        os: [macos-latest]\n+        os: [macos-13]\n         java_version: [8, 11, 17]\n         test-target: [rust, java]\n         is_push_event:\n"}
{"id": 84, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\ncc @Simon-Laux \r\n\r\nStill not fixed. I fixed python docs by pinning sphinx meanwhile.\r\n\r\nHere is a test failure: https://github.com/deltachat/deltachat-core-rust/actions/runs/8721401263/job/23925048947\n\n### Code Patch:\n@@ -63,7 +63,7 @@ commands =\n changedir=doc\n deps =\n # Pinned version, workaround for <https://github.com/breathe-doc/breathe/issues/981>\n-    sphinx==7.2.6\n+    sphinx<7.3\n     breathe\n     sphinx_rtd_theme\n commands =\n\n@@ -181,7 +181,7 @@ mod test_chatlist_events {\n             .await?;\n         set_muted(&bob, bob_chat.id, MuteDuration::Forever).await?;\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n \n         let sent_msg = alice.send_text(chat.id, \"moin2\").await;\n         bob.recv_msg(&sent_msg).await;\n@@ -216,7 +216,7 @@ mod test_chatlist_events {\n         let sent_msg = alice.send_text(chat.id, \"moin2\").await;\n         bob.recv_msg(&sent_msg).await;\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         chat::marknoticed_chat(&bob, DC_CHAT_ID_ARCHIVED_LINK).await?;\n         wait_for_chatlist_specific_item(&bob, DC_CHAT_ID_ARCHIVED_LINK).await;\n \n@@ -233,7 +233,7 @@ mod test_chatlist_events {\n         let sent_msg = alice.send_text(alice_to_bob_chat.id, \"hello\").await;\n         bob.recv_msg(&sent_msg).await;\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         // set alice name then receive messagefrom her with bob\n         alice.set_config(Config::Displayname, Some(\"Alice\")).await?;\n         let sent_msg = alice\n@@ -245,7 +245,7 @@ mod test_chatlist_events {\n \n         wait_for_chatlist_all_items(&bob).await;\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         // set name\n         let addr = alice_on_bob.get_addr();\n         Contact::create(&bob, \"Alice2\", addr).await?;\n@@ -266,7 +266,7 @@ mod test_chatlist_events {\n         let sent_msg = alice.send_text(alice_to_bob_chat.id, \"hello\").await;\n         bob.recv_msg(&sent_msg).await;\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         // set alice avatar then receive messagefrom her with bob\n         let file = alice.dir.path().join(\"avatar.png\");\n         let bytes = include_bytes!(\"../../test-data/image/avatar64x64.png\");\n@@ -292,7 +292,7 @@ mod test_chatlist_events {\n         let alice = tcm.alice().await;\n         let chat = create_group_chat(&alice, ProtectionStatus::Protected, \"My Group\").await?;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         chat.delete(&alice).await?;\n         wait_for_chatlist(&alice).await;\n         Ok(())\n@@ -303,7 +303,7 @@ mod test_chatlist_events {\n     async fn test_create_group_chat() -> Result<()> {\n         let mut tcm = TestContextManager::new();\n         let alice = tcm.alice().await;\n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         let chat = create_group_chat(&alice, ProtectionStatus::Protected, \"My Group\").await?;\n         wait_for_chatlist_and_specific_item(&alice, chat).await;\n         Ok(())\n@@ -314,7 +314,7 @@ mod test_chatlist_events {\n     async fn test_create_broadcastlist() -> Result<()> {\n         let mut tcm = TestContextManager::new();\n         let alice = tcm.alice().await;\n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         create_broadcast_list(&alice).await?;\n         wait_for_chatlist(&alice).await;\n         Ok(())\n@@ -327,11 +327,11 @@ mod test_chatlist_events {\n         let alice = tcm.alice().await;\n         let chat = create_group_chat(&alice, ProtectionStatus::Protected, \"My Group\").await?;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         chat::set_muted(&alice, chat, MuteDuration::Forever).await?;\n         wait_for_chatlist_specific_item(&alice, chat).await;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         chat::set_muted(&alice, chat, MuteDuration::NotMuted).await?;\n         wait_for_chatlist_specific_item(&alice, chat).await;\n \n@@ -352,7 +352,7 @@ mod test_chatlist_events {\n                 .unwrap(),\n         );\n         chat::set_muted(&alice, chat, mute_duration).await?;\n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         SystemTime::shift(Duration::from_secs(3));\n         wait_for_chatlist_specific_item(&alice, chat).await;\n \n@@ -366,7 +366,7 @@ mod test_chatlist_events {\n         let alice = tcm.alice().await;\n         let chat = create_group_chat(&alice, ProtectionStatus::Protected, \"My Group\").await?;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         chat::set_chat_name(&alice, chat, \"New Name\").await?;\n         wait_for_chatlist_specific_item(&alice, chat).await;\n \n@@ -380,7 +380,7 @@ mod test_chatlist_events {\n         let alice = tcm.alice().await;\n         let chat = create_group_chat(&alice, ProtectionStatus::Protected, \"My Group\").await?;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         let file = alice.dir.path().join(\"avatar.png\");\n         let bytes = include_bytes!(\"../../test-data/image/avatar64x64.png\");\n         tokio::fs::write(&file, bytes).await?;\n@@ -405,7 +405,7 @@ mod test_chatlist_events {\n         wait_for_chatlist_specific_item(&bob, chat_id_for_bob).await;\n         chat_id_for_bob.accept(&bob).await?;\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         chat::set_chat_name(&alice, chat, \"New Name\").await?;\n         let sent_msg = alice.send_text(chat, \"Hello\").await;\n         bob.recv_msg(&sent_msg).await;\n@@ -426,7 +426,7 @@ mod test_chatlist_events {\n         let sent_msg = alice.send_text(chat, \"Hello\").await;\n         let chat_id_for_bob = bob.recv_msg(&sent_msg).await.chat_id;\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         chat_id_for_bob.accept(&bob).await?;\n         wait_for_chatlist_specific_item(&bob, chat_id_for_bob).await;\n \n@@ -445,7 +445,7 @@ mod test_chatlist_events {\n         let sent_msg = alice.send_text(chat, \"Hello\").await;\n         let chat_id_for_bob = bob.recv_msg(&sent_msg).await.chat_id;\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         chat_id_for_bob.block(&bob).await?;\n         wait_for_chatlist(&bob).await;\n \n@@ -460,7 +460,7 @@ mod test_chatlist_events {\n         let chat = create_group_chat(&alice, ProtectionStatus::Protected, \"My Group\").await?;\n         let message = chat::send_text_msg(&alice, chat, \"Hello World\".to_owned()).await?;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         message::delete_msgs(&alice, &[message]).await?;\n         wait_for_chatlist_specific_item(&alice, chat).await;\n \n@@ -485,7 +485,7 @@ mod test_chatlist_events {\n         let chat_id_for_bob = bob.recv_msg(&sent_msg).await.chat_id;\n         assert!(chat_id_for_bob.get_fresh_msg_cnt(&bob).await? >= 1);\n \n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         chat::marknoticed_chat(&bob, chat_id_for_bob).await?;\n         wait_for_chatlist_specific_item(&bob, chat_id_for_bob).await;\n \n@@ -500,29 +500,36 @@ mod test_chatlist_events {\n         let contact_id = Contact::create(&alice, \"example\", \"example@example.com\").await?;\n         let _ = ChatId::create_for_contact(&alice, contact_id).await;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         Contact::block(&alice, contact_id).await?;\n         wait_for_chatlist(&alice).await;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         Contact::unblock(&alice, contact_id).await?;\n         wait_for_chatlist(&alice).await;\n \n         Ok(())\n     }\n \n-    /// ephemeral / disappearing messages\n+    /// Tests that expired disappearing message\n+    /// produces events about chatlist being modified.\n     #[tokio::test(flavor = \"multi_thread\", worker_threads = 2)]\n     async fn test_update_after_ephemeral_messages() -> Result<()> {\n         let mut tcm = TestContextManager::new();\n         let alice = tcm.alice().await;\n         let chat = create_group_chat(&alice, ProtectionStatus::Protected, \"My Group\").await?;\n-        chat.set_ephemeral_timer(&alice, crate::ephemeral::Timer::Enabled { duration: 1 })\n+        chat.set_ephemeral_timer(&alice, crate::ephemeral::Timer::Enabled { duration: 60 })\n             .await?;\n+        alice\n+            .evtracker\n+            .get_matching(|evt| matches!(evt, EventType::ChatEphemeralTimerModified { .. }))\n+            .await;\n+\n         let _ = chat::send_text_msg(&alice, chat, \"Hello\".to_owned()).await?;\n+        wait_for_chatlist_and_specific_item(&alice, chat).await;\n \n-        alice.evtracker.clear_events();\n-        SystemTime::shift(Duration::from_secs(3));\n+        SystemTime::shift(Duration::from_secs(70));\n+        crate::ephemeral::delete_expired_messages(&alice, crate::tools::time()).await?;\n         wait_for_chatlist_and_specific_item(&alice, chat).await;\n \n         Ok(())\n@@ -540,7 +547,7 @@ Content-Type: text/plain; charset=utf-8; format=flowed; delsp=no\n \n First thread.\"#;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         receive_imf(&alice, mime, false).await?;\n         wait_for_chatlist(&alice).await;\n \n@@ -561,34 +568,34 @@ First thread.\"#;\n         let qr = get_securejoin_qr(&alice.ctx, Some(alice_chatid)).await?;\n \n         // Step 2: Bob scans QR-code, sends vg-request\n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         let bob_chatid = join_securejoin(&bob.ctx, &qr).await?;\n         wait_for_chatlist(&bob).await;\n \n         let sent = bob.pop_sent_msg().await;\n \n         // Step 3: Alice receives vg-request, sends vg-auth-required\n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         alice.recv_msg_trash(&sent).await;\n \n         let sent = alice.pop_sent_msg().await;\n \n         // Step 4: Bob receives vg-auth-required, sends vg-request-with-auth\n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         bob.recv_msg_trash(&sent).await;\n         wait_for_chatlist_and_specific_item(&bob, bob_chatid).await;\n \n         let sent = bob.pop_sent_msg().await;\n \n         // Step 5+6: Alice receives vg-request-with-auth, sends vg-member-added\n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         alice.recv_msg_trash(&sent).await;\n         wait_for_chatlist_and_specific_item(&alice, alice_chatid).await;\n \n         let sent = alice.pop_sent_msg().await;\n \n         // Step 7: Bob receives vg-member-added\n-        bob.evtracker.clear_events();\n+        bob.evtracker.clear_events().await;\n         bob.recv_msg(&sent).await;\n         wait_for_chatlist_and_specific_item(&bob, bob_chatid).await;\n \n@@ -610,7 +617,7 @@ First thread.\"#;\n         let message = Message::load_from_db(&alice, msg_id).await?;\n         assert_eq!(message.get_state(), MessageState::OutDelivered);\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         chat::resend_msgs(&alice, &[msg_id]).await?;\n         wait_for_chatlist_specific_item(&alice, chat).await;\n \n@@ -626,7 +633,7 @@ First thread.\"#;\n         let msg_id = chat::send_text_msg(&alice, chat, \"Hello\".to_owned()).await?;\n         let _ = alice.pop_sent_msg().await;\n \n-        alice.evtracker.clear_events();\n+        alice.evtracker.clear_events().await;\n         reaction::send_reaction(&alice, msg_id, \"👍\").await?;\n         let _ = alice.pop_sent_msg().await;\n         wait_for_chatlist_specific_item(&alice, chat).await;\n\n@@ -1025,9 +1025,16 @@ impl EventTracker {\n             .await;\n     }\n \n-    /// Clears event queue\n-    pub fn clear_events(&self) {\n-        while self.try_recv().is_ok() {}\n+    /// Clears event queue.\n+    ///\n+    /// This spends 1 second instead of using `try_recv`\n+    /// to avoid accidentally leaving an event that\n+    /// was emitted right before calling `clear_events()`.\n+    ///\n+    /// Avoid using this function if you can\n+    /// by waiting for specific events you expect to receive.\n+    pub async fn clear_events(&self) {\n+        while let Ok(_ev) = tokio::time::timeout(Duration::from_secs(1), self.recv()).await {}\n     }\n }\n \n"}
{"id": 31, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nExamples (from yesterday 10 of 78 fails)\r\n\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7528201\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7527201\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7526808\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7526157\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7523749\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7519971\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7518972\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7518884\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7518515\r\nhttps://gitlab.parity.io/parity/mirrors/polkadot-sdk/-/jobs/7517649\r\n\r\ncc: @paritytech/sdk-node \n\n### Code Patch:\n@@ -70,11 +70,11 @@ zombienet-substrate-0001-basic-warp-sync:\n       --local-dir=\"${LOCAL_DIR}/0001-basic-warp-sync\"\n       --test=\"test-warp-sync.zndsl\"\n \n-.zombienet-substrate-0002-validators-warp-sync:\n+zombienet-substrate-0002-validators-warp-sync:\n   extends:\n     - .zombienet-substrate-warp-sync-common\n   before_script:\n-    - !reference [.zombienet-substrate-warp-sync-common, before_script]\n+    - !reference [ .zombienet-substrate-warp-sync-common, before_script ]\n     - cp --remove-destination ${LOCAL_DIR}/0001-basic-warp-sync/chain-spec.json ${LOCAL_DIR}/0002-validators-warp-sync\n   script:\n     - /home/nonroot/zombie-net/scripts/ci/run-test-local-env-manager.sh\n@@ -85,7 +85,7 @@ zombienet-substrate-0003-block-building-warp-sync:\n   extends:\n     - .zombienet-substrate-warp-sync-common\n   before_script:\n-    - !reference [.zombienet-substrate-warp-sync-common, before_script]\n+    - !reference [ .zombienet-substrate-warp-sync-common, before_script ]\n     - cp --remove-destination ${LOCAL_DIR}/0001-basic-warp-sync/chain-spec.json ${LOCAL_DIR}/0003-block-building-warp-sync\n   script:\n     - /home/nonroot/zombie-net/scripts/ci/run-test-local-env-manager.sh\n\n@@ -11,12 +11,12 @@ chain_spec_path = \"chain-spec.json\"\n   [[relaychain.nodes]]\n   name = \"alice\"\n   validator = true\n-  args = [\"--sync warp\"]\n+  args = [\"--log=beefy=debug\", \"--sync warp\"]\n \n   [[relaychain.nodes]]\n   name = \"bob\"\n   validator = true\n-  args = [\"--sync warp\"]\n+  args = [\"--log=beefy=debug\", \"--sync warp\"]\n \n   # we need at least 3 nodes for warp sync\n   [[relaychain.nodes]]\n\n@@ -31,11 +31,9 @@ bob: log line matches \"Block history download is complete\" within 120 seconds\n alice: reports block height is at least {{DB_BLOCK_HEIGHT}} within 10 seconds\n bob: reports block height is at least {{DB_BLOCK_HEIGHT}} within 10 seconds\n \n-alice: reports substrate_beefy_best_block is at least {{DB_BLOCK_HEIGHT}} within 180 seconds\n-bob: reports substrate_beefy_best_block is at least {{DB_BLOCK_HEIGHT}} within 180 seconds\n-\n-alice: reports substrate_beefy_best_block is greater than {{DB_BLOCK_HEIGHT}} within 180 seconds\n-bob: reports substrate_beefy_best_block is greater than {{DB_BLOCK_HEIGHT}} within 180 seconds\n+# In the worst case scenario, the validators should vote on 1 mandatory block each 6 seconds. And 1 era = 200 blocks.\n+alice: reports substrate_beefy_best_block is at least {{200*180/6}} within 180 seconds\n+bob: reports substrate_beefy_best_block is at least {{200*180/6}} within 180 seconds\n \n alice: count of log lines containing \"error\" is 0 within 10 seconds\n bob: count of log lines containing \"verification failed\" is 0 within 10 seconds\n"}
{"id": 20, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nFound in main\r\n```\r\nAssertionError: assert (['pg_xact/0000'], []) == ([], [])\r\n  At index 0 diff: ['pg_xact/0000'] != []\r\n  Full diff:\r\n  - ([], [])\r\n  + (['pg_xact/0000'], [])\r\nbatch_pg_regress/test_pg_regress.py:58: in test_pg_regress\r\n    check_restored_datadir_content(zenith_cli, test_output_dir, pg)\r\nfixtures/zenith_fixtures.py:1011: in check_restored_datadir_content\r\n    assert (mismatch, error) == ([], [])\r\nE   AssertionError: assert (['pg_xact/0000'], []) == ([], [])\r\nE     At index 0 diff: ['pg_xact/0000'] != []\r\nE     Full diff:\r\nE     - ([], [])\r\nE     + (['pg_xact/0000'], [])\r\n```\r\n\r\n@lubennikovaav is it because of the recent changes or this is a spurious error?\n\n### Code Patch:\n@@ -324,11 +324,11 @@ extern \"C\" fn finish_sync_safekeepers(wp: *mut WalProposer, lsn: XLogRecPtr) {\n     }\n }\n \n-extern \"C\" fn process_safekeeper_feedback(wp: *mut WalProposer, commit_lsn: XLogRecPtr) {\n+extern \"C\" fn process_safekeeper_feedback(wp: *mut WalProposer) {\n     unsafe {\n         let callback_data = (*(*wp).config).callback_data;\n         let api = callback_data as *mut Box<dyn ApiImpl>;\n-        (*api).process_safekeeper_feedback(&mut (*wp), commit_lsn)\n+        (*api).process_safekeeper_feedback(&mut (*wp))\n     }\n }\n \n\n@@ -142,7 +142,7 @@ pub trait ApiImpl {\n         todo!()\n     }\n \n-    fn process_safekeeper_feedback(&self, _wp: &mut WalProposer, _commit_lsn: u64) {\n+    fn process_safekeeper_feedback(&mut self, _wp: &mut WalProposer) {\n         todo!()\n     }\n \n\n@@ -1220,7 +1220,7 @@ PrepareAppendRequest(WalProposer *wp, AppendRequestHeader *req, XLogRecPtr begin\n \treq->epochStartLsn = wp->propEpochStartLsn;\n \treq->beginLsn = beginLsn;\n \treq->endLsn = endLsn;\n-\treq->commitLsn = GetAcknowledgedByQuorumWALPosition(wp);\n+\treq->commitLsn = wp->commitLsn;\n \treq->truncateLsn = wp->truncateLsn;\n \treq->proposerId = wp->greetRequest.proposerId;\n }\n@@ -1405,7 +1405,7 @@ static bool\n RecvAppendResponses(Safekeeper *sk)\n {\n \tWalProposer *wp = sk->wp;\n-\tXLogRecPtr\tminQuorumLsn;\n+\tXLogRecPtr\tnewCommitLsn;\n \tbool\t\treadAnything = false;\n \n \twhile (true)\n@@ -1444,18 +1444,19 @@ RecvAppendResponses(Safekeeper *sk)\n \tif (!readAnything)\n \t\treturn sk->state == SS_ACTIVE;\n \n-\tHandleSafekeeperResponse(wp);\n-\n+\t/* update commit_lsn */\n+\tnewCommitLsn = GetAcknowledgedByQuorumWALPosition(wp);\n \t/*\n-\t * Also send the new commit lsn to all the safekeepers.\n+\t * Send the new value to all safekeepers.\n \t */\n-\tminQuorumLsn = GetAcknowledgedByQuorumWALPosition(wp);\n-\tif (minQuorumLsn > wp->lastSentCommitLsn)\n+\tif (newCommitLsn > wp->commitLsn)\n \t{\n+\t\twp->commitLsn = newCommitLsn;\n \t\tBroadcastAppendRequest(wp);\n-\t\twp->lastSentCommitLsn = minQuorumLsn;\n \t}\n \n+\tHandleSafekeeperResponse(wp);\n+\n \treturn sk->state == SS_ACTIVE;\n }\n \n@@ -1632,11 +1633,9 @@ GetDonor(WalProposer *wp, XLogRecPtr *donor_lsn)\n static void\n HandleSafekeeperResponse(WalProposer *wp)\n {\n-\tXLogRecPtr\tminQuorumLsn;\n \tXLogRecPtr\tcandidateTruncateLsn;\n \n-\tminQuorumLsn = GetAcknowledgedByQuorumWALPosition(wp);\n-\twp->api.process_safekeeper_feedback(wp, minQuorumLsn);\n+\twp->api.process_safekeeper_feedback(wp);\n \n \t/*\n \t * Try to advance truncateLsn -- the last record flushed to all\n@@ -1649,7 +1648,7 @@ HandleSafekeeperResponse(WalProposer *wp)\n \t * can't commit entries from previous term' in Raft); 2)\n \t */\n \tcandidateTruncateLsn = CalculateMinFlushLsn(wp);\n-\tcandidateTruncateLsn = Min(candidateTruncateLsn, minQuorumLsn);\n+\tcandidateTruncateLsn = Min(candidateTruncateLsn, wp->commitLsn);\n \tif (candidateTruncateLsn > wp->truncateLsn)\n \t{\n \t\twp->truncateLsn = candidateTruncateLsn;\n\n@@ -564,7 +564,7 @@ typedef struct walproposer_api\n \t * backpressure feedback and to confirm WAL persistence (has been commited\n \t * on the quorum of safekeepers).\n \t */\n-\tvoid\t\t(*process_safekeeper_feedback) (WalProposer *wp, XLogRecPtr commitLsn);\n+\tvoid\t\t(*process_safekeeper_feedback) (WalProposer *wp);\n \n \t/*\n \t * Write a log message to the internal log processor. This is used only\n@@ -646,8 +646,8 @@ typedef struct WalProposer\n \t/* WAL has been generated up to this point */\n \tXLogRecPtr\tavailableLsn;\n \n-\t/* last commitLsn broadcasted to safekeepers */\n-\tXLogRecPtr\tlastSentCommitLsn;\n+\t/* cached GetAcknowledgedByQuorumWALPosition result */\n+\tXLogRecPtr\tcommitLsn;\n \n \tProposerGreeting greetRequest;\n \n\n@@ -68,6 +68,8 @@ static WalproposerShmemState *walprop_shared;\n static WalProposerConfig walprop_config;\n static XLogRecPtr sentPtr = InvalidXLogRecPtr;\n static const walproposer_api walprop_pg;\n+static volatile sig_atomic_t got_SIGUSR2 = false;\n+static bool reported_sigusr2 = false;\n \n static void nwp_shmem_startup_hook(void);\n static void nwp_register_gucs(void);\n@@ -101,6 +103,8 @@ static void add_nwr_event_set(Safekeeper *sk, uint32 events);\n static void update_nwr_event_set(Safekeeper *sk, uint32 events);\n static void rm_safekeeper_event_set(Safekeeper *to_remove, bool is_sk);\n \n+static void CheckGracefulShutdown(WalProposer *wp);\n+\n static XLogRecPtr GetLogRepRestartLSN(WalProposer *wp);\n \n static void\n@@ -492,6 +496,24 @@ walprop_pg_init_standalone_sync_safekeepers(void)\n \tBackgroundWorkerUnblockSignals();\n }\n \n+/*\n+ * We pretend to be a walsender process, and the lifecycle of a walsender is\n+ * slightly different than other procesess. At shutdown, walsender processes\n+ * stay alive until the very end, after the checkpointer has written the\n+ * shutdown checkpoint. When the checkpointer exits, the postmaster sends all\n+ * remaining walsender processes SIGUSR2. On receiving SIGUSR2, we try to send\n+ * the remaining WAL, and then exit. This ensures that the checkpoint record\n+ * reaches durable storage (in safekeepers), before the server shuts down\n+ * completely.\n+ */\n+static void\n+walprop_sigusr2(SIGNAL_ARGS)\n+{\n+\tgot_SIGUSR2 = true;\n+\n+\tSetLatch(MyLatch);\n+}\n+\n static void\n walprop_pg_init_bgworker(void)\n {\n@@ -503,6 +525,7 @@ walprop_pg_init_bgworker(void)\n \tpqsignal(SIGUSR1, procsignal_sigusr1_handler);\n \tpqsignal(SIGHUP, SignalHandlerForConfigReload);\n \tpqsignal(SIGTERM, die);\n+\tpqsignal(SIGUSR2, walprop_sigusr2);\n \n \tBackgroundWorkerUnblockSignals();\n \n@@ -1075,14 +1098,26 @@ StartProposerReplication(WalProposer *wp, StartReplicationCmd *cmd)\n #endif\n \n \t/*\n-\t * When we first start replication the standby will be behind the primary.\n-\t * For some applications, for example synchronous replication, it is\n-\t * important to have a clear state for this initial catchup mode, so we\n-\t * can trigger actions when we change streaming state later. We may stay\n-\t * in this state for a long time, which is exactly why we want to be able\n-\t * to monitor whether or not we are still here.\n+\t * XXX: Move straight to STOPPING state, skipping the STREAMING state.\n+\t *\n+\t * This is a bit weird. Normal walsenders stay in STREAMING state, until\n+\t * the checkpointer signals them that it is about to start writing the\n+\t * shutdown checkpoint. The walsenders acknowledge that they have received\n+\t * that signal by switching to STOPPING state. That tells the walsenders\n+\t * that they must not write any new WAL.\n+\t *\n+\t * However, we cannot easily intercept that signal from the checkpointer.\n+\t * It's sent by WalSndInitStopping(), using\n+\t * SendProcSignal(PROCSIGNAL_WALSND_INIT_STOPPING). It's received by\n+\t * HandleWalSndInitStopping, which sets a process-local got_STOPPING flag.\n+\t * However, that's all private to walsender.c.\n+\t *\n+\t * We don't need to do anything special upon receiving the signal, the\n+\t * walproposer doesn't write any WAL anyway, so we skip the STREAMING\n+\t * state and go directly to STOPPING mode. That way, the checkpointer\n+\t * won't wait for us.\n \t */\n-\tWalSndSetState(WALSNDSTATE_CATCHUP);\n+\tWalSndSetState(WALSNDSTATE_STOPPING);\n \n \t/*\n \t * Don't allow a request to stream from a future point in WAL that hasn't\n@@ -1122,6 +1157,8 @@ StartProposerReplication(WalProposer *wp, StartReplicationCmd *cmd)\n static void\n WalSndLoop(WalProposer *wp)\n {\n+\tXLogRecPtr\tflushPtr;\n+\n \t/* Clear any already-pending wakeups */\n \tResetLatch(MyLatch);\n \n@@ -1130,9 +1167,6 @@ WalSndLoop(WalProposer *wp)\n \t\tCHECK_FOR_INTERRUPTS();\n \n \t\tXLogBroadcastWalProposer(wp);\n-\n-\t\tif (MyWalSnd->state == WALSNDSTATE_CATCHUP)\n-\t\t\tWalSndSetState(WALSNDSTATE_STREAMING);\n \t\tWalProposerPoll(wp);\n \t}\n }\n@@ -1745,6 +1779,9 @@ walprop_pg_wait_event_set(WalProposer *wp, long timeout, Safekeeper **sk, uint32\n \t{\n \t\tConditionVariableCancelSleep();\n \t\tResetLatch(MyLatch);\n+\n+\t\tCheckGracefulShutdown(wp);\n+\n \t\t*events = WL_LATCH_SET;\n \t\treturn 1;\n \t}\n@@ -1798,6 +1835,41 @@ walprop_pg_finish_sync_safekeepers(WalProposer *wp, XLogRecPtr lsn)\n \texit(0);\n }\n \n+/*\n+ * Like vanilla walsender, on sigusr2 send all remaining WAL and exit.\n+ *\n+ * Note that unlike sync-safekeepers waiting here is not reliable: we\n+ * don't check that majority of safekeepers received and persisted\n+ * commit_lsn -- only that walproposer reached it (which immediately\n+ * broadcasts new value). Doing that without incurring redundant control\n+ * file syncing would need wp -> sk protocol change. OTOH unlike\n+ * sync-safekeepers which must bump commit_lsn or basebackup will fail,\n+ * this catchup is important only for tests where safekeepers/network\n+ * don't crash on their own.\n+ */\n+static void\n+CheckGracefulShutdown(WalProposer *wp)\n+{\n+\tif (got_SIGUSR2)\n+\t{\n+\t\tif (!reported_sigusr2)\n+\t\t{\n+\t\t\tXLogRecPtr\tflushPtr = walprop_pg_get_flush_rec_ptr(wp);\n+\n+\t\t\twpg_log(LOG, \"walproposer will send and wait for remaining WAL between %X/%X and %X/%X\",\n+\t\t\t\t\tLSN_FORMAT_ARGS(wp->commitLsn), LSN_FORMAT_ARGS(flushPtr));\n+\t\t\treported_sigusr2 = true;\n+\t\t}\n+\n+\t\tif (wp->commitLsn >= walprop_pg_get_flush_rec_ptr(wp))\n+\t\t{\n+\t\t\twpg_log(LOG, \"walproposer sent all WAL up to %X/%X, exiting\",\n+\t\t\t\t\tLSN_FORMAT_ARGS(wp->commitLsn));\n+\t\t\tproc_exit(0);\n+\t\t}\n+\t}\n+}\n+\n /*\n  * Choose most advanced PageserverFeedback and set it to *rf.\n  */\n@@ -1878,7 +1950,7 @@ CombineHotStanbyFeedbacks(HotStandbyFeedback *hs, WalProposer *wp)\n  * None of that is functional in sync-safekeepers.\n  */\n static void\n-walprop_pg_process_safekeeper_feedback(WalProposer *wp, XLogRecPtr commitLsn)\n+walprop_pg_process_safekeeper_feedback(WalProposer *wp)\n {\n \tHotStandbyFeedback hsFeedback;\n \tXLogRecPtr\toldDiskConsistentLsn;\n@@ -1893,10 +1965,10 @@ walprop_pg_process_safekeeper_feedback(WalProposer *wp, XLogRecPtr commitLsn)\n \treplication_feedback_set(&quorumFeedback.rf);\n \tSetZenithCurrentClusterSize(quorumFeedback.rf.currentClusterSize);\n \n-\tif (commitLsn > quorumFeedback.flushLsn || oldDiskConsistentLsn != quorumFeedback.rf.disk_consistent_lsn)\n+\tif (wp->commitLsn > quorumFeedback.flushLsn || oldDiskConsistentLsn != quorumFeedback.rf.disk_consistent_lsn)\n \t{\n-\t\tif (commitLsn > quorumFeedback.flushLsn)\n-\t\t\tquorumFeedback.flushLsn = commitLsn;\n+\t\tif (wp->commitLsn > quorumFeedback.flushLsn)\n+\t\t\tquorumFeedback.flushLsn = wp->commitLsn;\n \n \t\t/*\n \t\t * Advance the replication slot to commitLsn. WAL before it is\n@@ -1929,6 +2001,8 @@ walprop_pg_process_safekeeper_feedback(WalProposer *wp, XLogRecPtr commitLsn)\n \t\t\t\t\t\t\t\t XidFromFullTransactionId(hsFeedback.catalog_xmin),\n \t\t\t\t\t\t\t\t EpochFromFullTransactionId(hsFeedback.catalog_xmin));\n \t}\n+\n+\tCheckGracefulShutdown(wp);\n }\n \n static XLogRecPtr\n\n@@ -196,6 +196,7 @@ pub struct SimulationApi {\n     safekeepers: RefCell<Vec<SafekeeperConn>>,\n     disk: Arc<DiskWalProposer>,\n     redo_start_lsn: Option<Lsn>,\n+    last_logged_commit_lsn: u64,\n     shmem: UnsafeCell<walproposer::bindings::WalproposerShmemState>,\n     config: Config,\n     event_set: RefCell<Option<EventSet>>,\n@@ -228,6 +229,7 @@ impl SimulationApi {\n             safekeepers: RefCell::new(sk_conns),\n             disk: args.disk,\n             redo_start_lsn: args.redo_start_lsn,\n+            last_logged_commit_lsn: 0,\n             shmem: UnsafeCell::new(walproposer::bindings::WalproposerShmemState {\n                 mutex: 0,\n                 feedback: PageserverFeedback {\n@@ -596,14 +598,11 @@ impl ApiImpl for SimulationApi {\n         }\n     }\n \n-    fn process_safekeeper_feedback(\n-        &self,\n-        wp: &mut walproposer::bindings::WalProposer,\n-        commit_lsn: u64,\n-    ) {\n-        debug!(\"process_safekeeper_feedback, commit_lsn={}\", commit_lsn);\n-        if commit_lsn > wp.lastSentCommitLsn {\n-            self.os.log_event(format!(\"commit_lsn;{}\", commit_lsn));\n+    fn process_safekeeper_feedback(&mut self, wp: &mut walproposer::bindings::WalProposer) {\n+        debug!(\"process_safekeeper_feedback, commit_lsn={}\", wp.commitLsn);\n+        if wp.commitLsn > self.last_logged_commit_lsn {\n+            self.os.log_event(format!(\"commit_lsn;{}\", wp.commitLsn));\n+            self.last_logged_commit_lsn = wp.commitLsn;\n         }\n     }\n \n\n@@ -15,11 +15,11 @@\n import time\n import uuid\n from contextlib import closing, contextmanager\n-from dataclasses import dataclass, field\n+from dataclasses import dataclass\n from datetime import datetime\n from enum import Enum\n from fcntl import LOCK_EX, LOCK_UN, flock\n-from functools import cached_property\n+from functools import cached_property, partial\n from itertools import chain, product\n from pathlib import Path\n from types import TracebackType\n@@ -70,6 +70,8 @@\n     default_remote_storage,\n     remote_storage_to_toml_inline_table,\n )\n+from fixtures.safekeeper.http import SafekeeperHttpClient\n+from fixtures.safekeeper.utils import are_walreceivers_absent\n from fixtures.types import Lsn, TenantId, TenantShardId, TimelineId\n from fixtures.utils import (\n     ATTACHMENT_NAME_REGEX,\n@@ -2547,6 +2549,20 @@ def run_capture(\n         )\n         return base_path\n \n+    def get_pg_controldata_checkpoint_lsn(self, pgdata: str) -> Lsn:\n+        \"\"\"\n+        Run pg_controldata on given datadir and extract checkpoint lsn.\n+        \"\"\"\n+\n+        pg_controldata_path = os.path.join(self.pg_bin_path, \"pg_controldata\")\n+        cmd = f\"{pg_controldata_path} -D {pgdata}\"\n+        result = subprocess.run(cmd, capture_output=True, text=True, shell=True)\n+        checkpoint_lsn = re.findall(\n+            \"Latest checkpoint location:\\\\s+([0-9A-F]+/[0-9A-F]+)\", result.stdout\n+        )[0]\n+        log.info(f\"last checkpoint at {checkpoint_lsn}\")\n+        return Lsn(checkpoint_lsn)\n+\n \n @pytest.fixture(scope=\"function\")\n def pg_bin(test_output_dir: Path, pg_distrib_dir: Path, pg_version: PgVersion) -> PgBin:\n@@ -3565,220 +3581,6 @@ def list_segments(self, tenant_id, timeline_id) -> List[str]:\n         return segments\n \n \n-# Walreceiver as returned by sk's timeline status endpoint.\n-@dataclass\n-class Walreceiver:\n-    conn_id: int\n-    state: str\n-\n-\n-@dataclass\n-class SafekeeperTimelineStatus:\n-    acceptor_epoch: int\n-    pg_version: int  # Not exactly a PgVersion, safekeeper returns version as int, for example 150002 for 15.2\n-    flush_lsn: Lsn\n-    commit_lsn: Lsn\n-    timeline_start_lsn: Lsn\n-    backup_lsn: Lsn\n-    peer_horizon_lsn: Lsn\n-    remote_consistent_lsn: Lsn\n-    walreceivers: List[Walreceiver]\n-\n-\n-@dataclass\n-class SafekeeperMetrics:\n-    # These are metrics from Prometheus which uses float64 internally.\n-    # As a consequence, values may differ from real original int64s.\n-    flush_lsn_inexact: Dict[Tuple[TenantId, TimelineId], int] = field(default_factory=dict)\n-    commit_lsn_inexact: Dict[Tuple[TenantId, TimelineId], int] = field(default_factory=dict)\n-\n-\n-class SafekeeperHttpClient(requests.Session):\n-    HTTPError = requests.HTTPError\n-\n-    def __init__(self, port: int, auth_token: Optional[str] = None, is_testing_enabled=False):\n-        super().__init__()\n-        self.port = port\n-        self.auth_token = auth_token\n-        self.is_testing_enabled = is_testing_enabled\n-\n-        if auth_token is not None:\n-            self.headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n-\n-    def check_status(self):\n-        self.get(f\"http://localhost:{self.port}/v1/status\").raise_for_status()\n-\n-    def is_testing_enabled_or_skip(self):\n-        if not self.is_testing_enabled:\n-            pytest.skip(\"safekeeper was built without 'testing' feature\")\n-\n-    def configure_failpoints(self, config_strings: Tuple[str, str] | List[Tuple[str, str]]):\n-        self.is_testing_enabled_or_skip()\n-\n-        if isinstance(config_strings, tuple):\n-            pairs = [config_strings]\n-        else:\n-            pairs = config_strings\n-\n-        log.info(f\"Requesting config failpoints: {repr(pairs)}\")\n-\n-        res = self.put(\n-            f\"http://localhost:{self.port}/v1/failpoints\",\n-            json=[{\"name\": name, \"actions\": actions} for name, actions in pairs],\n-        )\n-        log.info(f\"Got failpoints request response code {res.status_code}\")\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert res_json is None\n-        return res_json\n-\n-    def debug_dump(self, params: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n-        params = params or {}\n-        res = self.get(f\"http://localhost:{self.port}/v1/debug_dump\", params=params)\n-        res.raise_for_status()\n-        res_json = json.loads(res.text)\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def patch_control_file(\n-        self,\n-        tenant_id: TenantId,\n-        timeline_id: TimelineId,\n-        patch: Dict[str, Any],\n-    ) -> Dict[str, Any]:\n-        res = self.patch(\n-            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/control_file\",\n-            json={\n-                \"updates\": patch,\n-                \"apply_fields\": list(patch.keys()),\n-            },\n-        )\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def pull_timeline(self, body: Dict[str, Any]) -> Dict[str, Any]:\n-        res = self.post(f\"http://localhost:{self.port}/v1/pull_timeline\", json=body)\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def copy_timeline(self, tenant_id: TenantId, timeline_id: TimelineId, body: Dict[str, Any]):\n-        res = self.post(\n-            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/copy\",\n-            json=body,\n-        )\n-        res.raise_for_status()\n-\n-    def timeline_digest(\n-        self, tenant_id: TenantId, timeline_id: TimelineId, from_lsn: Lsn, until_lsn: Lsn\n-    ) -> Dict[str, Any]:\n-        res = self.get(\n-            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/digest\",\n-            params={\n-                \"from_lsn\": str(from_lsn),\n-                \"until_lsn\": str(until_lsn),\n-            },\n-        )\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def timeline_create(\n-        self,\n-        tenant_id: TenantId,\n-        timeline_id: TimelineId,\n-        pg_version: int,  # Not exactly a PgVersion, safekeeper returns version as int, for example 150002 for 15.2\n-        commit_lsn: Lsn,\n-    ):\n-        body = {\n-            \"tenant_id\": str(tenant_id),\n-            \"timeline_id\": str(timeline_id),\n-            \"pg_version\": pg_version,\n-            \"commit_lsn\": str(commit_lsn),\n-        }\n-        res = self.post(f\"http://localhost:{self.port}/v1/tenant/timeline\", json=body)\n-        res.raise_for_status()\n-\n-    def timeline_status(\n-        self, tenant_id: TenantId, timeline_id: TimelineId\n-    ) -> SafekeeperTimelineStatus:\n-        res = self.get(f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}\")\n-        res.raise_for_status()\n-        resj = res.json()\n-        walreceivers = [Walreceiver(wr[\"conn_id\"], wr[\"status\"]) for wr in resj[\"walreceivers\"]]\n-        return SafekeeperTimelineStatus(\n-            acceptor_epoch=resj[\"acceptor_state\"][\"epoch\"],\n-            pg_version=resj[\"pg_info\"][\"pg_version\"],\n-            flush_lsn=Lsn(resj[\"flush_lsn\"]),\n-            commit_lsn=Lsn(resj[\"commit_lsn\"]),\n-            timeline_start_lsn=Lsn(resj[\"timeline_start_lsn\"]),\n-            backup_lsn=Lsn(resj[\"backup_lsn\"]),\n-            peer_horizon_lsn=Lsn(resj[\"peer_horizon_lsn\"]),\n-            remote_consistent_lsn=Lsn(resj[\"remote_consistent_lsn\"]),\n-            walreceivers=walreceivers,\n-        )\n-\n-    def record_safekeeper_info(self, tenant_id: TenantId, timeline_id: TimelineId, body):\n-        res = self.post(\n-            f\"http://localhost:{self.port}/v1/record_safekeeper_info/{tenant_id}/{timeline_id}\",\n-            json=body,\n-        )\n-        res.raise_for_status()\n-\n-    # only_local doesn't remove segments in the remote storage.\n-    def timeline_delete(\n-        self, tenant_id: TenantId, timeline_id: TimelineId, only_local: bool = False\n-    ) -> Dict[Any, Any]:\n-        res = self.delete(\n-            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}\",\n-            params={\n-                \"only_local\": str(only_local).lower(),\n-            },\n-        )\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def tenant_delete_force(self, tenant_id: TenantId) -> Dict[Any, Any]:\n-        res = self.delete(f\"http://localhost:{self.port}/v1/tenant/{tenant_id}\")\n-        res.raise_for_status()\n-        res_json = res.json()\n-        assert isinstance(res_json, dict)\n-        return res_json\n-\n-    def get_metrics_str(self) -> str:\n-        request_result = self.get(f\"http://localhost:{self.port}/metrics\")\n-        request_result.raise_for_status()\n-        return request_result.text\n-\n-    def get_metrics(self) -> SafekeeperMetrics:\n-        all_metrics_text = self.get_metrics_str()\n-\n-        metrics = SafekeeperMetrics()\n-        for match in re.finditer(\n-            r'^safekeeper_flush_lsn{tenant_id=\"([0-9a-f]+)\",timeline_id=\"([0-9a-f]+)\"} (\\S+)$',\n-            all_metrics_text,\n-            re.MULTILINE,\n-        ):\n-            metrics.flush_lsn_inexact[(TenantId(match.group(1)), TimelineId(match.group(2)))] = int(\n-                match.group(3)\n-            )\n-        for match in re.finditer(\n-            r'^safekeeper_commit_lsn{tenant_id=\"([0-9a-f]+)\",timeline_id=\"([0-9a-f]+)\"} (\\S+)$',\n-            all_metrics_text,\n-            re.MULTILINE,\n-        ):\n-            metrics.commit_lsn_inexact[\n-                (TenantId(match.group(1)), TimelineId(match.group(2)))\n-            ] = int(match.group(3))\n-        return metrics\n-\n-\n class S3Scrubber:\n     def __init__(self, env: NeonEnvBuilder, log_dir: Optional[Path] = None):\n         self.env = env\n@@ -4088,32 +3890,29 @@ def list_files_to_compare(pgdata_dir: Path) -> List[str]:\n \n # pg is the existing and running compute node, that we want to compare with a basebackup\n def check_restored_datadir_content(test_output_dir: Path, env: NeonEnv, endpoint: Endpoint):\n+    pg_bin = PgBin(test_output_dir, env.pg_distrib_dir, env.pg_version)\n+\n     # Get the timeline ID. We need it for the 'basebackup' command\n     timeline_id = TimelineId(endpoint.safe_psql(\"SHOW neon.timeline_id\")[0][0])\n \n-    # many tests already checkpoint, but do it just in case\n-    with closing(endpoint.connect()) as conn:\n-        with conn.cursor() as cur:\n-            cur.execute(\"CHECKPOINT\")\n-\n-    # wait for pageserver to catch up\n-    wait_for_last_flush_lsn(env, endpoint, endpoint.tenant_id, timeline_id)\n     # stop postgres to ensure that files won't change\n     endpoint.stop()\n \n+    # Read the shutdown checkpoint's LSN\n+    checkpoint_lsn = pg_bin.get_pg_controldata_checkpoint_lsn(endpoint.pg_data_dir_path())\n+\n     # Take a basebackup from pageserver\n     restored_dir_path = env.repo_dir / f\"{endpoint.endpoint_id}_restored_datadir\"\n     restored_dir_path.mkdir(exist_ok=True)\n \n-    pg_bin = PgBin(test_output_dir, env.pg_distrib_dir, env.pg_version)\n     psql_path = os.path.join(pg_bin.pg_bin_path, \"psql\")\n \n     pageserver_id = env.attachment_service.locate(endpoint.tenant_id)[0][\"node_id\"]\n     cmd = rf\"\"\"\n         {psql_path}                                    \\\n             --no-psqlrc                                \\\n             postgres://localhost:{env.get_pageserver(pageserver_id).service_port.pg}  \\\n-            -c 'basebackup {endpoint.tenant_id} {timeline_id}'  \\\n+            -c 'basebackup {endpoint.tenant_id} {timeline_id} {checkpoint_lsn}'  \\\n          | tar -x -C {restored_dir_path}\n     \"\"\"\n \n@@ -4262,6 +4061,49 @@ def wait_for_last_flush_lsn(\n     return min(results)\n \n \n+def flush_ep_to_pageserver(\n+    env: NeonEnv,\n+    ep: Endpoint,\n+    tenant: TenantId,\n+    timeline: TimelineId,\n+    pageserver_id: Optional[int] = None,\n+) -> Lsn:\n+    \"\"\"\n+    Stop endpoint and wait until all committed WAL reaches the pageserver\n+    (last_record_lsn). This is for use by tests which want everything written so\n+    far to reach pageserver *and* expecting that no more data will arrive until\n+    endpoint starts again, so unlike wait_for_last_flush_lsn it polls\n+    safekeepers instead of compute to learn LSN.\n+\n+    Returns the catch up LSN.\n+    \"\"\"\n+    ep.stop()\n+\n+    commit_lsn: Lsn = Lsn(0)\n+    # In principle in the absense of failures polling single sk would be enough.\n+    for sk in env.safekeepers:\n+        cli = sk.http_client()\n+        # wait until compute connections are gone\n+        wait_until(30, 0.5, partial(are_walreceivers_absent, cli, tenant, timeline))\n+        commit_lsn = max(cli.get_commit_lsn(tenant, timeline), commit_lsn)\n+\n+    # Note: depending on WAL filtering implementation, probably most shards\n+    # won't be able to reach commit_lsn (unless gaps are also ack'ed), so this\n+    # is broken in sharded case.\n+    shards = tenant_get_shards(env, tenant, pageserver_id)\n+    for tenant_shard_id, pageserver in shards:\n+        log.info(\n+            f\"flush_ep_to_pageserver: waiting for {commit_lsn} on shard {tenant_shard_id} on pageserver {pageserver.id})\"\n+        )\n+        waited = wait_for_last_record_lsn(\n+            pageserver.http_client(), tenant_shard_id, timeline, commit_lsn\n+        )\n+\n+        assert waited >= commit_lsn\n+\n+    return commit_lsn\n+\n+\n def wait_for_wal_insert_lsn(\n     env: NeonEnv,\n     endpoint: Endpoint,\n\n\n\n@@ -0,0 +1,227 @@\n+import json\n+import re\n+from dataclasses import dataclass, field\n+from typing import Any, Dict, List, Optional, Tuple, Union\n+\n+import pytest\n+import requests\n+\n+from fixtures.log_helper import log\n+from fixtures.types import Lsn, TenantId, TimelineId\n+\n+\n+# Walreceiver as returned by sk's timeline status endpoint.\n+@dataclass\n+class Walreceiver:\n+    conn_id: int\n+    state: str\n+\n+\n+@dataclass\n+class SafekeeperTimelineStatus:\n+    acceptor_epoch: int\n+    pg_version: int  # Not exactly a PgVersion, safekeeper returns version as int, for example 150002 for 15.2\n+    flush_lsn: Lsn\n+    commit_lsn: Lsn\n+    timeline_start_lsn: Lsn\n+    backup_lsn: Lsn\n+    peer_horizon_lsn: Lsn\n+    remote_consistent_lsn: Lsn\n+    walreceivers: List[Walreceiver]\n+\n+\n+@dataclass\n+class SafekeeperMetrics:\n+    # These are metrics from Prometheus which uses float64 internally.\n+    # As a consequence, values may differ from real original int64s.\n+    flush_lsn_inexact: Dict[Tuple[TenantId, TimelineId], int] = field(default_factory=dict)\n+    commit_lsn_inexact: Dict[Tuple[TenantId, TimelineId], int] = field(default_factory=dict)\n+\n+\n+class SafekeeperHttpClient(requests.Session):\n+    HTTPError = requests.HTTPError\n+\n+    def __init__(self, port: int, auth_token: Optional[str] = None, is_testing_enabled=False):\n+        super().__init__()\n+        self.port = port\n+        self.auth_token = auth_token\n+        self.is_testing_enabled = is_testing_enabled\n+\n+        if auth_token is not None:\n+            self.headers[\"Authorization\"] = f\"Bearer {auth_token}\"\n+\n+    def check_status(self):\n+        self.get(f\"http://localhost:{self.port}/v1/status\").raise_for_status()\n+\n+    def is_testing_enabled_or_skip(self):\n+        if not self.is_testing_enabled:\n+            pytest.skip(\"safekeeper was built without 'testing' feature\")\n+\n+    def configure_failpoints(self, config_strings: Union[Tuple[str, str], List[Tuple[str, str]]]):\n+        self.is_testing_enabled_or_skip()\n+\n+        if isinstance(config_strings, tuple):\n+            pairs = [config_strings]\n+        else:\n+            pairs = config_strings\n+\n+        log.info(f\"Requesting config failpoints: {repr(pairs)}\")\n+\n+        res = self.put(\n+            f\"http://localhost:{self.port}/v1/failpoints\",\n+            json=[{\"name\": name, \"actions\": actions} for name, actions in pairs],\n+        )\n+        log.info(f\"Got failpoints request response code {res.status_code}\")\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert res_json is None\n+        return res_json\n+\n+    def debug_dump(self, params: Optional[Dict[str, str]] = None) -> Dict[str, Any]:\n+        params = params or {}\n+        res = self.get(f\"http://localhost:{self.port}/v1/debug_dump\", params=params)\n+        res.raise_for_status()\n+        res_json = json.loads(res.text)\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def patch_control_file(\n+        self,\n+        tenant_id: TenantId,\n+        timeline_id: TimelineId,\n+        patch: Dict[str, Any],\n+    ) -> Dict[str, Any]:\n+        res = self.patch(\n+            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/control_file\",\n+            json={\n+                \"updates\": patch,\n+                \"apply_fields\": list(patch.keys()),\n+            },\n+        )\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def pull_timeline(self, body: Dict[str, Any]) -> Dict[str, Any]:\n+        res = self.post(f\"http://localhost:{self.port}/v1/pull_timeline\", json=body)\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def copy_timeline(self, tenant_id: TenantId, timeline_id: TimelineId, body: Dict[str, Any]):\n+        res = self.post(\n+            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/copy\",\n+            json=body,\n+        )\n+        res.raise_for_status()\n+\n+    def timeline_digest(\n+        self, tenant_id: TenantId, timeline_id: TimelineId, from_lsn: Lsn, until_lsn: Lsn\n+    ) -> Dict[str, Any]:\n+        res = self.get(\n+            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}/digest\",\n+            params={\n+                \"from_lsn\": str(from_lsn),\n+                \"until_lsn\": str(until_lsn),\n+            },\n+        )\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def timeline_create(\n+        self,\n+        tenant_id: TenantId,\n+        timeline_id: TimelineId,\n+        pg_version: int,  # Not exactly a PgVersion, safekeeper returns version as int, for example 150002 for 15.2\n+        commit_lsn: Lsn,\n+    ):\n+        body = {\n+            \"tenant_id\": str(tenant_id),\n+            \"timeline_id\": str(timeline_id),\n+            \"pg_version\": pg_version,\n+            \"commit_lsn\": str(commit_lsn),\n+        }\n+        res = self.post(f\"http://localhost:{self.port}/v1/tenant/timeline\", json=body)\n+        res.raise_for_status()\n+\n+    def timeline_status(\n+        self, tenant_id: TenantId, timeline_id: TimelineId\n+    ) -> SafekeeperTimelineStatus:\n+        res = self.get(f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}\")\n+        res.raise_for_status()\n+        resj = res.json()\n+        walreceivers = [Walreceiver(wr[\"conn_id\"], wr[\"status\"]) for wr in resj[\"walreceivers\"]]\n+        return SafekeeperTimelineStatus(\n+            acceptor_epoch=resj[\"acceptor_state\"][\"epoch\"],\n+            pg_version=resj[\"pg_info\"][\"pg_version\"],\n+            flush_lsn=Lsn(resj[\"flush_lsn\"]),\n+            commit_lsn=Lsn(resj[\"commit_lsn\"]),\n+            timeline_start_lsn=Lsn(resj[\"timeline_start_lsn\"]),\n+            backup_lsn=Lsn(resj[\"backup_lsn\"]),\n+            peer_horizon_lsn=Lsn(resj[\"peer_horizon_lsn\"]),\n+            remote_consistent_lsn=Lsn(resj[\"remote_consistent_lsn\"]),\n+            walreceivers=walreceivers,\n+        )\n+\n+    def get_commit_lsn(self, tenant_id: TenantId, timeline_id: TimelineId) -> Lsn:\n+        return self.timeline_status(tenant_id, timeline_id).commit_lsn\n+\n+    def record_safekeeper_info(self, tenant_id: TenantId, timeline_id: TimelineId, body):\n+        res = self.post(\n+            f\"http://localhost:{self.port}/v1/record_safekeeper_info/{tenant_id}/{timeline_id}\",\n+            json=body,\n+        )\n+        res.raise_for_status()\n+\n+    # only_local doesn't remove segments in the remote storage.\n+    def timeline_delete(\n+        self, tenant_id: TenantId, timeline_id: TimelineId, only_local: bool = False\n+    ) -> Dict[Any, Any]:\n+        res = self.delete(\n+            f\"http://localhost:{self.port}/v1/tenant/{tenant_id}/timeline/{timeline_id}\",\n+            params={\n+                \"only_local\": str(only_local).lower(),\n+            },\n+        )\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def tenant_delete_force(self, tenant_id: TenantId) -> Dict[Any, Any]:\n+        res = self.delete(f\"http://localhost:{self.port}/v1/tenant/{tenant_id}\")\n+        res.raise_for_status()\n+        res_json = res.json()\n+        assert isinstance(res_json, dict)\n+        return res_json\n+\n+    def get_metrics_str(self) -> str:\n+        request_result = self.get(f\"http://localhost:{self.port}/metrics\")\n+        request_result.raise_for_status()\n+        return request_result.text\n+\n+    def get_metrics(self) -> SafekeeperMetrics:\n+        all_metrics_text = self.get_metrics_str()\n+\n+        metrics = SafekeeperMetrics()\n+        for match in re.finditer(\n+            r'^safekeeper_flush_lsn{tenant_id=\"([0-9a-f]+)\",timeline_id=\"([0-9a-f]+)\"} (\\S+)$',\n+            all_metrics_text,\n+            re.MULTILINE,\n+        ):\n+            metrics.flush_lsn_inexact[(TenantId(match.group(1)), TimelineId(match.group(2)))] = int(\n+                match.group(3)\n+            )\n+        for match in re.finditer(\n+            r'^safekeeper_commit_lsn{tenant_id=\"([0-9a-f]+)\",timeline_id=\"([0-9a-f]+)\"} (\\S+)$',\n+            all_metrics_text,\n+            re.MULTILINE,\n+        ):\n+            metrics.commit_lsn_inexact[\n+                (TenantId(match.group(1)), TimelineId(match.group(2)))\n+            ] = int(match.group(3))\n+        return metrics\n\n@@ -0,0 +1,11 @@\n+from fixtures.log_helper import log\n+from fixtures.safekeeper.http import SafekeeperHttpClient\n+from fixtures.types import TenantId, TimelineId\n+\n+\n+def are_walreceivers_absent(\n+    sk_http_cli: SafekeeperHttpClient, tenant_id: TenantId, timeline_id: TimelineId\n+):\n+    status = sk_http_cli.timeline_status(tenant_id, timeline_id)\n+    log.info(f\"waiting for walreceivers to be gone, currently {status.walreceivers}\")\n+    return len(status.walreceivers) == 0\n\n@@ -4,12 +4,11 @@\n from fixtures.log_helper import log\n from fixtures.neon_fixtures import (\n     NeonEnvBuilder,\n+    flush_ep_to_pageserver,\n     wait_for_last_flush_lsn,\n )\n-from fixtures.pageserver.utils import wait_for_last_record_lsn, wait_for_upload\n+from fixtures.pageserver.utils import wait_for_upload\n from fixtures.remote_storage import RemoteStorageKind\n-from fixtures.types import Lsn\n-from fixtures.utils import query_scalar\n \n \n # Crates a few layers, ensures that we can evict them (removing locally but keeping track of them anyway)\n@@ -46,14 +45,15 @@ def test_basic_eviction(\n             FROM generate_series(1, 5000000) g\n             \"\"\"\n         )\n-        current_lsn = Lsn(query_scalar(cur, \"SELECT pg_current_wal_flush_lsn()\"))\n \n-    wait_for_last_record_lsn(client, tenant_id, timeline_id, current_lsn)\n+    # stops the endpoint\n+    current_lsn = flush_ep_to_pageserver(env, endpoint, tenant_id, timeline_id)\n+\n     client.timeline_checkpoint(tenant_id, timeline_id)\n     wait_for_upload(client, tenant_id, timeline_id, current_lsn)\n \n-    # disable compute & sks to avoid on-demand downloads by walreceiver / getpage\n-    endpoint.stop()\n+    # stop sks to avoid on-demand downloads by walreceiver / getpage; endpoint\n+    # has already been stopped by flush_ep_to_pageserver\n     for sk in env.safekeepers:\n         sk.stop()\n \n\n@@ -1,7 +1,7 @@\n import time\n \n from fixtures.log_helper import log\n-from fixtures.neon_fixtures import NeonEnvBuilder\n+from fixtures.neon_fixtures import NeonEnvBuilder, flush_ep_to_pageserver\n from fixtures.pageserver.types import (\n     DeltaLayerFileName,\n     ImageLayerFileName,\n@@ -115,8 +115,7 @@ def get_future_layers():\n                     )\n                     == 0\n                 )\n-\n-    endpoint.stop()\n+    last_record_lsn = flush_ep_to_pageserver(env, endpoint, tenant_id, timeline_id)\n \n     wait_for_upload_queue_empty(ps_http, tenant_id, timeline_id)\n \n\n@@ -8,6 +8,7 @@\n from fixtures.log_helper import log\n from fixtures.neon_fixtures import (\n     NeonEnvBuilder,\n+    flush_ep_to_pageserver,\n     last_flush_lsn_upload,\n     wait_for_last_flush_lsn,\n )\n@@ -517,7 +518,7 @@ def downloaded_bytes_and_count(pageserver_http: PageserverHttpClient) -> Tuple[i\n \n         with endpoint.cursor() as cur:\n             cur.execute(\"update a set id = -id\")\n-        wait_for_last_flush_lsn(env, endpoint, tenant_id, timeline_id)\n+        flush_ep_to_pageserver(env, endpoint, tenant_id, timeline_id)\n         pageserver_http.timeline_checkpoint(tenant_id, timeline_id)\n \n     layers = pageserver_http.layer_map_info(tenant_id, timeline_id)\n\n@@ -28,7 +28,6 @@\n     PgBin,\n     PgProtocol,\n     Safekeeper,\n-    SafekeeperHttpClient,\n     SafekeeperPort,\n     last_flush_lsn_upload,\n )\n@@ -46,6 +45,8 @@\n     default_remote_storage,\n     s3_storage,\n )\n+from fixtures.safekeeper.http import SafekeeperHttpClient\n+from fixtures.safekeeper.utils import are_walreceivers_absent\n from fixtures.types import Lsn, TenantId, TimelineId\n from fixtures.utils import get_dir_size, query_scalar, start_in_background\n \n@@ -1097,12 +1098,6 @@ def is_flush_lsn_aligned(sk_http_clis, tenant_id, timeline_id):\n     return all([flush_lsns[0] == flsn for flsn in flush_lsns])\n \n \n-def are_walreceivers_absent(sk_http_cli, tenant_id: TenantId, timeline_id: TimelineId):\n-    status = sk_http_cli.timeline_status(tenant_id, timeline_id)\n-    log.info(f\"waiting for walreceivers to be gone, currently {status.walreceivers}\")\n-    return len(status.walreceivers) == 0\n-\n-\n # Assert by xxd that WAL on given safekeepers is identical. No compute must be\n # running for this to be reliable.\n def cmp_sk_wal(sks: List[Safekeeper], tenant_id: TenantId, timeline_id: TimelineId):\n@@ -1347,6 +1342,36 @@ def test_peer_recovery(neon_env_builder: NeonEnvBuilder):\n     endpoint.safe_psql(\"insert into t select generate_series(1,100), 'payload'\")\n \n \n+# Test that when compute is terminated in fast (or smart) mode, walproposer is\n+# allowed to run and self terminate after shutdown checkpoint is written, so it\n+# commits it to safekeepers before exiting. This not required for correctness,\n+# but needed for tests using check_restored_datadir_content.\n+def test_wp_graceful_shutdown(neon_env_builder: NeonEnvBuilder, pg_bin: PgBin):\n+    neon_env_builder.num_safekeepers = 1\n+    env = neon_env_builder.init_start()\n+\n+    tenant_id = env.initial_tenant\n+    timeline_id = env.neon_cli.create_branch(\"test_wp_graceful_shutdown\")\n+    ep = env.endpoints.create_start(\"test_wp_graceful_shutdown\")\n+    ep.safe_psql(\"create table t(key int, value text)\")\n+    ep.stop()\n+\n+    # figure out checkpoint lsn\n+    ckpt_lsn = pg_bin.get_pg_controldata_checkpoint_lsn(ep.pg_data_dir_path())\n+\n+    sk_http_cli = env.safekeepers[0].http_client()\n+    commit_lsn = sk_http_cli.timeline_status(tenant_id, timeline_id).commit_lsn\n+    # Note: this is in memory value. Graceful shutdown of walproposer currently\n+    # doesn't guarantee persisted value, which is ok as we need it only for\n+    # tests. Persisting it without risking too many cf flushes needs a wp -> sk\n+    # protocol change. (though in reality shutdown sync-safekeepers does flush\n+    # of cf, so most of the time persisted value wouldn't lag)\n+    log.info(f\"sk commit_lsn {commit_lsn}\")\n+    # note that ckpt_lsn is the *beginning* of checkpoint record, so commit_lsn\n+    # must be actually higher\n+    assert commit_lsn > ckpt_lsn, \"safekeeper must have checkpoint record\"\n+\n+\n class SafekeeperEnv:\n     def __init__(\n         self,\n\n@@ -1 +1 @@\n-Subproject commit f49a962b9b3715d6f47017d1dcf905c36f93ae5e\n+Subproject commit b980d6f090c676e55fb2c830fb2434f532f635c0\n\n@@ -1 +1 @@\n-Subproject commit e8b9a28006a550d7ca7cbb9bd0238eb9cd57bbd8\n+Subproject commit 56f32c0e7330d17aaeee8bf211a73995180bd133\n\n@@ -1 +1 @@\n-Subproject commit 072697b2250da3251af75887b577104554b9cd44\n+Subproject commit 90078947229aa7f9ac5f7ed4527b2c7386d5332b\n\n@@ -1,6 +1,5 @@\n {\n-    \"postgres-v16\": \"072697b2250da3251af75887b577104554b9cd44\",\n-    \"postgres-v15\": \"e8b9a28006a550d7ca7cbb9bd0238eb9cd57bbd8\",\n-    \"postgres-v14\": \"f49a962b9b3715d6f47017d1dcf905c36f93ae5e\"\n+  \"postgres-v16\": \"90078947229aa7f9ac5f7ed4527b2c7386d5332b\",\n+  \"postgres-v15\": \"56f32c0e7330d17aaeee8bf211a73995180bd133\",\n+  \"postgres-v14\": \"b980d6f090c676e55fb2c830fb2434f532f635c0\"\n }\n-\n"}
{"id": 69, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nLooks like we’ve got a flaky batcher test:\r\n\r\n```\r\nthread 'sync::tests::send_full_capacity' panicked at batcher/src/sync.rs:303:9:\r\n  assertion `left == right` failed\r\n    left: [5, 6, 7, 8, 9]\r\n   right: [0, 1, 2, 3, 4]\r\n```\n\n### Code Patch:\n@@ -167,16 +167,11 @@ mod tests {\n     use std::{sync::mpsc, thread};\n \n     enum SenderCommand<T> {\n-        Send(T),\n         BlockingSend(T, Duration),\n         Stop,\n     }\n \n     impl<T> SenderCommand<T> {\n-        fn send(msg: T) -> Self {\n-            SenderCommand::Send(msg)\n-        }\n-\n         fn blocking_send(msg: T, timeout: Duration) -> Self {\n             SenderCommand::BlockingSend(msg, timeout)\n         }\n@@ -205,9 +200,6 @@ mod tests {\n \n         let handle = thread::spawn(move || loop {\n             match rx.recv().unwrap() {\n-                SenderCommand::Send(msg) => {\n-                    sender.send(msg);\n-                }\n                 SenderCommand::BlockingSend(msg, timeout) => {\n                     let _ = blocking_send(&sender, msg, timeout);\n                 }\n@@ -237,12 +229,11 @@ mod tests {\n \n         let (sender, receiver) = crate::bounded(10);\n \n-        let (sender, sender_handle) = spawn_sender(sender);\n         let (receiver, receiver_handle) = spawn_receiver(receiver);\n \n         // Send some messages\n         for _ in 0..10 {\n-            sender.send(SenderCommand::send(())).unwrap();\n+            sender.send(());\n         }\n \n         // Process the messages\n@@ -265,8 +256,7 @@ mod tests {\n         while { *received.lock().unwrap() } != 10 {}\n \n         // Shutdown\n-        sender.send(SenderCommand::stop()).unwrap();\n-        sender_handle.join().unwrap();\n+        drop(sender);\n         receiver_handle.join().unwrap();\n     }\n \n@@ -276,14 +266,14 @@ mod tests {\n \n         let (sender, receiver) = crate::bounded(5);\n \n-        let (sender, sender_handle) = spawn_sender(sender);\n-        let (receiver, receiver_handle) = spawn_receiver(receiver);\n-\n         // Send some messages\n         for i in 0..10 {\n-            sender.send(SenderCommand::send(i)).unwrap();\n+            sender.send(i);\n         }\n \n+        // Spawn the receiver after attempting to send all messages\n+        let (receiver, receiver_handle) = spawn_receiver(receiver);\n+\n         // Everything should be processed in a single batch\n         receiver\n             .send(ReceiverCommand::process_batch({\n@@ -299,12 +289,11 @@ mod tests {\n \n         // Only the last 5 messages should be processed\n         // The others were truncated\n-        while { received.lock().unwrap().len() } != 5 {}\n+        while { received.lock().unwrap().len() } == 0 {}\n         assert_eq!(vec![5, 6, 7, 8, 9], *received.lock().unwrap());\n \n         // Shutdown\n-        sender.send(SenderCommand::stop()).unwrap();\n-        sender_handle.join().unwrap();\n+        drop(sender);\n         receiver_handle.join().unwrap();\n     }\n \n"}
{"id": 74, "ground_truth": "Network", "predicted": "Network", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Summary**\r\n\r\n```shell\r\n--- /workspace/tests/suites/0_stateless/13_tpch/13_0007_q7.result        2022-09-30 05:37:31.856084133 +0000\r\n+++ /workspace/tests/suites/0_stateless/13_tpch/13_0007_q7.stdout        2022-09-30 05:38:54.725091643 +0000\r\n@@ -1,4 +1 @@\r\n-FRANCE        GERMANY        1995        4637235.15\r\n-FRANCE        GERMANY        1996        5224779.573\r\n-GERMANY        FRANCE        1995        6232818.703\r\n-GERMANY        FRANCE        1996        5557312.112\r\n+ERROR 1105 (HY000) at line 2: Code: 1067, displayText = error reading a body from connection: broken pipe (while in processor thread 0).\r\n```\r\n\r\nhttps://github.com/datafuselabs/databend/actions/runs/3156329493/jobs/5136357403\n\n### Code Patch:\n@@ -15,5 +15,4 @@ cd \"$SCRIPT_PATH/../../tests\" || exit\n \n echo \"Starting databend-test\"\n # 13_0004_q4: https://github.com/datafuselabs/databend/issues/8107\n-# 13_0005_q5: https://github.com/datafuselabs/databend/issues/7986\n-./databend-test --mode 'cluster' --run-dir 0_stateless --skip '13_0004_q4' --skip '13_0005_q5'\n+./databend-test --mode 'cluster' --run-dir 0_stateless --skip '13_0004_q4'\n\n@@ -264,6 +264,7 @@ impl Processor for ExchangeTransform {\n                 DataPacket::ProgressAndPrecommit { .. } => unreachable!(),\n                 DataPacket::FetchProgressAndPrecommit => unreachable!(),\n                 DataPacket::FragmentData(v) => self.on_recv_data(v),\n+                DataPacket::ClosingClient => Ok(()),\n             };\n         }\n \n\n@@ -130,6 +130,7 @@ impl Processor for ExchangeSourceTransform {\n                 DataPacket::FragmentData(v) => self.on_recv_data(v),\n                 DataPacket::FetchProgressAndPrecommit => unreachable!(),\n                 DataPacket::ProgressAndPrecommit { .. } => unreachable!(),\n+                DataPacket::ClosingClient => Ok(()),\n             };\n         }\n \n\n@@ -139,6 +139,7 @@ impl StatisticsReceiver {\n     fn recv_data(ctx: &Arc<QueryContext>, recv_data: Result<Option<DataPacket>>) -> Result<bool> {\n         match recv_data {\n             Ok(None) => Ok(true),\n+            Ok(Some(DataPacket::ClosingClient)) => Ok(true),\n             Err(transport_error) => Err(transport_error),\n             Ok(Some(DataPacket::ErrorCode(error))) => Err(error),\n             Ok(Some(DataPacket::FragmentData(_))) => unreachable!(),\n\n@@ -90,6 +90,11 @@ impl StatisticsSender {\n                         notified = right;\n                         recv = Box::pin(flight_exchange.recv());\n \n+                        if matches!(command, DataPacket::ClosingClient) {\n+                            ctx.get_exchange_manager().shutdown_query(&query_id);\n+                            return;\n+                        }\n+\n                         if let Err(_cause) = Self::on_command(&ctx, command, &flight_exchange).await\n                         {\n                             ctx.get_exchange_manager().shutdown_query(&query_id);\n@@ -100,6 +105,10 @@ impl StatisticsSender {\n             }\n \n             if let Ok(Some(command)) = flight_exchange.recv().await {\n+                if matches!(command, DataPacket::ClosingClient) {\n+                    return;\n+                }\n+\n                 if let Err(error) = Self::on_command(&ctx, command, &flight_exchange).await {\n                     tracing::warn!(\"Statistics send has error, cause: {:?}.\", error);\n                 }\n@@ -140,6 +149,7 @@ impl StatisticsSender {\n                     })\n                     .await\n             }\n+            DataPacket::ClosingClient => unreachable!(),\n         }\n     }\n \n\n@@ -135,10 +135,18 @@ impl FlightExchange {\n     ) -> FlightExchange {\n         let mut streaming = streaming.into_inner();\n         let (tx, rx) = async_channel::bounded(1);\n+\n         common_base::base::tokio::spawn(async move {\n             while let Some(message) = streaming.next().await {\n-                if let Err(_cause) = tx.send(message).await {\n-                    break;\n+                match message {\n+                    Ok(message) if DataPacket::is_closing_client(&message) => {\n+                        break;\n+                    }\n+                    other => {\n+                        if let Err(_c) = tx.send(other).await {\n+                            break;\n+                        }\n+                    }\n                 }\n             }\n         });\n@@ -159,8 +167,15 @@ impl FlightExchange {\n         let (tx, request_rx) = async_channel::bounded(1);\n         common_base::base::tokio::spawn(async move {\n             while let Some(message) = streaming.next().await {\n-                if let Err(_cause) = tx.send(message).await {\n-                    break;\n+                match message {\n+                    Ok(flight_data) if DataPacket::is_closing_client(&flight_data) => {\n+                        break;\n+                    }\n+                    other => {\n+                        if let Err(_cause) = tx.send(other).await {\n+                            break;\n+                        }\n+                    }\n                 }\n             }\n         });\n@@ -282,7 +297,9 @@ impl ClientFlightExchange {\n         if !self.is_closed_response.fetch_or(true, Ordering::SeqCst)\n             && self.state.response_count.fetch_sub(1, Ordering::AcqRel) == 1\n         {\n-            self.response_tx.close();\n+            let _ = self\n+                .response_tx\n+                .send_blocking(FlightData::from(DataPacket::ClosingClient));\n         }\n     }\n }\n@@ -304,17 +321,8 @@ impl Clone for ClientFlightExchange {\n \n impl Drop for ClientFlightExchange {\n     fn drop(&mut self) {\n-        if !self.is_closed_request.fetch_or(true, Ordering::SeqCst)\n-            && self.state.request_count.fetch_sub(1, Ordering::AcqRel) == 1\n-        {\n-            self.request_rx.close();\n-        }\n-\n-        if !self.is_closed_response.fetch_or(true, Ordering::SeqCst)\n-            && self.state.response_count.fetch_sub(1, Ordering::AcqRel) == 1\n-        {\n-            self.response_tx.close();\n-        }\n+        self.close_input();\n+        self.close_output();\n     }\n }\n \n@@ -398,7 +406,9 @@ impl ServerFlightExchange {\n         if !self.is_closed_response.fetch_or(true, Ordering::SeqCst)\n             && self.state.response_count.fetch_sub(1, Ordering::AcqRel) == 1\n         {\n-            self.response_tx.close();\n+            let _ = self\n+                .response_tx\n+                .send_blocking(Ok(FlightData::from(DataPacket::ClosingClient)));\n         }\n     }\n }\n\n@@ -56,6 +56,18 @@ pub enum DataPacket {\n         progress: Vec<ProgressInfo>,\n         precommit: Vec<PrecommitBlock>,\n     },\n+    // NOTE: Unknown reason. This may be tonic's bug.\n+    // when we use two-way streaming grpc for data exchange,\n+    // if the client side is closed and the server side reads data immediately.\n+    // we will get a broken pipe or connect reset error.\n+    // we use the ClosingClient to notify the server side to close the connection for avoid errors.\n+    ClosingClient,\n+}\n+\n+impl DataPacket {\n+    pub fn is_closing_client(data: &FlightData) -> bool {\n+        data.app_metadata.last() == Some(&0x05)\n+    }\n }\n \n impl From<DataPacket> for FlightData {\n@@ -103,6 +115,12 @@ impl From<DataPacket> for FlightData {\n                     app_metadata: vec![0x04],\n                 }\n             }\n+            DataPacket::ClosingClient => FlightData {\n+                data_body: vec![],\n+                data_header: vec![],\n+                flight_descriptor: None,\n+                app_metadata: vec![0x05],\n+            },\n         }\n     }\n }\n@@ -155,6 +173,7 @@ impl TryFrom<FlightData> for DataPacket {\n                     progress: progress_info,\n                 })\n             }\n+            0x05 => Ok(DataPacket::ClosingClient),\n             _ => Err(ErrorCode::BadBytes(\"Unknown flight data packet type.\")),\n         }\n     }\n"}
{"id": 66, "ground_truth": "Concurrency", "predicted": "Network", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/denoland/deno/runs/555522900\r\n```\r\n---- inspector_pause stdout ----\r\ntarget_dir D:\\a\\deno\\deno\\target\\release\r\nws_url ws://127.0.0.1:9230/ws/32db3e15-0d08-4ee4-af90-b3f9ca5a7273\r\n##[error]thread 'inspector_pause' panicked at 'called `Result::unwrap()` on an `Err` value: Io(Os { code: 10054, kind: ConnectionReset, message: \"An existing connection was forcibly closed by the remote host.\" })', cli\\tests\\integration_tests.rs:2076:17\r\n```\r\n\r\n\n\n### Code Patch:\n@@ -2015,7 +2015,6 @@ fn extract_ws_url_from_stderr(\n   url::Url::parse(ws_url).unwrap()\n }\n \n-#[cfg(not(target_os = \"linux\"))] // TODO(ry) broken on github actions.\n #[tokio::test]\n async fn inspector_connect() {\n   let script = deno::test_util::root_path()\n@@ -2122,7 +2121,6 @@ async fn inspector_break_on_first_line() {\n   child.kill().unwrap();\n }\n \n-#[cfg(not(target_os = \"linux\"))] // TODO(ry) broken on github actions.\n #[tokio::test]\n async fn inspector_pause() {\n   let script = deno::test_util::root_path()\n@@ -2182,7 +2180,6 @@ async fn inspector_pause() {\n   child.kill().unwrap();\n }\n \n-#[cfg(not(target_os = \"linux\"))] // TODO(ry) broken on github actions.\n #[tokio::test]\n async fn inspector_port_collision() {\n   let script = deno::test_util::root_path()\n"}
{"id": 35, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe test `Payload Index Test test_is_empty_conditions` is flaky.\r\n\r\n```\r\nfailures:\r\n\r\n---- tests::test_is_empty_conditions stdout ----\r\nthread 'tests::test_is_empty_conditions' panicked at 'assertion failed: (estimation_struct.exp as f64 - real_number as f64).abs() <\\n    (estimation_plain.exp as f64 - real_number as f64).abs()', lib/segment/tests/payload_index_test.rs:107:9\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n\r\n\r\nfailures:\r\n    tests::test_is_empty_conditions\r\n```\r\n\r\n## Current Behavior\r\nThe issue has been observed both locally and on [CI](https://github.com/qdrant/qdrant/runs/5993313794?check_suite_focus=true#step%3A7%3A202=)\r\n\r\n## Steps to Reproduce\r\nNo clear reproduction steps due to the flaky aspect.\n\n### Code Patch:\n@@ -106,11 +106,8 @@ mod tests {\n \n         assert!(\n             (estimation_struct.exp as f64 - real_number as f64).abs()\n-                < (estimation_plain.exp as f64 - real_number as f64).abs()\n+                <= (estimation_plain.exp as f64 - real_number as f64).abs()\n         );\n-\n-        eprintln!(\"estimation_struct = {:#?}\", estimation_struct);\n-        eprintln!(\"estimation_plain = {:#?}\", estimation_plain);\n     }\n \n     #[test]\n"}
{"id": 14, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nI have twice seen this error when running `scripts/cli-test.sh`:\r\n\r\n```\r\nError: Peg-out address received 0.000005 BTC, expected 0.00000500\r\n```\r\n\r\nThe two numbers are equal, but padded differently with zeros.\n\n### Code Patch:\n@@ -256,15 +256,15 @@ async fn cli_tests(dev_fed: DevFed) -> Result<()> {\n         let received_by_addr = bitcoind\n             .client()\n             .get_received_by_address(&pegout_addr.clone(), Some(0))?;\n-        Ok(received_by_addr != amount)\n+        Ok(received_by_addr == amount)\n     })\n     .await?;\n     bitcoind.mine_blocks(10).await?;\n     let received = bitcoind\n         .client()\n         .get_received_by_address(&pegout_addr, Some(0))?;\n     anyhow::ensure!(\n-        received != amount,\n+        received == amount,\n         \"Peg-out address received {}, expected {}\",\n         received,\n         amount\n@@ -654,14 +654,16 @@ async fn cli_tests(dev_fed: DevFed) -> Result<()> {\n         .await?\n         .as_u64()\n         .unwrap();\n+\n+    let expected_diff = 3030;\n     anyhow::ensure!(\n-        initial_client_ng_balance - final_cln_outgoing_client_ng_balance == 3030,\n-        \"Client NG balance changed by {} on CLN outgoing payment, expected 1010\",\n+        initial_client_ng_balance - final_cln_outgoing_client_ng_balance == expected_diff,\n+        \"Client NG balance changed by {} on CLN outgoing payment, expected {expected_diff}\",\n         initial_client_ng_balance - final_cln_outgoing_client_ng_balance\n     );\n     anyhow::ensure!(\n-        final_cln_outgoing_gateway_balance - initial_cln_gateway_balance == 3030,\n-        \"CLN Gateway balance changed by {} on CLN outgoing payment, expected 1010\",\n+        final_cln_outgoing_gateway_balance - initial_cln_gateway_balance == expected_diff,\n+        \"CLN Gateway balance changed by {} on CLN outgoing payment, expected {expected_diff}\",\n         final_cln_outgoing_gateway_balance - initial_cln_gateway_balance\n     );\n \n"}
{"id": 39, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nSee: https://gitlab.parity.io/parity/mirrors/polkadot/-/jobs/1530940 from the totally unrelated https://github.com/paritytech/polkadot/pull/5405 \r\n\r\ncc @drahnr @eskimor \n\n### Code Patch:\n@@ -75,7 +75,7 @@ fn test_harness<T: Future<Output = VirtualOverseer>>(\n \tstate\n }\n \n-const TIMEOUT: Duration = Duration::from_millis(100);\n+const TIMEOUT: Duration = Duration::from_millis(200);\n \n async fn overseer_send(overseer: &mut VirtualOverseer, msg: ApprovalDistributionMessage) {\n \tgum::trace!(msg = ?msg, \"Sending message\");\n"}
{"id": 73, "ground_truth": "Concurrency", "predicted": "Logic", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nOften fail with connection refused, e.g:\r\n\r\ngc_after_sync.py (connection refused):\r\nhttp://52.149.162.182:3000/#/test/7746\r\nhttp://52.149.162.182:3000/#/test/7626\r\n\r\ngc_after_sync1.py (rpc timeout)\r\nhttp://52.149.162.182:3000/#/test/4627\r\n\r\ngc_sync_after_sync.py (connection refused):\r\nhttp://52.149.162.182:3000/#/test/7748\r\nhttp://52.149.162.182:3000/#/test/7628\r\nhttp://52.149.162.182:3000/#/test/7508\n\n### Code Patch:\n@@ -529,17 +529,16 @@ impl Chain {\n         }\n         let prev_epoch_id = self.get_block_header(&head.prev_block_hash)?.epoch_id();\n         let epoch_change = prev_epoch_id != &head.epoch_id;\n-        let fork_tail = if epoch_change {\n+        let mut fork_tail = self.store.fork_tail()?;\n+        if epoch_change && fork_tail < gc_stop_height {\n             // if head doesn't change on the epoch boundary, we may update fork tail several times\n             // but that is fine since it doesn't affect correctness and also we limit the number of\n             // heights that fork cleaning goes through so it doesn't slow down client either.\n             let mut chain_store_update = self.store.store_update();\n             chain_store_update.update_fork_tail(gc_stop_height);\n             chain_store_update.commit()?;\n-            gc_stop_height\n-        } else {\n-            self.store.fork_tail()?\n-        };\n+            fork_tail = gc_stop_height;\n+        }\n         let mut gc_blocks_remaining = gc_blocks_limit;\n \n         // Forks Cleaning\n@@ -925,7 +924,8 @@ impl Chain {\n         // Get header we were syncing into.\n         let header = self.get_block_header(&sync_hash)?;\n         let prev_hash = *header.prev_hash();\n-        let gc_height = std::cmp::min(head.height + 1, header.height());\n+        let sync_height = header.height();\n+        let gc_height = std::cmp::min(head.height + 1, sync_height);\n \n         // GC all the data from current tail up to `gc_height`\n         let tail = self.store.tail()?;\n@@ -946,7 +946,9 @@ impl Chain {\n \n         // Clear Chunks data\n         let mut chain_store_update = self.mut_store().store_update();\n-        chain_store_update.clear_chunk_data(gc_height)?;\n+        // The largest height of chunk we have in storage is head.height + 1\n+        let chunk_height = std::cmp::min(head.height + 2, sync_height);\n+        chain_store_update.clear_chunk_data(chunk_height)?;\n         chain_store_update.commit()?;\n \n         // clear all trie data\n@@ -996,6 +998,7 @@ impl Chain {\n         // New Tail can not be earlier than `prev_block.header.inner_lite.height`\n         chain_store_update.update_tail(new_tail);\n         // New Chunk Tail can not be earlier than minimum of height_created in Block `prev_block`\n+        println!(\"resetting chunk tail to {}\", new_chunk_tail);\n         chain_store_update.update_chunk_tail(new_chunk_tail);\n         chain_store_update.commit()?;\n \n"}
{"id": 41, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nWe use [svm-rs](https://github.com/roynalnaruto/svm-rs) by @roynalnaruto, which is used for managing the global `solc` binary's version, by installing each version under `~/.svm/4version/solc-$version`. For some reason, when running on CI the installation process seems to be flaky, e.g. below:\r\n\r\n![telegram-cloud-photo-size-2-5366071301866894295-y](https://user-images.githubusercontent.com/17802178/134912737-ae1f3389-ec6a-49b0-b279-aecbf587ae7a.jpg)\r\n\r\nWe should fix that. Maybe there's a bug downstream in `svm-rs` (e.g. there should be some sort of global mutex that says \"hey a solc version is being installed right now, wait for it to unlock before you try running solc\"\r\n\r\nI tried doing that manually with the [lock inside solc/lib.rs](https://github.com/gakonst/dapptools-rs/blob/master/solc/src/lib.rs#L118-L120) but it was not enough apparently.\n\n### Code Patch:\n@@ -33,3 +33,4 @@ evm-adapters = { path = \"./../evm-adapters\", features = [\"sputnik\", \"sputnik-hel\n evmodin = { git = \"https://github.com/vorot93/evmodin\", features = [\"util\"] }\n # evm = { version = \"0.30.1\" }\n evm = { git = \"https://github.com/rust-blockchain/evm\" }\n+dapp-solc = { path = \"./../solc\", features = [\"sync\"] }\n\n@@ -19,3 +19,6 @@ tracing = \"0.1.28\"\n glob = \"0.3.0\"\n tokio = \"1.12.0\"\n rayon = \"1.5.1\"\n+\n+[features]\n+sync = []\n\n@@ -10,11 +10,11 @@ use std::{\n     time::Instant,\n };\n \n-#[cfg(test)]\n+#[cfg(any(test, feature = \"sync\"))]\n use std::sync::Mutex;\n-#[cfg(test)]\n+#[cfg(any(test, feature = \"sync\"))]\n static LOCK: Lazy<Mutex<()>> = Lazy::new(|| Mutex::new(()));\n-#[cfg(test)]\n+#[cfg(any(test, feature = \"sync\"))]\n use ethers::prelude::Lazy;\n \n /// Supports building contracts\n@@ -23,7 +23,6 @@ pub struct SolcBuilder<'a> {\n     contracts: &'a str,\n     remappings: &'a [String],\n     lib_paths: &'a [String],\n-    versions: Vec<Version>,\n     releases: Vec<Version>,\n }\n \n@@ -33,7 +32,6 @@ impl<'a> SolcBuilder<'a> {\n         remappings: &'a [String],\n         lib_paths: &'a [String],\n     ) -> Result<Self> {\n-        let versions = svm::installed_versions().unwrap_or_default();\n         // Try to download the releases, if it fails default to empty\n         let releases = match tokio::runtime::Runtime::new()?.block_on(svm::all_versions()) {\n             Ok(inner) => inner,\n@@ -42,7 +40,7 @@ impl<'a> SolcBuilder<'a> {\n                 Vec::new()\n             }\n         };\n-        Ok(Self { contracts, remappings, lib_paths, versions, releases })\n+        Ok(Self { contracts, remappings, lib_paths, releases })\n     }\n \n     /// Builds all provided contract files with the specified compiler version.\n@@ -83,7 +81,7 @@ impl<'a> SolcBuilder<'a> {\n \n     /// Builds all contracts with their corresponding compiler versions\n     #[tracing::instrument(skip(self))]\n-    pub fn build_all(&mut self) -> Result<HashMap<String, CompiledContract>> {\n+    pub fn build_all(&self) -> Result<HashMap<String, CompiledContract>> {\n         tracing::info!(\"starting compilation\");\n         let contracts_by_version = self.contract_versions()?;\n         let start = Instant::now();\n@@ -117,7 +115,7 @@ impl<'a> SolcBuilder<'a> {\n     /// to build it, and returns it along with its canonicalized path. If the required\n     /// compiler version is not installed, it also proceeds to install it.\n     #[tracing::instrument(err)]\n-    fn detect_version(&mut self, fname: &Path) -> Result<Option<(Version, String)>> {\n+    fn detect_version(&self, fname: &Path) -> Result<Option<(Version, String)>> {\n         let path = std::fs::canonicalize(fname)?;\n \n         // detects the required solc version\n@@ -128,14 +126,15 @@ impl<'a> SolcBuilder<'a> {\n             .into_string()\n             .map_err(|_| eyre::eyre!(\"invalid path, maybe not utf-8?\"))?;\n \n-        #[cfg(test)]\n+        #[cfg(any(test, feature = \"sync\"))]\n         // take the lock in tests, we use this to enforce that\n         // a test does not run while a compiler version is being installed\n         let _lock = LOCK.lock();\n \n         // load the local / remote versions\n-        let local_versions = Self::find_matching_installation(&mut self.versions, &sol_version);\n-        let remote_versions = Self::find_matching_installation(&mut self.releases, &sol_version);\n+        let versions = svm::installed_versions().unwrap_or_default();\n+        let local_versions = Self::find_matching_installation(&versions, &sol_version);\n+        let remote_versions = Self::find_matching_installation(&self.releases, &sol_version);\n \n         // if there's a better upstream version than the one we have, install it\n         let res = match (local_versions, remote_versions) {\n@@ -158,16 +157,15 @@ impl<'a> SolcBuilder<'a> {\n         Ok(res)\n     }\n \n-    fn install_version(&mut self, version: &Version) {\n+    fn install_version(&self, version: &Version) {\n         println!(\"Installing {}\", version);\n         // Blocking call to install it over RPC.\n         install_blocking(version).expect(\"could not install solc remotely\");\n-        self.versions.push(version.clone());\n         println!(\"Done!\");\n     }\n \n     /// Gets a map of compiler version -> vec[contract paths]\n-    fn contract_versions(&mut self) -> Result<HashMap<String, Vec<String>>> {\n+    fn contract_versions(&self) -> Result<HashMap<String, Vec<String>>> {\n         // Group contracts in the nones with the same version pragma\n         let files = glob::glob(self.contracts)?;\n         // tracing::trace!(\"Compiling files under {}\", self.contracts);\n@@ -222,11 +220,9 @@ impl<'a> SolcBuilder<'a> {\n \n     /// Find a matching local installation for the specified required version\n     fn find_matching_installation(\n-        versions: &mut [Version],\n+        versions: &[Version],\n         required_version: &VersionReq,\n     ) -> Option<Version> {\n-        // sort through them\n-        versions.sort();\n         // iterate in reverse to find the last match\n         versions.iter().rev().find(|version| required_version.matches(version)).cloned()\n     }\n@@ -263,6 +259,9 @@ mod tests {\n \n     #[test]\n     fn test_find_installed_version_path() {\n+        // this test does not take the lock by default, so we need to manually\n+        // add it here.\n+        let _lock = LOCK.lock();\n         let ver = \"0.8.6\";\n         let version = Version::from_str(ver).unwrap();\n         if !svm::installed_versions().unwrap().contains(&version) {\n@@ -283,26 +282,26 @@ mod tests {\n \n     #[test]\n     fn test_find_latest_matching_installation() {\n-        let mut versions = [\"0.4.24\", \"0.5.1\", \"0.5.2\"]\n+        let versions = [\"0.4.24\", \"0.5.1\", \"0.5.2\"]\n             .iter()\n             .map(|version| Version::from_str(version).unwrap())\n             .collect::<Vec<_>>();\n \n         let required = VersionReq::from_str(\">=0.4.24\").unwrap();\n \n-        let got = SolcBuilder::find_matching_installation(&mut versions, &required).unwrap();\n+        let got = SolcBuilder::find_matching_installation(&versions, &required).unwrap();\n         assert_eq!(got, versions[2]);\n     }\n \n     #[test]\n     fn test_no_matching_installation() {\n-        let mut versions = [\"0.4.24\", \"0.5.1\", \"0.5.2\"]\n+        let versions = [\"0.4.24\", \"0.5.1\", \"0.5.2\"]\n             .iter()\n             .map(|version| Version::from_str(version).unwrap())\n             .collect::<Vec<_>>();\n \n         let required = VersionReq::from_str(\">=0.6.0\").unwrap();\n-        let got = SolcBuilder::find_matching_installation(&mut versions, &required);\n+        let got = SolcBuilder::find_matching_installation(&versions, &required);\n         assert!(got.is_none());\n     }\n \n@@ -371,7 +370,7 @@ mod tests {\n     fn test_detect_version() {\n         let dir = mkdir();\n \n-        let mut builder = SolcBuilder::new(\"\", &[], &[]).unwrap();\n+        let builder = SolcBuilder::new(\"\", &[], &[]).unwrap();\n         for (pragma, expected) in [\n             // pinned\n             (\"=0.4.14\", \"0.4.14\"),\n@@ -422,7 +421,7 @@ mod tests {\n \n         let dir_str = dir.clone().into_os_string().into_string().unwrap();\n         let glob = format!(\"{}/**/*.sol\", dir_str);\n-        let mut builder = SolcBuilder::new(&glob, &[], &[]).unwrap();\n+        let builder = SolcBuilder::new(&glob, &[], &[]).unwrap();\n \n         let versions = builder.contract_versions().unwrap();\n         assert_eq!(versions[\"0.4.14\"].len(), 2);\n@@ -442,7 +441,7 @@ mod tests {\n     #[test]\n     fn test_build_all_versions() {\n         let path = get_glob(\"testdata/test-contract-versions\");\n-        let mut builder = SolcBuilder::new(&path, &[], &[]).unwrap();\n+        let builder = SolcBuilder::new(&path, &[], &[]).unwrap();\n         let res = builder.build_all().unwrap();\n         // Contracts A to F\n         assert_eq!(res.keys().collect::<Vec<_>>().len(), 5);\n@@ -460,7 +459,7 @@ mod tests {\n             .into_string()\n             .unwrap();\n         let libs = vec![lib];\n-        let mut builder = SolcBuilder::new(&path, &remappings, &libs).unwrap();\n+        let builder = SolcBuilder::new(&path, &remappings, &libs).unwrap();\n         let res = builder.build_all().unwrap();\n         // Foo & Bar\n         assert_eq!(res.keys().collect::<Vec<_>>().len(), 2);\n@@ -481,7 +480,7 @@ mod tests {\n             canonicalized_path(\"testdata/test-contract-libs/lib1\"),\n             canonicalized_path(\"testdata/test-contract-libs/lib2\"),\n         ];\n-        let mut builder = SolcBuilder::new(&path, &[], &libs).unwrap();\n+        let builder = SolcBuilder::new(&path, &[], &libs).unwrap();\n         let res = builder.build_all().unwrap();\n         // Foo & Bar\n         assert_eq!(res.keys().collect::<Vec<_>>().len(), 3);\n"}
{"id": 2, "ground_truth": "Randomness", "predicted": "Randomness", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/near/nearcore/pull/10015 introduced a flaky test test_trie_consistency_random, fails about 2% of the time; will fix at first convenience.\n\n### Code Patch:\n@@ -1243,7 +1243,7 @@ mod tests {\n             let num_insertions =\n                 rand::thread_rng().gen_range(0..=(MAX_KEYS - existing_keys.len()) / SLOWDOWN);\n             let num_deletions =\n-                rand::thread_rng().gen_range(0..=existing_keys.len() / SLOWDOWN + 1);\n+                rand::thread_rng().gen_range(0..=(existing_keys.len() + SLOWDOWN - 1) / SLOWDOWN);\n             let mut changes = Vec::new();\n             for _ in 0..num_insertions {\n                 let key_length = rand::thread_rng().gen_range(0..=10);\n"}
{"id": 25, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nI encounter this time and time again: you open a PR, wait for checks for 10/20/30 minutes, and then you see that same workflows fail on the same tests. During that time I get distracted on something else and forget to re-start checks again just in time. Sometimes it takes 5-6 restarts to get them work. This way, the moment of all green lights might delay for days. And it delays development in my case.\r\n\r\nFrom my observation, the following workflows are flaky:\r\n\r\n- `I2::Dev::Tests` > `with_coverage`, `integration`, `unstable`\r\n\r\nAnd these are particular flaky tests:\r\n\r\n- `integration::extra_functional::offline_peers::genesis_block_is_committed_with_some_offline_peers`\r\n- `integration::extra_functional::unstable_network::soft_fork`\r\n- And _maybe_ some others, haven't collected much data\r\n\r\nAre these tests worth it?\r\n\r\nMy another concern is that I don't see the rationale behind having so many workflows:\r\n\r\n1. `I2::Dev::Static`\r\n    1. smart contracts\r\n    2. workspace\r\n2. `I2::Tests::UI`\r\n    1. test with all features\r\n    2. test with no default features\r\n3. `I2::Dev::Tests`\r\n    1. consistency\r\n    2. with_coverage\r\n    3. integration\r\n    4. unstable\r\n    5. client-cli-tests\r\n\r\n(there are some others too)\r\n\r\nThese workflows all run Cargo and compile more or less the same stuff. Yes, there are variations in features presets, but Cargo handles it for us. It can granularly reuse compilation artifacts depending on the context (apart from cases with different RUSTC flags, I suppose).\r\n\r\nSo, I guess that it is worth trying to combine all these workflows into a single one, and build it in a way so that it can report as many useful information as possible in a single run. I wonder how much more/less it would be efficient.\r\n\r\nAnother useful implication of this would be a shorter feedback on some early errors. For example, a certain change in PR introduces something and Iroha cannot even compile. Currently, all 8+ workflows will run and fail on the same error. In the case of a unified CI, there will be less work repetition.\r\n\r\n### Proposals\r\n\r\n- Prioritise zero-tolerance to flaky tests from development side\r\n- If flaky tests couldn't be easily fixed, possibly move them away from PR checks to after-merge checks.\r\n- Create a single unified workflow, and research the performance impact of it.\r\n- Explore ways to use a sane scripting language for CI, not Shell. That's for a separate issue, maybe.\n\n### Code Patch:\n@@ -207,6 +207,11 @@ impl ChainWide {\n     pub fn pipeline_time(&self) -> Duration {\n         self.block_time + self.commit_time\n     }\n+\n+    /// Estimates as `block_time + commit_time / 2`\n+    pub fn consensus_estimation(&self) -> Duration {\n+        self.block_time + (self.commit_time / 2)\n+    }\n }\n \n impl Default for ChainWide {\n\n@@ -6,7 +6,6 @@\n //! [`Block`]s are organised into a linear sequence over time (also known as the block chain).\n use std::error::Error as _;\n \n-use iroha_config::parameters::defaults::chain_wide::CONSENSUS_ESTIMATION as DEFAULT_CONSENSUS_ESTIMATION;\n use iroha_crypto::{HashOf, KeyPair, MerkleTree, SignatureOf, SignaturesOf};\n use iroha_data_model::{\n     block::*,\n@@ -101,7 +100,7 @@ pub enum SignatureVerificationError {\n pub struct BlockBuilder<B>(B);\n \n mod pending {\n-    use std::time::SystemTime;\n+    use std::time::{Duration, SystemTime};\n \n     use iroha_data_model::transaction::CommittedTransaction;\n \n@@ -149,6 +148,7 @@ mod pending {\n             prev_block_hash: Option<HashOf<SignedBlock>>,\n             view_change_index: usize,\n             transactions: &[CommittedTransaction],\n+            consensus_estimation: Duration,\n         ) -> BlockHeader {\n             BlockHeader {\n                 height: prev_height\n@@ -171,7 +171,7 @@ mod pending {\n                 view_change_index: view_change_index\n                     .try_into()\n                     .expect(\"INTERNAL BUG: Number of view changes exceeds u32::MAX\"),\n-                consensus_estimation_ms: DEFAULT_CONSENSUS_ESTIMATION\n+                consensus_estimation_ms: consensus_estimation\n                     .as_millis()\n                     .try_into()\n                     .expect(\"INTERNAL BUG: Time should fit into u64\"),\n@@ -222,6 +222,7 @@ mod pending {\n                     state.latest_block_hash(),\n                     view_change_index,\n                     &transactions,\n+                    state.config.consensus_estimation(),\n                 ),\n                 transactions,\n                 commit_topology: self.0.commit_topology.ordered_peers,\n@@ -500,10 +501,7 @@ mod valid {\n                     transactions_hash: None,\n                     timestamp_ms: 0,\n                     view_change_index: 0,\n-                    consensus_estimation_ms: DEFAULT_CONSENSUS_ESTIMATION\n-                        .as_millis()\n-                        .try_into()\n-                        .expect(\"Time should fit into u64\"),\n+                    consensus_estimation_ms: 4_000,\n                 },\n                 transactions: Vec::new(),\n                 commit_topology: UniqueVec::new(),\n\n@@ -317,14 +317,14 @@ pub mod isi {\n             let parameter = self.parameter;\n             let parameter_id = parameter.id.clone();\n \n-            let world = &mut state_transaction.world;\n-            if !world.parameters.swap_remove(&parameter) {\n+            if !state_transaction.world.parameters.swap_remove(&parameter) {\n                 return Err(FindError::Parameter(parameter_id).into());\n             }\n-\n-            world.parameters.insert(parameter);\n-\n-            world.emit_events(Some(ConfigurationEvent::Changed(parameter_id)));\n+            state_transaction.world.parameters.insert(parameter.clone());\n+            state_transaction\n+                .world\n+                .emit_events(Some(ConfigurationEvent::Changed(parameter_id)));\n+            state_transaction.try_apply_core_parameter(parameter);\n \n             Ok(())\n         }\n@@ -340,16 +340,17 @@ pub mod isi {\n             let parameter = self.parameter;\n             let parameter_id = parameter.id.clone();\n \n-            let world = &mut state_transaction.world;\n-            if !world.parameters.insert(parameter) {\n+            if !state_transaction.world.parameters.insert(parameter.clone()) {\n                 return Err(RepetitionError {\n                     instruction_type: InstructionType::NewParameter,\n                     id: IdBox::ParameterId(parameter_id),\n                 }\n                 .into());\n             }\n-\n-            world.emit_events(Some(ConfigurationEvent::Created(parameter_id)));\n+            state_transaction\n+                .world\n+                .emit_events(Some(ConfigurationEvent::Created(parameter_id)));\n+            state_transaction.try_apply_core_parameter(parameter);\n \n             Ok(())\n         }\n\n@@ -1,6 +1,10 @@\n //! This module provides the [`State`] — an in-memory representation of the current blockchain state.\n use std::{\n-    borrow::Borrow, collections::BTreeSet, marker::PhantomData, num::NonZeroUsize, sync::Arc,\n+    borrow::Borrow,\n+    collections::BTreeSet,\n+    marker::PhantomData,\n+    num::{NonZeroU32, NonZeroUsize},\n+    sync::Arc,\n     time::Duration,\n };\n \n@@ -1252,7 +1256,6 @@ impl<'state> StateBlock<'state> {\n \n         self.block_hashes.push(block_hash);\n \n-        self.apply_parameters();\n         self.world.events_buffer.push(\n             BlockEvent {\n                 header: block.as_ref().header().clone(),\n@@ -1266,20 +1269,18 @@ impl<'state> StateBlock<'state> {\n \n     /// Create time event using previous and current blocks\n     fn create_time_event(&self, block: &CommittedBlock) -> TimeEvent {\n-        use iroha_config::parameters::defaults::chain_wide::CONSENSUS_ESTIMATION as DEFAULT_CONSENSUS_ESTIMATION;\n-\n         let prev_interval = self.latest_block_ref().map(|latest_block| {\n             let header = &latest_block.as_ref().header();\n \n             TimeInterval {\n                 since: header.timestamp(),\n-                length: DEFAULT_CONSENSUS_ESTIMATION,\n+                length: header.consensus_estimation(),\n             }\n         });\n \n         let interval = TimeInterval {\n             since: block.as_ref().header().timestamp(),\n-            length: DEFAULT_CONSENSUS_ESTIMATION,\n+            length: block.as_ref().header().consensus_estimation(),\n         };\n \n         TimeEvent {\n@@ -1336,32 +1337,6 @@ impl<'state> StateBlock<'state> {\n \n         errors.is_empty().then_some(()).ok_or(errors)\n     }\n-\n-    fn apply_parameters(&mut self) {\n-        use iroha_data_model::parameter::default::*;\n-\n-        macro_rules! update_params {\n-            ($($param:expr => $config:expr),+ $(,)?) => {\n-                $(if let Some(param) = self.world.query_param($param) {\n-                    $config = param;\n-                })+\n-            };\n-        }\n-\n-        update_params! {\n-            WSV_DOMAIN_METADATA_LIMITS => self.config.domain_metadata_limits,\n-            WSV_ASSET_DEFINITION_METADATA_LIMITS => self.config.asset_definition_metadata_limits,\n-            WSV_ACCOUNT_METADATA_LIMITS => self.config.account_metadata_limits,\n-            WSV_ASSET_METADATA_LIMITS => self.config.asset_metadata_limits,\n-            WSV_TRIGGER_METADATA_LIMITS => self.config.trigger_metadata_limits,\n-            WSV_IDENT_LENGTH_LIMITS => self.config.ident_length_limits,\n-            EXECUTOR_FUEL_LIMIT => self.config.executor_runtime.fuel_limit,\n-            EXECUTOR_MAX_MEMORY => self.config.executor_runtime.max_memory_bytes,\n-            WASM_FUEL_LIMIT => self.config.wasm_runtime.fuel_limit,\n-            WASM_MAX_MEMORY => self.config.wasm_runtime.max_memory_bytes,\n-            TRANSACTION_LIMITS => self.config.transaction_limits,\n-        }\n-    }\n }\n \n impl StateTransaction<'_, '_> {\n@@ -1373,6 +1348,96 @@ impl StateTransaction<'_, '_> {\n         self.world.apply();\n     }\n \n+    /// If given [`Parameter`] represents some of the core chain-wide\n+    /// parameters ([`Config`]), apply it\n+    pub fn try_apply_core_parameter(&mut self, parameter: Parameter) {\n+        use iroha_data_model::parameter::default::*;\n+\n+        struct Reader(Option<Parameter>);\n+\n+        impl Reader {\n+            fn try_and_then<T: TryFrom<ParameterValueBox>>(\n+                self,\n+                id: &str,\n+                fun: impl FnOnce(T),\n+            ) -> Self {\n+                if let Some(param) = self.0 {\n+                    if param.id().name().as_ref() == id {\n+                        if let Some(value) = param.val.try_into().ok() {\n+                            fun(value);\n+                        }\n+                        Self(None)\n+                    } else {\n+                        Self(Some(param))\n+                    }\n+                } else {\n+                    Self(None)\n+                }\n+            }\n+\n+            fn try_and_write<T: TryFrom<ParameterValueBox>>(\n+                self,\n+                id: &str,\n+                destination: &mut T,\n+            ) -> Self {\n+                self.try_and_then(id, |value| {\n+                    *destination = value;\n+                })\n+            }\n+        }\n+\n+        Reader(Some(parameter))\n+            .try_and_then(MAX_TRANSACTIONS_IN_BLOCK, |value| {\n+                if let Some(checked) = NonZeroU32::new(value) {\n+                    self.config.max_transactions_in_block = checked;\n+                }\n+            })\n+            .try_and_then(BLOCK_TIME, |value| {\n+                self.config.block_time = Duration::from_millis(value);\n+            })\n+            .try_and_then(COMMIT_TIME_LIMIT, |value| {\n+                self.config.commit_time = Duration::from_millis(value);\n+            })\n+            .try_and_write(\n+                WSV_DOMAIN_METADATA_LIMITS,\n+                &mut self.config.domain_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_ASSET_DEFINITION_METADATA_LIMITS,\n+                &mut self.config.asset_definition_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_ACCOUNT_METADATA_LIMITS,\n+                &mut self.config.account_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_ASSET_METADATA_LIMITS,\n+                &mut self.config.asset_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_TRIGGER_METADATA_LIMITS,\n+                &mut self.config.trigger_metadata_limits,\n+            )\n+            .try_and_write(\n+                WSV_IDENT_LENGTH_LIMITS,\n+                &mut self.config.ident_length_limits,\n+            )\n+            .try_and_write(\n+                EXECUTOR_FUEL_LIMIT,\n+                &mut self.config.executor_runtime.fuel_limit,\n+            )\n+            .try_and_write(\n+                EXECUTOR_MAX_MEMORY,\n+                &mut self.config.executor_runtime.max_memory_bytes,\n+            )\n+            .try_and_write(WASM_FUEL_LIMIT, &mut self.config.wasm_runtime.fuel_limit)\n+            .try_and_write(\n+                WASM_MAX_MEMORY,\n+                &mut self.config.wasm_runtime.max_memory_bytes,\n+            )\n+            .try_and_write(TRANSACTION_LIMITS, &mut self.config.transaction_limits);\n+    }\n+\n     fn process_executable(&mut self, executable: &Executable, authority: AccountId) -> Result<()> {\n         match executable {\n             Executable::Instructions(instructions) => {\n\n@@ -359,20 +359,9 @@ impl Sumeragi {\n     }\n \n     fn update_params(&mut self, state_block: &StateBlock<'_>) {\n-        use iroha_data_model::parameter::default::*;\n-\n-        if let Some(block_time) = state_block.world.query_param(BLOCK_TIME) {\n-            self.block_time = Duration::from_millis(block_time);\n-        }\n-        if let Some(commit_time) = state_block.world.query_param(COMMIT_TIME_LIMIT) {\n-            self.commit_time = Duration::from_millis(commit_time);\n-        }\n-        if let Some(max_txs_in_block) = state_block\n-            .world\n-            .query_param::<u32, _>(MAX_TRANSACTIONS_IN_BLOCK)\n-        {\n-            self.max_txs_in_block = max_txs_in_block as usize;\n-        }\n+        self.block_time = state_block.config.block_time;\n+        self.commit_time = state_block.config.commit_time;\n+        self.max_txs_in_block = state_block.config.max_transactions_in_block.get() as usize;\n     }\n \n     fn cache_transaction(&mut self, state_block: &StateBlock<'_>) {\n\n@@ -127,9 +127,14 @@ impl BlockHeader {\n     }\n \n     /// Creation timestamp\n-    pub fn timestamp(&self) -> Duration {\n+    pub const fn timestamp(&self) -> Duration {\n         Duration::from_millis(self.timestamp_ms)\n     }\n+\n+    /// Consensus estimation\n+    pub const fn consensus_estimation(&self) -> Duration {\n+        Duration::from_millis(self.consensus_estimation_ms)\n+    }\n }\n \n impl SignedBlockV1 {\n"}
{"id": 55, "ground_truth": "Unordered data", "predicted": "Randomness", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nI've been experiencing a significant increase in flaky tests recently. I'm guessing this is due to the `FindNodes` test added to `peertest`. Would be curious to get confirmation from others. But, this has also raised an issue that when CI runs peertest, if there is a failure, then it doesn't display the error message, it simply just says that the test run failed ([example](https://app.circleci.com/pipelines/github/ethereum/trin/1267/workflows/11b900c3-b67e-40d7-9de7-9c516303c714/jobs/2473)). This makes it more difficult to diagnose the source of flakiness, so we should update how the tests are run to make sure that an error message / stack trace is displayed if it occurs.\n\n### Code Patch:\n@@ -45,7 +45,7 @@ jobs:\n                 command: cargo build --workspace --jobs 2\n             - run:\n                 name: Test Trin workspace\n-                command: cargo test --workspace --jobs 2\n+                command: cargo test --workspace --jobs 2 -- --nocapture\n             - save_cache:\n                 key: cargo-{{ checksum \"Cargo.lock\" }}-v1\n                 paths:\n@@ -88,7 +88,7 @@ jobs:\n           command: cargo build --target x86_64-pc-windows-msvc\n       - run:\n           name: Cargo Test --target x86_64-pc-windows-msvc\n-          command: cargo test --target x86_64-pc-windows-msvc\n+          command: cargo test --target x86_64-pc-windows-msvc -- --nocapture\n       - save_cache:\n           key: cargo-{{ checksum \"Cargo.lock\" }}-v1\n           paths:\n\n@@ -34,6 +34,11 @@ pub async fn launch_node(id: u16, bootnode_enr: Option<&String>) -> anyhow::Resu\n     let discovery_port: u16 = 9000 + id;\n     let discovery_port: String = discovery_port.to_string();\n     let web3_ipc_path = format!(\"/tmp/ethportal-peertest-buddy-{id}.ipc\");\n+    // This specific private key scheme is chosen to enforce that the first peer node will be in\n+    // the 256 kbucket of the bootnode, to ensure consistent `FindNodes` tests.\n+    let mut private_key = vec![id as u8; 3];\n+    private_key.append(&mut vec![0u8; 29]);\n+    let private_key = hex::encode(private_key);\n     let trin_config_args: Vec<&str> = match bootnode_enr {\n         Some(enr) => vec![\n             \"trin\",\n@@ -44,6 +49,8 @@ pub async fn launch_node(id: u16, bootnode_enr: Option<&String>) -> anyhow::Resu\n             discovery_port.as_str(),\n             \"--web3-ipc-path\",\n             web3_ipc_path.as_str(),\n+            \"--unsafe-private-key\",\n+            private_key.as_str(),\n         ],\n         None => vec![\n             \"trin\",\n@@ -52,6 +59,8 @@ pub async fn launch_node(id: u16, bootnode_enr: Option<&String>) -> anyhow::Resu\n             discovery_port.as_str(),\n             \"--web3-ipc-path\",\n             web3_ipc_path.as_str(),\n+            \"--unsafe-private-key\",\n+            private_key.as_str(),\n         ],\n     };\n     let trin_config = TrinConfig::new_from(trin_config_args.iter()).unwrap();\n\n@@ -172,6 +172,7 @@ impl TryFrom<[&Value; 2]> for FindNodesParams {\n     }\n }\n \n+#[derive(Debug)]\n pub struct NodesParams {\n     pub total: u8,\n     pub enrs: Vec<SszEnr>,\n"}
{"id": 1, "ground_truth": "Logic", "predicted": "Logic", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n## 🐛 Bug Report\r\n\r\nThe following error likes to pop up in the CircleCI `tests` job from time to time:\r\n\r\n```\r\nrunning 1 test\r\ntest helpers::block_requests::tests::test_block_requests_case_2cbb ... FAILED\r\n\r\nfailures:\r\n\r\n---- helpers::block_requests::tests::test_block_requests_case_2cbb stdout ----\r\nthread 'helpers::block_requests::tests::test_block_requests_case_2cbb' panicked at 'assertion failed: `(left == right)`\r\n  left: `AbortAndDisconnect(TwoCBA, ExceededForkRange)`,\r\n right: `Proceed(TwoCBB, BlockRequestHandlerProceed { start_block_height: 966, end_block_height: 1215, ledger_is_on_fork: true })`', src/helpers/block_requests.rs:572:13\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n\r\n\r\nfailures:\r\n    helpers::block_requests::tests::test_block_requests_case_2cbb\r\n```\r\n\r\nIt's likely an off-by one in the test, but I'm not sure.\n\n### Code Patch:\n@@ -209,7 +209,7 @@ pub(crate) fn handle_block_requests<N: Network, E: Environment>(\n             else if let Some(first_deviating_locator) = first_deviating_locator {\n                 // Case 2(c)(b)(a) - Check if the real common ancestor is NOT within `ALEO_MAXIMUM_FORK_DEPTH`.\n                 // If this peer is outside of the fork range of this ledger, proceed to disconnect from the peer.\n-                if latest_block_height.saturating_sub(first_deviating_locator) >= N::ALEO_MAXIMUM_FORK_DEPTH {\n+                if latest_block_height.saturating_sub(first_deviating_locator) > N::ALEO_MAXIMUM_FORK_DEPTH {\n                     debug!(\"Peer {} exceeded the permitted fork range, disconnecting\", maximal_peer);\n                     return BlockRequestHandler::AbortAndDisconnect(Case::TwoCBA, DisconnectReason::ExceededForkRange);\n                 }\n"}
{"id": 60, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Is your feature request related to a problem?\n\nhttps://github.com/xmtp/libxmtp/blob/75ee4aeb869d66340226893ff98d8daf188166ff/bindings_ffi/src/mls.rs#L1044\r\n\r\nTest is flaking b/c the streamed messages are arriving too slow to one end of the `stream`. Calls to `tokio::sleep` are supposed to give time for messages to arrive but depending on architecture/system speed this varies. \r\n\r\n\n\n### Describe the solution to the problem\n\nIt would be great to use `async-barrier` for these tests in order to guarantee sync points and avoid flakiness\n\n### Describe the uses cases for the feature\n\n_No response_\n\n### Additional details\n\n_No response_\n\n### Code Patch:\n@@ -2582,6 +2582,15 @@ version = \"0.4.21\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"90ed8c1e510134f979dbc4f070f87d4313098b704861a105fe34231c70a3901c\"\n \n+[[package]]\n+name = \"matchers\"\n+version = \"0.1.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"8263075bb86c5a1b1427b5ae862e8889656f126e9f77c484496e8b47cf5c5558\"\n+dependencies = [\n+ \"regex-automata 0.1.10\",\n+]\n+\n [[package]]\n name = \"matchit\"\n version = \"0.7.3\"\n@@ -2706,6 +2715,16 @@ dependencies = [\n  \"minimal-lexical\",\n ]\n \n+[[package]]\n+name = \"nu-ansi-term\"\n+version = \"0.46.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"77a8165726e8236064dbb45459242600304b42a5ea24ee2948e18e023bf7ba84\"\n+dependencies = [\n+ \"overload\",\n+ \"winapi\",\n+]\n+\n [[package]]\n name = \"num-bigint\"\n version = \"0.4.4\"\n@@ -2970,6 +2989,12 @@ version = \"0.2.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"04744f49eae99ab78e0d5c0b603ab218f515ea8cfe5a456d7629ad883a3b6e7d\"\n \n+[[package]]\n+name = \"overload\"\n+version = \"0.1.1\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"b15813163c1d831bf4a13c3610c05c0d03b39feb07f7e09fa234dac9b15aaf39\"\n+\n [[package]]\n name = \"p256\"\n version = \"0.13.2\"\n@@ -3707,10 +3732,19 @@ checksum = \"c117dbdfde9c8308975b6a18d71f3f385c89461f7b3fb054288ecf2a2058ba4c\"\n dependencies = [\n  \"aho-corasick\",\n  \"memchr\",\n- \"regex-automata\",\n+ \"regex-automata 0.4.4\",\n  \"regex-syntax 0.8.2\",\n ]\n \n+[[package]]\n+name = \"regex-automata\"\n+version = \"0.1.10\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"6c230d73fb8d8c1b9c0b3135c5142a8acee3a0558fb8db5cf1cb65f8d7862132\"\n+dependencies = [\n+ \"regex-syntax 0.6.29\",\n+]\n+\n [[package]]\n name = \"regex-automata\"\n version = \"0.4.4\"\n@@ -3722,6 +3756,12 @@ dependencies = [\n  \"regex-syntax 0.8.2\",\n ]\n \n+[[package]]\n+name = \"regex-syntax\"\n+version = \"0.6.29\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"f162c6dd7b008981e4d40210aca20b4bd0f9b60ca9271061b07f78537722f2e1\"\n+\n [[package]]\n name = \"regex-syntax\"\n version = \"0.7.5\"\n@@ -4349,6 +4389,15 @@ dependencies = [\n  \"keccak\",\n ]\n \n+[[package]]\n+name = \"sharded-slab\"\n+version = \"0.1.7\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"f40ca3c46823713e0d4209592e8d6e826aa57e928f09752619fc696c499637f6\"\n+dependencies = [\n+ \"lazy_static\",\n+]\n+\n [[package]]\n name = \"signal-hook-registry\"\n version = \"1.4.1\"\n@@ -4673,6 +4722,16 @@ dependencies = [\n  \"winapi\",\n ]\n \n+[[package]]\n+name = \"thread_local\"\n+version = \"1.1.8\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"8b9ef9bad013ada3808854ceac7b46812a6465ba368859a37e2100283d2d719c\"\n+dependencies = [\n+ \"cfg-if\",\n+ \"once_cell\",\n+]\n+\n [[package]]\n name = \"time\"\n version = \"0.3.31\"\n@@ -5051,6 +5110,7 @@ source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"c06d3da6113f116aaee68e4d601191614c9053067f9ab7f6edbcb161237daa54\"\n dependencies = [\n  \"once_cell\",\n+ \"valuable\",\n ]\n \n [[package]]\n@@ -5063,6 +5123,35 @@ dependencies = [\n  \"tracing\",\n ]\n \n+[[package]]\n+name = \"tracing-log\"\n+version = \"0.2.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"ee855f1f400bd0e5c02d150ae5de3840039a3f54b025156404e34c23c03f47c3\"\n+dependencies = [\n+ \"log\",\n+ \"once_cell\",\n+ \"tracing-core\",\n+]\n+\n+[[package]]\n+name = \"tracing-subscriber\"\n+version = \"0.3.18\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"ad0f048c97dbd9faa9b7df56362b8ebcaa52adb06b498c050d2f4e32f90a7a8b\"\n+dependencies = [\n+ \"matchers\",\n+ \"nu-ansi-term\",\n+ \"once_cell\",\n+ \"regex\",\n+ \"sharded-slab\",\n+ \"smallvec\",\n+ \"thread_local\",\n+ \"tracing\",\n+ \"tracing-core\",\n+ \"tracing-log\",\n+]\n+\n [[package]]\n name = \"try-lock\"\n version = \"0.2.5\"\n@@ -5359,6 +5448,12 @@ dependencies = [\n  \"rand\",\n ]\n \n+[[package]]\n+name = \"valuable\"\n+version = \"0.1.0\"\n+source = \"registry+https://github.com/rust-lang/crates.io-index\"\n+checksum = \"830b7e5d4d90034032940e4ace0d9a9a057e7a45cd94e6c007832e39edb82f6d\"\n+\n [[package]]\n name = \"vcpkg\"\n version = \"0.2.15\"\n@@ -5903,6 +5998,7 @@ dependencies = [\n  \"thiserror\",\n  \"tls_codec 0.4.0\",\n  \"tokio\",\n+ \"tokio-stream\",\n  \"toml 0.8.8\",\n  \"tracing\",\n  \"xmtp_cryptography\",\n@@ -5970,6 +6066,7 @@ dependencies = [\n  \"thread-id\",\n  \"tokio\",\n  \"tokio-test\",\n+ \"tracing-subscriber\",\n  \"uniffi\",\n  \"uniffi_macros\",\n  \"uuid 1.9.1\",\n\n@@ -23,6 +23,7 @@ xmtp_proto = { path = \"../xmtp_proto\", features = [\"proto_full\", \"grpc\"] }\n xmtp_user_preferences = { path = \"../xmtp_user_preferences\" }\n xmtp_v2 = { path = \"../xmtp_v2\" }\n \n+tracing-subscriber = { version = \"0.3\", features = [\"env-filter\"] }\n # NOTE: A regression in openssl-sys exists where libatomic is dynamically linked \n # for i686-linux-android targets. https://github.com/sfackler/rust-openssl/issues/2163\n #\n@@ -46,6 +47,7 @@ tempfile = \"3.5.0\"\n tokio = { version = \"1.28.1\", features = [\"full\"] }\n tokio-test = \"0.4\"\n uniffi = { version = \"0.27.2\", features = [\"bindgen-tests\"] }\n+tracing-subscriber = \"0.3\"\n uuid = { version = \"1.9\", features = [\"v4\", \"fast-rng\" ] }\n \n # NOTE: The release profile reduces bundle size from 230M to 41M - may have performance impliciations\n\n@@ -4,11 +4,8 @@ use crate::logger::FfiLogger;\n use crate::GenericError;\n use std::collections::HashMap;\n use std::convert::TryInto;\n-use std::sync::{\n-    atomic::{AtomicBool, Ordering},\n-    Arc, Mutex,\n-};\n-use tokio::sync::oneshot::Sender;\n+use std::sync::Arc;\n+use tokio::{sync::Mutex, task::AbortHandle};\n use xmtp_api_grpc::grpc_api_helper::Client as TonicApiClient;\n use xmtp_id::{\n     associations::{\n@@ -32,6 +29,7 @@ use xmtp_mls::{\n     api::ApiClientWrapper,\n     builder::ClientBuilder,\n     client::Client as MlsClient,\n+    client::ClientError,\n     groups::{\n         group_metadata::{ConversationType, GroupMetadata},\n         group_permissions::GroupMutablePermissions,\n@@ -44,6 +42,7 @@ use xmtp_mls::{\n         group_message::{DeliveryStatus, GroupMessageKind, StoredGroupMessage},\n         EncryptedMessageStore, EncryptionKey, StorageOption,\n     },\n+    subscriptions::StreamHandle,\n };\n \n pub type RustXmtpClient = MlsClient<TonicApiClient>;\n@@ -175,8 +174,7 @@ pub fn generate_inbox_id(account_address: String, nonce: u64) -> String {\n \n #[derive(uniffi::Object)]\n pub struct FfiSignatureRequest {\n-    // Using `tokio::sync::Mutex` bc rust MutexGuard cannot be sent between threads.\n-    inner: Arc<tokio::sync::Mutex<SignatureRequest>>,\n+    inner: Arc<Mutex<SignatureRequest>>,\n }\n \n #[uniffi::export(async_runtime = \"tokio\")]\n@@ -305,7 +303,7 @@ impl FfiXmtpClient {\n             .signature_request()\n             .map(|request| {\n                 Arc::new(FfiSignatureRequest {\n-                    inner: Arc::new(tokio::sync::Mutex::new(request)),\n+                    inner: Arc::new(Mutex::new(request)),\n                 })\n             })\n     }\n@@ -616,43 +614,30 @@ impl FfiConversations {\n         Ok(convo_list)\n     }\n \n-    pub async fn stream(\n-        &self,\n-        callback: Box<dyn FfiConversationCallback>,\n-    ) -> Result<Arc<FfiStreamCloser>, GenericError> {\n+    pub fn stream(&self, callback: Box<dyn FfiConversationCallback>) -> FfiStreamCloser {\n         let client = self.inner_client.clone();\n-        let stream_closer = RustXmtpClient::stream_conversations_with_callback(\n-            client.clone(),\n-            move |convo| {\n+        let handle =\n+            RustXmtpClient::stream_conversations_with_callback(client.clone(), move |convo| {\n                 callback.on_conversation(Arc::new(FfiGroup {\n                     inner_client: client.clone(),\n                     group_id: convo.group_id,\n                     created_at_ns: convo.created_at_ns,\n                 }))\n-            },\n-            || {}, // on_close_callback\n-        )?;\n+            });\n \n-        Ok(Arc::new(FfiStreamCloser {\n-            close_fn: stream_closer.close_fn,\n-            is_closed_atomic: stream_closer.is_closed_atomic,\n-        }))\n+        FfiStreamCloser::new(handle)\n     }\n \n-    pub async fn stream_all_messages(\n+    pub fn stream_all_messages(\n         &self,\n         message_callback: Box<dyn FfiMessageCallback>,\n-    ) -> Result<Arc<FfiStreamCloser>, GenericError> {\n-        let stream_closer = RustXmtpClient::stream_all_messages_with_callback(\n+    ) -> FfiStreamCloser {\n+        let handle = RustXmtpClient::stream_all_messages_with_callback(\n             self.inner_client.clone(),\n             move |message| message_callback.on_message(message.into()),\n-        )\n-        .await?;\n+        );\n \n-        Ok(Arc::new(FfiStreamCloser {\n-            close_fn: stream_closer.close_fn,\n-            is_closed_atomic: stream_closer.is_closed_atomic,\n-        }))\n+        FfiStreamCloser::new(handle)\n     }\n }\n \n@@ -1131,22 +1116,16 @@ impl FfiGroup {\n         Ok(())\n     }\n \n-    pub async fn stream(\n-        &self,\n-        message_callback: Box<dyn FfiMessageCallback>,\n-    ) -> Result<Arc<FfiStreamCloser>, GenericError> {\n+    pub fn stream(&self, message_callback: Box<dyn FfiMessageCallback>) -> FfiStreamCloser {\n         let inner_client = Arc::clone(&self.inner_client);\n-        let stream_closer = MlsGroup::stream_with_callback(\n+        let handle = MlsGroup::stream_with_callback(\n             inner_client,\n             self.group_id.clone(),\n             self.created_at_ns,\n             move |message| message_callback.on_message(message.into()),\n-        )?;\n+        );\n \n-        Ok(Arc::new(FfiStreamCloser {\n-            close_fn: stream_closer.close_fn,\n-            is_closed_atomic: stream_closer.is_closed_atomic,\n-        }))\n+        FfiStreamCloser::new(handle)\n     }\n \n     pub fn created_at_ns(&self) -> i64 {\n@@ -1261,27 +1240,67 @@ impl From<StoredGroupMessage> for FfiMessage {\n     }\n }\n \n-#[derive(uniffi::Object)]\n+#[derive(uniffi::Object, Clone, Debug)]\n pub struct FfiStreamCloser {\n-    close_fn: Arc<Mutex<Option<Sender<()>>>>,\n-    is_closed_atomic: Arc<AtomicBool>,\n+    #[allow(clippy::type_complexity)]\n+    stream_handle: Arc<Mutex<Option<StreamHandle<Result<(), ClientError>>>>>,\n+    // for convenience, does not require locking mutex.\n+    abort_handle: Arc<AbortHandle>,\n+}\n+\n+impl FfiStreamCloser {\n+    pub fn new(stream_handle: StreamHandle<Result<(), ClientError>>) -> Self {\n+        Self {\n+            abort_handle: Arc::new(stream_handle.handle.abort_handle()),\n+            stream_handle: Arc::new(Mutex::new(Some(stream_handle))),\n+        }\n+    }\n+\n+    #[cfg(test)]\n+    pub async fn wait_for_ready(&self) {\n+        let mut handle = self.stream_handle.lock().await;\n+        if let Some(ref mut h) = &mut *handle {\n+            h.wait_for_ready().await;\n+        }\n+    }\n }\n \n #[uniffi::export]\n impl FfiStreamCloser {\n+    /// Signal the stream to end\n+    /// Does not wait for the stream to end.\n     pub fn end(&self) {\n-        match self.close_fn.lock() {\n-            Ok(mut close_fn_option) => {\n-                let _ = close_fn_option.take().map(|close_fn| close_fn.send(()));\n-            }\n-            _ => {\n-                log::warn!(\"close_fn already closed\");\n+        self.abort_handle.abort();\n+    }\n+\n+    /// End the stream and asyncronously wait for it to shutdown\n+    pub async fn end_and_wait(&self) -> Result<(), GenericError> {\n+        if self.abort_handle.is_finished() {\n+            return Ok(());\n+        }\n+\n+        let mut stream_handle = self.stream_handle.lock().await;\n+        let stream_handle = stream_handle.take();\n+        if let Some(h) = stream_handle {\n+            h.handle.abort();\n+            match h.handle.await {\n+                Err(e) if !e.is_cancelled() => Err(GenericError::Generic {\n+                    err: format!(\"subscription event loop join error {}\", e),\n+                }),\n+                Err(e) if e.is_cancelled() => Ok(()),\n+                Ok(t) => t.map_err(|e| GenericError::Generic { err: e.to_string() }),\n+                Err(e) => Err(GenericError::Generic {\n+                    err: format!(\"error joining task {}\", e),\n+                }),\n             }\n+        } else {\n+            log::warn!(\"subscription already closed\");\n+            Ok(())\n         }\n     }\n \n     pub fn is_closed(&self) -> bool {\n-        self.is_closed_atomic.load(Ordering::Relaxed)\n+        self.abort_handle.is_finished()\n     }\n }\n \n@@ -1360,15 +1379,15 @@ impl FfiGroupPermissions {\n mod tests {\n     use crate::{\n         get_inbox_id_for_address, inbox_owner::SigningError, logger::FfiLogger,\n-        FfiConversationCallback, FfiCreateGroupOptions, FfiGroupPermissionsOptions, FfiInboxOwner,\n-        FfiListConversationsOptions, FfiListMessagesOptions, FfiMetadataField, FfiPermissionPolicy,\n-        FfiPermissionPolicySet, FfiPermissionUpdateType,\n+        FfiConversationCallback, FfiCreateGroupOptions, FfiGroup, FfiGroupPermissionsOptions,\n+        FfiInboxOwner, FfiListConversationsOptions, FfiListMessagesOptions, FfiMetadataField,\n+        FfiPermissionPolicy, FfiPermissionPolicySet, FfiPermissionUpdateType,\n     };\n     use std::{\n         env,\n         sync::{\n             atomic::{AtomicU32, Ordering},\n-            Arc,\n+            Arc, Mutex,\n         },\n     };\n \n@@ -1378,6 +1397,7 @@ mod tests {\n         self,\n         distributions::{Alphanumeric, DistString},\n     };\n+    use tokio::{sync::Notify, time::error::Elapsed};\n     use xmtp_cryptography::{signature::RecoverableSignature, utils::rng};\n     use xmtp_id::associations::generate_inbox_id;\n     use xmtp_mls::{storage::EncryptionKey, InboxOwner};\n@@ -1417,36 +1437,48 @@ mod tests {\n         }\n     }\n \n-    #[derive(Clone)]\n+    #[derive(Default, Clone)]\n     struct RustStreamCallback {\n         num_messages: Arc<AtomicU32>,\n+        messages: Arc<Mutex<Vec<FfiMessage>>>,\n+        conversations: Arc<Mutex<Vec<Arc<FfiGroup>>>>,\n+        notify: Arc<Notify>,\n     }\n \n     impl RustStreamCallback {\n-        pub fn new() -> Self {\n-            Self {\n-                num_messages: Arc::new(AtomicU32::new(0)),\n-            }\n-        }\n-\n         pub fn message_count(&self) -> u32 {\n             self.num_messages.load(Ordering::SeqCst)\n         }\n+\n+        pub async fn wait_for_delivery(&self) -> Result<(), Elapsed> {\n+            tokio::time::timeout(std::time::Duration::from_secs(60), async {\n+                self.notify.notified().await\n+            })\n+            .await?;\n+            Ok(())\n+        }\n     }\n \n     impl FfiMessageCallback for RustStreamCallback {\n         fn on_message(&self, message: FfiMessage) {\n-            println!(\"Got a message\");\n-            let message = String::from_utf8(message.content).unwrap_or(\"<not UTF8>\".to_string());\n-            log::info!(\"Received: {}\", message);\n+            let mut messages = self.messages.lock().unwrap();\n+            log::info!(\n+                \"ON MESSAGE Received\\n-------- \\n{}\\n----------\",\n+                String::from_utf8_lossy(&message.content)\n+            );\n+            messages.push(message);\n             let _ = self.num_messages.fetch_add(1, Ordering::SeqCst);\n+            self.notify.notify_one();\n         }\n     }\n \n     impl FfiConversationCallback for RustStreamCallback {\n-        fn on_conversation(&self, _: Arc<super::FfiGroup>) {\n-            println!(\"received new conversation\");\n+        fn on_conversation(&self, group: Arc<super::FfiGroup>) {\n+            log::debug!(\"received conversation\");\n             let _ = self.num_messages.fetch_add(1, Ordering::SeqCst);\n+            let mut convos = self.conversations.lock().unwrap();\n+            convos.push(group);\n+            self.notify.notify_one();\n         }\n     }\n \n@@ -1974,13 +2006,11 @@ mod tests {\n         let bo = new_test_client().await;\n \n         // Stream all group messages\n-        let message_callbacks = RustStreamCallback::new();\n+        let message_callbacks = RustStreamCallback::default();\n         let stream_messages = bo\n             .conversations()\n-            .stream_all_messages(Box::new(message_callbacks.clone()))\n-            .await\n-            .unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+            .stream_all_messages(Box::new(message_callbacks.clone()));\n+        stream_messages.wait_for_ready().await;\n \n         // Create group and send first message\n         let alix_group = alix\n@@ -1992,12 +2022,11 @@ mod tests {\n             .await\n             .unwrap();\n \n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n-\n         alix_group\n             .update_group_name(\"Old Name\".to_string())\n             .await\n             .unwrap();\n+        message_callbacks.wait_for_delivery().await.unwrap();\n \n         let bo_groups = bo\n             .conversations()\n@@ -2010,41 +2039,35 @@ mod tests {\n             .update_group_name(\"Old Name2\".to_string())\n             .await\n             .unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n+        message_callbacks.wait_for_delivery().await.unwrap();\n \n         // Uncomment the following lines to add more group name updates\n-        // alix_group.update_group_name(\"Again Name\".to_string()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n-\n         bo_group\n-            .update_group_name(\"Old Name2\".to_string())\n+            .update_group_name(\"Old Name3\".to_string())\n             .await\n             .unwrap();\n+        message_callbacks.wait_for_delivery().await.unwrap();\n \n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n         assert_eq!(message_callbacks.message_count(), 3);\n \n-        stream_messages.end();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(5)).await;\n+        stream_messages.end_and_wait().await.unwrap();\n+\n         assert!(stream_messages.is_closed());\n     }\n \n     // test is also showing intermittent failures with database locked msg\n-    #[tokio::test(flavor = \"multi_thread\", worker_threads = 5)]\n     #[ignore]\n+    #[tokio::test(flavor = \"multi_thread\", worker_threads = 5)]\n     async fn test_can_stream_and_update_name_without_forking_group() {\n         let alix = new_test_client().await;\n         let bo = new_test_client().await;\n \n         // Stream all group messages\n-        let message_callbacks = RustStreamCallback::new();\n+        let message_callbacks = RustStreamCallback::default();\n         let stream_messages = bo\n             .conversations()\n-            .stream_all_messages(Box::new(message_callbacks.clone()))\n-            .await\n-            .unwrap();\n-\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+            .stream_all_messages(Box::new(message_callbacks.clone()));\n+        stream_messages.wait_for_ready().await;\n \n         let first_msg_check = 2;\n         let second_msg_check = 5;\n@@ -2059,16 +2082,13 @@ mod tests {\n             .await\n             .unwrap();\n \n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n-\n         alix_group\n             .update_group_name(\"hello\".to_string())\n             .await\n             .unwrap();\n+        message_callbacks.wait_for_delivery().await.unwrap();\n         alix_group.send(\"hello1\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n-        bo.conversations().sync().await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n+        message_callbacks.wait_for_delivery().await.unwrap();\n \n         let bo_groups = bo\n             .conversations()\n@@ -2085,49 +2105,41 @@ mod tests {\n         assert_eq!(bo_messages1.len(), first_msg_check);\n \n         bo_group.send(\"hello2\".as_bytes().to_vec()).await.unwrap();\n+        message_callbacks.wait_for_delivery().await.unwrap();\n         bo_group.send(\"hello3\".as_bytes().to_vec()).await.unwrap();\n+        message_callbacks.wait_for_delivery().await.unwrap();\n+\n         alix_group.sync().await.unwrap();\n \n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n         let alix_messages = alix_group\n             .find_messages(FfiListMessagesOptions::default())\n             .unwrap();\n         assert_eq!(alix_messages.len(), second_msg_check);\n \n         alix_group.send(\"hello4\".as_bytes().to_vec()).await.unwrap();\n+        message_callbacks.wait_for_delivery().await.unwrap();\n         bo_group.sync().await.unwrap();\n \n         let bo_messages2 = bo_group\n             .find_messages(FfiListMessagesOptions::default())\n             .unwrap();\n         assert_eq!(bo_messages2.len(), second_msg_check);\n+        assert_eq!(message_callbacks.message_count(), second_msg_check as u32);\n \n-        // TODO: message_callbacks should eventually come through here, why does this\n-        // not work?\n-        // tokio::time::sleep(tokio::time::Duration::from_millis(10000)).await;\n-        // assert_eq!(message_callbacks.message_count(), second_msg_check as u32);\n-\n-        stream_messages.end();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(5)).await;\n+        stream_messages.end_and_wait().await.unwrap();\n         assert!(stream_messages.is_closed());\n     }\n \n     #[tokio::test(flavor = \"multi_thread\", worker_threads = 5)]\n-    // This one is flaky for me. Passes reliably locally and fails on CI\n-    #[ignore]\n     async fn test_conversation_streaming() {\n         let amal = new_test_client().await;\n         let bola = new_test_client().await;\n \n-        let stream_callback = RustStreamCallback::new();\n+        let stream_callback = RustStreamCallback::default();\n \n         let stream = bola\n             .conversations()\n-            .stream(Box::new(stream_callback.clone()))\n-            .await\n-            .unwrap();\n-\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+            .stream(Box::new(stream_callback.clone()));\n \n         amal.conversations()\n             .create_group(\n@@ -2137,7 +2149,7 @@ mod tests {\n             .await\n             .unwrap();\n \n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n+        stream_callback.wait_for_delivery().await.unwrap();\n \n         assert_eq!(stream_callback.message_count(), 1);\n         // Create another group and add bola\n@@ -2148,12 +2160,11 @@ mod tests {\n             )\n             .await\n             .unwrap();\n+        stream_callback.wait_for_delivery().await.unwrap();\n \n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n         assert_eq!(stream_callback.message_count(), 2);\n \n-        stream.end();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(5)).await;\n+        stream.end_and_wait().await.unwrap();\n         assert!(stream.is_closed());\n     }\n \n@@ -2171,19 +2182,17 @@ mod tests {\n             )\n             .await\n             .unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n \n-        let stream_callback = RustStreamCallback::new();\n+        let stream_callback = RustStreamCallback::default();\n \n         let stream = caro\n             .conversations()\n-            .stream_all_messages(Box::new(stream_callback.clone()))\n-            .await\n-            .unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+            .stream_all_messages(Box::new(stream_callback.clone()));\n+        stream.wait_for_ready().await;\n \n         alix_group.send(\"first\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+        stream_callback.wait_for_delivery().await.unwrap();\n+\n         let bo_group = bo\n             .conversations()\n             .create_group(\n@@ -2192,27 +2201,26 @@ mod tests {\n             )\n             .await\n             .unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(200)).await;\n+        let _ = caro.inner_client.sync_welcomes().await.unwrap();\n+\n         bo_group.send(\"second\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+        stream_callback.wait_for_delivery().await.unwrap();\n         alix_group.send(\"third\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+        stream_callback.wait_for_delivery().await.unwrap();\n         bo_group.send(\"fourth\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(300)).await;\n+        stream_callback.wait_for_delivery().await.unwrap();\n \n         assert_eq!(stream_callback.message_count(), 4);\n-        stream.end();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(5)).await;\n+        stream.end_and_wait().await.unwrap();\n         assert!(stream.is_closed());\n     }\n \n-    #[tokio::test(flavor = \"multi_thread\", worker_threads = 5)]\n-    #[ignore]\n+    #[tokio::test(flavor = \"multi_thread\")]\n     async fn test_message_streaming() {\n         let amal = new_test_client().await;\n         let bola = new_test_client().await;\n \n-        let group = amal\n+        let amal_group: Arc<FfiGroup> = amal\n             .conversations()\n             .create_group(\n                 vec![bola.account_address.clone()],\n@@ -2221,19 +2229,25 @@ mod tests {\n             .await\n             .unwrap();\n \n-        let stream_callback = RustStreamCallback::new();\n-        let stream_closer = group\n-            .stream(Box::new(stream_callback.clone()))\n+        bola.inner_client.sync_welcomes().await.unwrap();\n+        let bola_group = bola.group(amal_group.group_id.clone()).unwrap();\n+\n+        let stream_callback = RustStreamCallback::default();\n+        let stream_closer = bola_group.stream(Box::new(stream_callback.clone()));\n+\n+        stream_closer.wait_for_ready().await;\n+\n+        amal_group.send(\"hello\".as_bytes().to_vec()).await.unwrap();\n+        stream_callback.wait_for_delivery().await.unwrap();\n+\n+        amal_group\n+            .send(\"goodbye\".as_bytes().to_vec())\n             .await\n             .unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(250)).await;\n+        stream_callback.wait_for_delivery().await.unwrap();\n \n-        group.send(\"hello\".as_bytes().to_vec()).await.unwrap();\n-        group.send(\"goodbye\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(250)).await;\n         assert_eq!(stream_callback.message_count(), 2);\n-\n-        stream_closer.end();\n+        stream_closer.end_and_wait().await.unwrap();\n     }\n \n     #[tokio::test(flavor = \"multi_thread\", worker_threads = 5)]\n@@ -2254,51 +2268,49 @@ mod tests {\n             )\n             .await\n             .unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n \n-        let stream_callback = RustStreamCallback::new();\n+        let stream_callback = RustStreamCallback::default();\n         let stream_closer = bola\n             .conversations()\n-            .stream_all_messages(Box::new(stream_callback.clone()))\n-            .await\n-            .unwrap();\n+            .stream_all_messages(Box::new(stream_callback.clone()));\n+        stream_closer.wait_for_ready().await;\n \n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+        amal_group.send(b\"hello1\".to_vec()).await.unwrap();\n+        stream_callback.wait_for_delivery().await.unwrap();\n+        amal_group.send(b\"hello2\".to_vec()).await.unwrap();\n+        stream_callback.wait_for_delivery().await.unwrap();\n \n-        amal_group.send(\"hello1\".as_bytes().to_vec()).await.unwrap();\n-        amal_group.send(\"hello2\".as_bytes().to_vec()).await.unwrap();\n-\n-        tokio::time::sleep(tokio::time::Duration::from_millis(200)).await;\n         assert_eq!(stream_callback.message_count(), 2);\n         assert!(!stream_closer.is_closed());\n \n         amal_group\n             .remove_members_by_inbox_id(vec![bola.inbox_id().clone()])\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(2000)).await;\n+        stream_callback.wait_for_delivery().await.unwrap();\n         assert_eq!(stream_callback.message_count(), 3); // Member removal transcript message\n-\n-        amal_group.send(\"hello3\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(200)).await;\n+                                                        //\n+        amal_group.send(b\"hello3\".to_vec()).await.unwrap();\n+        //TODO: could verify with a log message\n+        tokio::time::sleep(std::time::Duration::from_millis(200)).await;\n         assert_eq!(stream_callback.message_count(), 3); // Don't receive messages while removed\n         assert!(!stream_closer.is_closed());\n \n-        tokio::time::sleep(tokio::time::Duration::from_millis(200)).await;\n         amal_group\n             .add_members(vec![bola.account_address.clone()])\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(500)).await;\n+\n+        // TODO: could check for LOG message with a Eviction error on receive\n+        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n         assert_eq!(stream_callback.message_count(), 3); // Don't receive transcript messages while removed\n \n         amal_group.send(\"hello4\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(200)).await;\n+        stream_callback.wait_for_delivery().await.unwrap();\n         assert_eq!(stream_callback.message_count(), 4); // Receiving messages again\n         assert!(!stream_closer.is_closed());\n \n-        stream_closer.end();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(100)).await;\n+        stream_closer.end_and_wait().await.unwrap();\n         assert!(stream_closer.is_closed());\n     }\n \n@@ -2353,21 +2365,14 @@ mod tests {\n         let bo = new_test_client().await;\n \n         // Stream all group messages\n-        let message_callbacks = RustStreamCallback::new();\n-        let group_callbacks = RustStreamCallback::new();\n-        let stream_groups = bo\n-            .conversations()\n-            .stream(Box::new(group_callbacks.clone()))\n-            .await\n-            .unwrap();\n+        let message_callback = RustStreamCallback::default();\n+        let group_callback = RustStreamCallback::default();\n+        let stream_groups = bo.conversations().stream(Box::new(group_callback.clone()));\n \n         let stream_messages = bo\n             .conversations()\n-            .stream_all_messages(Box::new(message_callbacks.clone()))\n-            .await\n-            .unwrap();\n-\n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n+            .stream_all_messages(Box::new(message_callback.clone()));\n+        stream_messages.wait_for_ready().await;\n \n         // Create group and send first message\n         let alix_group = alix\n@@ -2378,19 +2383,18 @@ mod tests {\n             )\n             .await\n             .unwrap();\n+        group_callback.wait_for_delivery().await.unwrap();\n \n         alix_group.send(\"hello1\".as_bytes().to_vec()).await.unwrap();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(1000)).await;\n+        message_callback.wait_for_delivery().await.unwrap();\n \n-        assert_eq!(group_callbacks.message_count(), 1);\n-        assert_eq!(message_callbacks.message_count(), 1);\n+        assert_eq!(group_callback.message_count(), 1);\n+        assert_eq!(message_callback.message_count(), 1);\n \n-        stream_messages.end();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(5)).await;\n+        stream_messages.end_and_wait().await.unwrap();\n         assert!(stream_messages.is_closed());\n \n-        stream_groups.end();\n-        tokio::time::sleep(tokio::time::Duration::from_millis(5)).await;\n+        stream_groups.end_and_wait().await.unwrap();\n         assert!(stream_groups.is_closed());\n     }\n \n\n@@ -312,9 +312,15 @@ impl FfiV2Subscription {\n         let handle = handle.take();\n         if let Some(h) = handle {\n             h.abort();\n-            h.await.map_err(|_| GenericError::Generic {\n-                err: \"subscription event loop join error\".into(),\n-            })?;\n+            let join_result = h.await;\n+            if matches!(join_result, Err(ref e) if !e.is_cancelled()) {\n+                return Err(GenericError::Generic {\n+                    err: format!(\n+                        \"subscription event loop join error {}\",\n+                        join_result.unwrap_err()\n+                    ),\n+                });\n+            }\n         }\n         Ok(())\n     }\n\n@@ -300,6 +300,7 @@ version = \"0.0.1\"\n dependencies = [\n  \"futures\",\n  \"hex\",\n+ \"log\",\n  \"napi\",\n  \"napi-build\",\n  \"napi-derive\",\n\n@@ -25,6 +25,7 @@ xmtp_mls = { path = \"../xmtp_mls\", features = [\"grpc\", \"native\"] }\n xmtp_proto = { path = \"../xmtp_proto\", features = [\"proto_full\"] }\n xmtp_id = { path = \"../xmtp_id\" }\n rand = \"0.8.5\"\n+log = { version = \"0.4\", features = [\"release_max_level_debug\"] }\n \n [build-dependencies]\n napi-build = \"2.0.1\"\n\n@@ -186,9 +186,8 @@ impl NapiConversations {\n     let tsfn: ThreadsafeFunction<NapiGroup, ErrorStrategy::CalleeHandled> =\n       callback.create_threadsafe_function(0, |ctx| Ok(vec![ctx.value]))?;\n     let client = self.inner_client.clone();\n-    let stream_closer = RustXmtpClient::stream_conversations_with_callback(\n-      client.clone(),\n-      move |convo| {\n+    let stream_closer =\n+      RustXmtpClient::stream_conversations_with_callback(client.clone(), move |convo| {\n         tsfn.call(\n           Ok(NapiGroup::new(\n             client.clone(),\n@@ -197,32 +196,22 @@ impl NapiConversations {\n           )),\n           ThreadsafeFunctionCallMode::Blocking,\n         );\n-      },\n-      || {}, // on_close_callback\n-    )\n-    .map_err(|e| Error::from_reason(format!(\"{}\", e)))?;\n+      });\n \n-    Ok(NapiStreamCloser::new(\n-      stream_closer.close_fn,\n-      stream_closer.is_closed_atomic,\n-    ))\n+    Ok(NapiStreamCloser::new(stream_closer))\n   }\n \n   #[napi(ts_args_type = \"callback: (err: null | Error, result: NapiMessage) => void\")]\n   pub fn stream_all_messages(&self, callback: JsFunction) -> Result<NapiStreamCloser> {\n     let tsfn: ThreadsafeFunction<NapiMessage, ErrorStrategy::CalleeHandled> =\n       callback.create_threadsafe_function(0, |ctx| Ok(vec![ctx.value]))?;\n-    let stream_closer = RustXmtpClient::stream_all_messages_with_callback_sync(\n+    let stream_closer = RustXmtpClient::stream_all_messages_with_callback(\n       self.inner_client.clone(),\n       move |message| {\n         tsfn.call(Ok(message.into()), ThreadsafeFunctionCallMode::Blocking);\n       },\n-    )\n-    .map_err(|e| Error::from_reason(format!(\"{}\", e)))?;\n+    );\n \n-    Ok(NapiStreamCloser::new(\n-      stream_closer.close_fn,\n-      stream_closer.is_closed_atomic,\n-    ))\n+    Ok(NapiStreamCloser::new(stream_closer))\n   }\n }\n\n@@ -562,13 +562,9 @@ impl NapiGroup {\n       move |message| {\n         tsfn.call(Ok(message.into()), ThreadsafeFunctionCallMode::Blocking);\n       },\n-    )\n-    .map_err(|e| Error::from_reason(format!(\"{}\", e)))?;\n+    );\n \n-    Ok(NapiStreamCloser::new(\n-      stream_closer.close_fn,\n-      stream_closer.is_closed_atomic,\n-    ))\n+    Ok(stream_closer.into())\n   }\n \n   #[napi]\n\n@@ -1,35 +1,73 @@\n-use std::sync::{\n-  atomic::{AtomicBool, Ordering},\n-  Arc, Mutex,\n-};\n-use tokio::sync::oneshot::Sender;\n+use napi::bindgen_prelude::Error;\n+use std::sync::Arc;\n+use tokio::{sync::Mutex, task::AbortHandle};\n+use xmtp_mls::{client::ClientError, subscriptions::StreamHandle};\n \n use napi_derive::napi;\n \n #[napi]\n pub struct NapiStreamCloser {\n-  close_fn: Arc<Mutex<Option<Sender<()>>>>,\n-  is_closed_atomic: Arc<AtomicBool>,\n+  #[allow(clippy::type_complexity)]\n+  handle: Arc<Mutex<Option<StreamHandle<Result<(), ClientError>>>>>,\n+  // for convenience, does not require locking mutex.\n+  abort_handle: Arc<AbortHandle>,\n }\n \n-#[napi]\n impl NapiStreamCloser {\n-  pub fn new(close_fn: Arc<Mutex<Option<Sender<()>>>>, is_closed_atomic: Arc<AtomicBool>) -> Self {\n+  pub fn new(handle: StreamHandle<Result<(), ClientError>>) -> Self {\n     Self {\n-      close_fn,\n-      is_closed_atomic,\n+      abort_handle: Arc::new(handle.handle.abort_handle()),\n+      handle: Arc::new(Mutex::new(Some(handle))),\n     }\n   }\n+}\n+\n+impl From<StreamHandle<Result<(), ClientError>>> for NapiStreamCloser {\n+  fn from(handle: StreamHandle<Result<(), ClientError>>) -> Self {\n+    NapiStreamCloser::new(handle)\n+  }\n+}\n \n+#[napi]\n+impl NapiStreamCloser {\n+  /// Signal the stream to end\n+  /// Does not wait for the stream to end.\n   #[napi]\n   pub fn end(&self) {\n-    if let Ok(mut close_fn_option) = self.close_fn.lock() {\n-      let _ = close_fn_option.take().map(|close_fn| close_fn.send(()));\n+    self.abort_handle.abort();\n+  }\n+\n+  /// End the stream and `await` for it to shutdown\n+  /// Returns the `Result` of the task.\n+  #[napi]\n+  /// End the stream and asyncronously wait for it to shutdown\n+  pub async fn end_and_wait(&self) -> Result<(), Error> {\n+    if self.abort_handle.is_finished() {\n+      return Ok(());\n+    }\n+\n+    let mut stream_handle = self.handle.lock().await;\n+    let stream_handle = stream_handle.take();\n+    if let Some(h) = stream_handle {\n+      h.handle.abort();\n+      match h.handle.await {\n+        Err(e) if !e.is_cancelled() => Err(Error::from_reason(format!(\n+          \"subscription event loop join error {}\",\n+          e\n+        ))),\n+        Err(e) if e.is_cancelled() => Ok(()),\n+        Ok(t) => t.map_err(|e| Error::from_reason(e.to_string())),\n+        Err(e) => Err(Error::from_reason(format!(\"error joining task {}\", e))),\n+      }\n+    } else {\n+      log::warn!(\"subscription already closed\");\n+      Ok(())\n     }\n   }\n \n+  /// Checks if this stream is closed\n   #[napi]\n   pub fn is_closed(&self) -> bool {\n-    self.is_closed_atomic.load(Ordering::Relaxed)\n+    self.abort_handle.is_finished()\n   }\n }\n\n@@ -71,6 +71,8 @@ flume = \"0.11\"\n mockall = \"0.11.4\"\n mockito = \"1.4.0\"\n tempfile = \"3.5.0\"\n+tracing.workspace = true\n+tracing-subscriber.workspace = true\n tracing-log = \"0.2.0\"\n tracing-test = \"0.2.4\"\n xmtp_api_grpc = { path = \"../xmtp_api_grpc\" }\n\n@@ -5,11 +5,10 @@ use std::sync::Arc;\n use futures::Stream;\n \n use super::{extract_message_v1, GroupError, MlsGroup};\n-use crate::retry::Retry;\n use crate::storage::group_message::StoredGroupMessage;\n-use crate::subscriptions::{MessagesStreamInfo, StreamCloser};\n+use crate::subscriptions::{MessagesStreamInfo, StreamHandle};\n use crate::XmtpApi;\n-use crate::{retry_async, Client};\n+use crate::{retry::Retry, retry_async, Client};\n use prost::Message;\n use xmtp_proto::xmtp::mls::api::v1::GroupMessage;\n \n@@ -66,8 +65,10 @@ impl MlsGroup {\n             })\n         );\n \n-        if let Some(GroupError::ReceiveError(_)) = process_result.err() {\n+        if let Some(GroupError::ReceiveError(_)) = process_result.as_ref().err() {\n             self.sync(&client).await?;\n+        } else if process_result.is_err() {\n+            log::error!(\"Process stream entry {:?}\", process_result.err());\n         }\n \n         // Load the message from the DB to handle cases where it may have been already processed in\n@@ -119,11 +120,11 @@ impl MlsGroup {\n         group_id: Vec<u8>,\n         created_at_ns: i64,\n         callback: impl FnMut(StoredGroupMessage) + Send + 'static,\n-    ) -> Result<StreamCloser, GroupError>\n+    ) -> StreamHandle<Result<(), crate::groups::ClientError>>\n     where\n         ApiClient: crate::XmtpApi,\n     {\n-        Ok(Client::<ApiClient>::stream_messages_with_callback(\n+        Client::<ApiClient>::stream_messages_with_callback(\n             client,\n             HashMap::from([(\n                 group_id,\n@@ -133,7 +134,7 @@ impl MlsGroup {\n                 },\n             )]),\n             callback,\n-        )?)\n+        )\n     }\n }\n \n\n@@ -762,6 +762,11 @@ impl MlsGroup {\n                 sha256(payload_slice),\n                 post_commit_data,\n             )?;\n+            log::debug!(\n+                \"client [{}] set stored intent [{}] to state `published`\",\n+                client.inbox_id(),\n+                intent.id\n+            );\n         }\n \n         Ok(())\n\n@@ -1,16 +1,12 @@\n-use std::{\n-    collections::HashMap,\n-    pin::Pin,\n-    sync::{\n-        atomic::{AtomicBool, Ordering},\n-        Arc, Mutex,\n-    },\n-};\n+use std::{collections::HashMap, pin::Pin, sync::Arc};\n \n-use futures::{Stream, StreamExt};\n+use futures::{FutureExt, Stream, StreamExt};\n use prost::Message;\n-use tokio::sync::oneshot::{self, Sender};\n-use tokio_stream::wrappers::errors::BroadcastStreamRecvError;\n+use tokio::{\n+    sync::{mpsc, oneshot},\n+    task::JoinHandle,\n+};\n+use tokio_stream::wrappers::{errors::BroadcastStreamRecvError, UnboundedReceiverStream};\n use xmtp_proto::xmtp::mls::api::v1::WelcomeMessage;\n \n use crate::{\n@@ -19,10 +15,18 @@ use crate::{\n     groups::{extract_group_id, GroupError, MlsGroup},\n     retry::Retry,\n     retry_async,\n-    storage::group_message::StoredGroupMessage,\n+    storage::{group::StoredGroup, group_message::StoredGroupMessage},\n     Client, XmtpApi,\n };\n \n+#[derive(Debug)]\n+/// Wrapper around a [`tokio::task::JoinHandle`] but with a oneshot receiver\n+/// which allows waiting for a `with_callback` stream fn to be ready for stream items.\n+pub struct StreamHandle<T> {\n+    pub handle: JoinHandle<T>,\n+    start: Option<oneshot::Receiver<()>>,\n+}\n+\n /// Events local to this client\n /// are broadcast across all senders/receivers of streams\n #[derive(Clone, Debug)]\n@@ -31,35 +35,39 @@ pub(crate) enum LocalEvents {\n     NewGroup(MlsGroup),\n }\n \n-// TODO simplify FfiStreamCloser + StreamCloser duplication\n-pub struct StreamCloser {\n-    pub close_fn: Arc<Mutex<Option<Sender<()>>>>,\n-    pub is_closed_atomic: Arc<AtomicBool>,\n-}\n-\n-impl StreamCloser {\n-    pub fn end(&self) {\n-        match self.close_fn.lock() {\n-            Ok(mut close_fn_option) => {\n-                let _ = close_fn_option.take().map(|close_fn| close_fn.send(()));\n-            }\n-            _ => {\n-                log::warn!(\"close_fn already closed\");\n-            }\n+impl<T> StreamHandle<T> {\n+    /// Waits for the stream to be fully spawned\n+    pub async fn wait_for_ready(&mut self) {\n+        if let Some(s) = self.start.take() {\n+            let _ = s.await;\n         }\n     }\n+}\n \n-    pub fn is_closed(&self) -> bool {\n-        self.is_closed_atomic.load(Ordering::Relaxed)\n+impl<T> From<StreamHandle<T>> for JoinHandle<T> {\n+    fn from(stream: StreamHandle<T>) -> JoinHandle<T> {\n+        stream.handle\n     }\n }\n \n-#[derive(Clone)]\n+#[derive(Clone, Debug)]\n pub(crate) struct MessagesStreamInfo {\n     pub convo_created_at_ns: i64,\n     pub cursor: u64,\n }\n \n+impl From<StoredGroup> for (Vec<u8>, MessagesStreamInfo) {\n+    fn from(group: StoredGroup) -> (Vec<u8>, MessagesStreamInfo) {\n+        (\n+            group.id,\n+            MessagesStreamInfo {\n+                convo_created_at_ns: group.created_at_ns,\n+                cursor: 0,\n+            },\n+        )\n+    }\n+}\n+\n impl<ApiClient> Client<ApiClient>\n where\n     ApiClient: XmtpApi,\n@@ -144,20 +152,19 @@ where\n \n         let subscription = self\n             .api_client\n-            .subscribe_welcome_messages(installation_key, Some(id_cursor as u64))\n+            .subscribe_welcome_messages(installation_key, Some(id_cursor))\n             .await?;\n \n         let stream = subscription\n-            .map(|welcome_result| async {\n+            .map(|welcome| async {\n                 log::info!(\"Received conversation streaming payload\");\n-                let welcome = welcome_result?;\n-                self.process_streamed_welcome(welcome).await\n+                self.process_streamed_welcome(welcome?).await\n             })\n             .filter_map(|res| async {\n                 match res.await {\n                     Ok(group) => Some(group),\n                     Err(err) => {\n-                        log::error!(\"Error processing stream entry: {:?}\", err);\n+                        log::error!(\"Error processing stream entry for conversation: {:?}\", err);\n                         None\n                     }\n                 }\n@@ -166,6 +173,7 @@ where\n         Ok(Box::pin(futures::stream::select(stream, event_queue)))\n     }\n \n+    #[tracing::instrument(skip(self, group_id_to_info))]\n     pub(crate) async fn stream_messages(\n         self: Arc<Self>,\n         group_id_to_info: HashMap<Vec<u8>, MessagesStreamInfo>,\n@@ -228,166 +236,159 @@ where\n     pub fn stream_conversations_with_callback(\n         client: Arc<Client<ApiClient>>,\n         mut convo_callback: impl FnMut(MlsGroup) + Send + 'static,\n-        mut on_close_callback: impl FnMut() + Send + 'static,\n-    ) -> Result<StreamCloser, ClientError> {\n-        let (close_sender, close_receiver) = oneshot::channel::<()>();\n-        let is_closed = Arc::new(AtomicBool::new(false));\n-        let is_closed_clone = is_closed.clone();\n+    ) -> StreamHandle<Result<(), ClientError>> {\n+        let (tx, rx) = oneshot::channel();\n \n-        tokio::spawn(async move {\n+        let handle = tokio::spawn(async move {\n             let mut stream = client.stream_conversations().await.unwrap();\n-            let mut close_receiver = close_receiver;\n-            loop {\n-                tokio::select! {\n-                    item = stream.next() => {\n-                        match item {\n-                            Some(convo) => { convo_callback(convo) },\n-                            None => break\n-                        }\n-                    }\n-                    _ = &mut close_receiver => {\n-                        on_close_callback();\n-                        break;\n-                    }\n-                }\n+            let _ = tx.send(());\n+            while let Some(convo) = stream.next().await {\n+                convo_callback(convo)\n             }\n-            is_closed_clone.store(true, Ordering::Relaxed);\n-            log::info!(\"closing stream\");\n+            Ok(())\n         });\n \n-        Ok(StreamCloser {\n-            close_fn: Arc::new(Mutex::new(Some(close_sender))),\n-            is_closed_atomic: is_closed,\n-        })\n+        StreamHandle {\n+            start: Some(rx),\n+            handle,\n+        }\n     }\n \n     pub(crate) fn stream_messages_with_callback(\n         client: Arc<Client<ApiClient>>,\n         group_id_to_info: HashMap<Vec<u8>, MessagesStreamInfo>,\n         mut callback: impl FnMut(StoredGroupMessage) + Send + 'static,\n-    ) -> Result<StreamCloser, ClientError> {\n-        let (close_sender, close_receiver) = oneshot::channel::<()>();\n-        let is_closed = Arc::new(AtomicBool::new(false));\n-\n-        let is_closed_clone = is_closed.clone();\n-        tokio::spawn(async move {\n-            let mut stream = Self::stream_messages(client, group_id_to_info)\n-                .await\n-                .unwrap();\n-            let mut close_receiver = close_receiver;\n-            loop {\n-                tokio::select! {\n-                    item = stream.next() => {\n-                        match item {\n-                            Some(message) => callback(message),\n-                            None => break\n-                        }\n-                    }\n-                    _ = &mut close_receiver => {\n-                        break;\n-                    }\n-                }\n+    ) -> StreamHandle<Result<(), ClientError>> {\n+        let (tx, rx) = oneshot::channel();\n+\n+        let handle = tokio::spawn(async move {\n+            let mut stream = Self::stream_messages(client, group_id_to_info).await?;\n+            let _ = tx.send(());\n+            while let Some(message) = stream.next().await {\n+                callback(message)\n             }\n-            is_closed_clone.store(true, Ordering::Relaxed);\n-            log::info!(\"closing stream\");\n+            Ok(())\n         });\n \n-        Ok(StreamCloser {\n-            close_fn: Arc::new(Mutex::new(Some(close_sender))),\n-            is_closed_atomic: is_closed,\n-        })\n+        StreamHandle {\n+            start: Some(rx),\n+            handle,\n+        }\n     }\n \n-    pub async fn stream_all_messages_with_callback(\n+    pub async fn stream_all_messages(\n         client: Arc<Client<ApiClient>>,\n-        callback: impl FnMut(StoredGroupMessage) + Send + Sync + 'static,\n-    ) -> Result<StreamCloser, ClientError> {\n-        client.sync_welcomes().await?; // TODO pipe cursor from welcomes sync into groups_stream\n-        Self::stream_all_messages_with_callback_sync(client, callback)\n-    }\n+    ) -> Result<impl Stream<Item = StoredGroupMessage>, ClientError> {\n+        let (tx, rx) = mpsc::unbounded_channel();\n \n-    /// Requires a sync welcomes before use\n-    pub fn stream_all_messages_with_callback_sync(\n-        client: Arc<Client<ApiClient>>,\n-        callback: impl FnMut(StoredGroupMessage) + Send + Sync + 'static,\n-    ) -> Result<StreamCloser, ClientError> {\n-        let callback = Arc::new(Mutex::new(callback));\n+        client.sync_welcomes().await?;\n \n-        let mut group_id_to_info: HashMap<Vec<u8>, MessagesStreamInfo> = client\n+        let mut group_id_to_info = client\n             .store()\n             .conn()?\n             .find_groups(None, None, None, None)?\n             .into_iter()\n-            .map(|group| {\n-                (\n-                    group.id.clone(),\n-                    MessagesStreamInfo {\n-                        convo_created_at_ns: group.created_at_ns,\n-                        cursor: 0,\n+            .map(Into::into)\n+            .collect::<HashMap<Vec<u8>, MessagesStreamInfo>>();\n+\n+        tokio::spawn(async move {\n+            let client = client.clone();\n+            let mut messages_stream = client\n+                .clone()\n+                .stream_messages(group_id_to_info.clone())\n+                .await?;\n+            let mut convo_stream = Self::stream_conversations(&client).await?;\n+            let mut extra_messages = Vec::new();\n+\n+            loop {\n+                tokio::select! {\n+                    // biased enforces an order to select!. If a message and a group are both ready\n+                    // at the same time, `biased` mode will process the message before the new\n+                    // group.\n+                    biased;\n+\n+                    messages = futures::future::ready(&mut extra_messages), if !extra_messages.is_empty() => {\n+                        for message in messages.drain(0..) {\n+                            if tx.send(message).is_err() {\n+                                break;\n+                            }\n+                        }\n                     },\n-                )\n-            })\n-            .collect();\n+                    Some(message) = messages_stream.next() => {\n+                        // an error can only mean the receiver has been dropped or closed so we're\n+                        // safe to end the stream\n+                        if tx.send(message).is_err() {\n+                            break;\n+                        }\n+                    }\n+                    Some(new_group) = convo_stream.next() => {\n+                        if tx.is_closed() {\n+                            break;\n+                        }\n+                        if group_id_to_info.contains_key(&new_group.group_id) {\n+                            continue;\n+                        }\n \n-        let callback_clone = callback.clone();\n-        let messages_stream_closer_mutex =\n-            Arc::new(Mutex::new(Self::stream_messages_with_callback(\n-                client.clone(),\n-                group_id_to_info.clone(),\n-                move |message| callback_clone.lock().unwrap()(message), // TODO fix unwrap\n-            )?));\n-        let messages_stream_closer_mutex_clone = messages_stream_closer_mutex.clone();\n-        let groups_stream_closer = Self::stream_conversations_with_callback(\n-            client.clone(),\n-            move |convo| {\n-                // TODO make sure key comparison works correctly\n-                if group_id_to_info.contains_key(&convo.group_id) {\n-                    return;\n-                }\n-                // Close existing message stream\n-                // TODO remove unwrap\n-                let mut messages_stream_closer = messages_stream_closer_mutex.lock().unwrap();\n-                messages_stream_closer.end();\n-\n-                // Set up new stream. For existing groups, stream new messages only by unsetting the cursor\n-                for info in group_id_to_info.values_mut() {\n-                    info.cursor = 0;\n-                }\n-                group_id_to_info.insert(\n-                    convo.group_id,\n-                    MessagesStreamInfo {\n-                        convo_created_at_ns: convo.created_at_ns,\n-                        cursor: 1, // For the new group, stream all messages since the group was created\n+                        for info in group_id_to_info.values_mut() {\n+                            info.cursor = 0;\n+                        }\n+                        group_id_to_info.insert(\n+                            new_group.group_id,\n+                            MessagesStreamInfo {\n+                                convo_created_at_ns: new_group.created_at_ns,\n+                                cursor: 1, // For the new group, stream all messages since the group was created\n+                            },\n+                        );\n+                        let new_messages_stream = client.clone().stream_messages(group_id_to_info.clone()).await?;\n+\n+                        // attempt to drain all ready messages from existing stream\n+                        while let Some(Some(message)) = messages_stream.next().now_or_never() {\n+                            extra_messages.push(message);\n+                        }\n+                        let _ = std::mem::replace(&mut messages_stream, new_messages_stream);\n                     },\n-                );\n-\n-                // Open new message stream\n-                let callback_clone = callback.clone();\n-                *messages_stream_closer = Self::stream_messages_with_callback(\n-                    client.clone(),\n-                    group_id_to_info.clone(),\n-                    move |message| callback_clone.lock().unwrap()(message), // TODO fix unwrap\n-                )\n-                .unwrap(); // TODO fix unwrap\n-            },\n-            move || {\n-                messages_stream_closer_mutex_clone.lock().unwrap().end();\n-            },\n-        )?;\n+                }\n+            }\n+            Ok::<_, ClientError>(())\n+        });\n \n-        Ok(groups_stream_closer)\n+        Ok(UnboundedReceiverStream::new(rx))\n+    }\n+\n+    pub fn stream_all_messages_with_callback(\n+        client: Arc<Client<ApiClient>>,\n+        mut callback: impl FnMut(StoredGroupMessage) + Send + Sync + 'static,\n+    ) -> StreamHandle<Result<(), ClientError>> {\n+        let (tx, rx) = oneshot::channel();\n+\n+        let handle = tokio::spawn(async move {\n+            let mut stream = Self::stream_all_messages(client).await?;\n+            let _ = tx.send(());\n+            while let Some(message) = stream.next().await {\n+                callback(message)\n+            }\n+            Ok(())\n+        });\n+\n+        StreamHandle {\n+            start: Some(rx),\n+            handle,\n+        }\n     }\n }\n \n #[cfg(test)]\n mod tests {\n+    use crate::utils::test::Delivery;\n     use crate::{\n         builder::ClientBuilder, groups::GroupMetadataOptions,\n         storage::group_message::StoredGroupMessage, Client,\n     };\n     use futures::StreamExt;\n-    use std::sync::{Arc, Mutex};\n-    use tokio::sync::Notify;\n+    use std::sync::{\n+        atomic::{AtomicU64, Ordering},\n+        Arc, Mutex,\n+    };\n     use xmtp_api_grpc::grpc_api_helper::Client as GrpcClient;\n     use xmtp_cryptography::utils::generate_local_wallet;\n \n@@ -435,47 +436,49 @@ mod tests {\n \n         let messages: Arc<Mutex<Vec<StoredGroupMessage>>> = Arc::new(Mutex::new(Vec::new()));\n         let messages_clone = messages.clone();\n-        let stream = Client::<GrpcClient>::stream_all_messages_with_callback(\n+\n+        let notify = Delivery::new();\n+        let notify_pointer = notify.clone();\n+        let mut handle = Client::<GrpcClient>::stream_all_messages_with_callback(\n             Arc::new(caro),\n             move |message| {\n                 (*messages_clone.lock().unwrap()).push(message);\n+                notify_pointer.notify_one();\n             },\n-        )\n-        .await\n-        .unwrap();\n-\n-        tokio::time::sleep(std::time::Duration::from_millis(50)).await;\n+        );\n+        handle.wait_for_ready().await;\n \n         alix_group\n             .send_message(\"first\".as_bytes(), &alix)\n             .await\n             .unwrap();\n+        notify.wait_for_delivery().await.unwrap();\n         bo_group\n             .send_message(\"second\".as_bytes(), &bo)\n             .await\n             .unwrap();\n+        notify.wait_for_delivery().await.unwrap();\n         alix_group\n             .send_message(\"third\".as_bytes(), &alix)\n             .await\n             .unwrap();\n+        notify.wait_for_delivery().await.unwrap();\n         bo_group\n             .send_message(\"fourth\".as_bytes(), &bo)\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(200)).await;\n+        notify.wait_for_delivery().await.unwrap();\n \n         let messages = messages.lock().unwrap();\n-        assert_eq!(messages[0].decrypted_message_bytes, \"first\".as_bytes());\n-        assert_eq!(messages[1].decrypted_message_bytes, \"second\".as_bytes());\n-        assert_eq!(messages[2].decrypted_message_bytes, \"third\".as_bytes());\n-        assert_eq!(messages[3].decrypted_message_bytes, \"fourth\".as_bytes());\n-\n-        stream.end();\n+        assert_eq!(messages[0].decrypted_message_bytes, b\"first\");\n+        assert_eq!(messages[1].decrypted_message_bytes, b\"second\");\n+        assert_eq!(messages[2].decrypted_message_bytes, b\"third\");\n+        assert_eq!(messages[3].decrypted_message_bytes, b\"fourth\");\n     }\n \n     #[tokio::test(flavor = \"multi_thread\", worker_threads = 10)]\n     async fn test_stream_all_messages_changing_group_list() {\n-        let alix = ClientBuilder::new_test_client(&generate_local_wallet()).await;\n+        let alix = Arc::new(ClientBuilder::new_test_client(&generate_local_wallet()).await);\n         let bo = ClientBuilder::new_test_client(&generate_local_wallet()).await;\n         let caro = Arc::new(ClientBuilder::new_test_client(&generate_local_wallet()).await);\n \n@@ -487,27 +490,22 @@ mod tests {\n             .await\n             .unwrap();\n \n-        tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n-\n         let messages: Arc<Mutex<Vec<StoredGroupMessage>>> = Arc::new(Mutex::new(Vec::new()));\n         let messages_clone = messages.clone();\n-        let stream =\n+        let delivery = Delivery::new();\n+        let delivery_pointer = delivery.clone();\n+        let mut handle =\n             Client::<GrpcClient>::stream_all_messages_with_callback(caro.clone(), move |message| {\n-                let text = String::from_utf8(message.decrypted_message_bytes.clone())\n-                    .unwrap_or(\"<not UTF8>\".to_string());\n-                println!(\"Received: {}\", text);\n+                delivery_pointer.notify_one();\n                 (*messages_clone.lock().unwrap()).push(message);\n-            })\n-            .await\n-            .unwrap();\n-\n-        tokio::time::sleep(std::time::Duration::from_millis(50)).await;\n+            });\n+        handle.wait_for_ready().await;\n \n         alix_group\n             .send_message(\"first\".as_bytes(), &alix)\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n+        delivery.wait_for_delivery().await.unwrap();\n \n         let bo_group = bo\n             .create_group(None, GroupMetadataOptions::default())\n@@ -516,19 +514,18 @@ mod tests {\n             .add_members_by_inbox_id(&bo, vec![caro.inbox_id()])\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(300)).await;\n \n         bo_group\n             .send_message(\"second\".as_bytes(), &bo)\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n+        delivery.wait_for_delivery().await.unwrap();\n \n         alix_group\n             .send_message(\"third\".as_bytes(), &alix)\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n+        delivery.wait_for_delivery().await.unwrap();\n \n         let alix_group_2 = alix\n             .create_group(None, GroupMetadataOptions::default())\n@@ -537,31 +534,31 @@ mod tests {\n             .add_members_by_inbox_id(&alix, vec![caro.inbox_id()])\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(300)).await;\n \n         alix_group\n             .send_message(\"fourth\".as_bytes(), &alix)\n             .await\n             .unwrap();\n-        tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n+        delivery.wait_for_delivery().await.unwrap();\n+\n         alix_group_2\n             .send_message(\"fifth\".as_bytes(), &alix)\n             .await\n             .unwrap();\n-\n-        tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n+        delivery.wait_for_delivery().await.unwrap();\n \n         {\n             let messages = messages.lock().unwrap();\n             assert_eq!(messages.len(), 5);\n         }\n \n-        stream.end();\n-        tokio::time::sleep(std::time::Duration::from_millis(50)).await;\n-        assert!(stream.is_closed());\n+        let a = handle.handle.abort_handle();\n+        a.abort();\n+        let _ = handle.handle.await;\n+        assert!(a.is_finished());\n \n         alix_group\n-            .send_message(\"first\".as_bytes(), &alix)\n+            .send_message(\"should not show up\".as_bytes(), &alix)\n             .await\n             .unwrap();\n         tokio::time::sleep(std::time::Duration::from_millis(100)).await;\n@@ -570,34 +567,96 @@ mod tests {\n         assert_eq!(messages.len(), 5);\n     }\n \n+    #[ignore]\n+    #[tokio::test(flavor = \"multi_thread\", worker_threads = 10)]\n+    async fn test_stream_all_messages_does_not_lose_messages() {\n+        let alix = Arc::new(ClientBuilder::new_test_client(&generate_local_wallet()).await);\n+        let caro = Arc::new(ClientBuilder::new_test_client(&generate_local_wallet()).await);\n+\n+        let alix_group = alix\n+            .create_group(None, GroupMetadataOptions::default())\n+            .unwrap();\n+        alix_group\n+            .add_members_by_inbox_id(&alix, vec![caro.inbox_id()])\n+            .await\n+            .unwrap();\n+\n+        let messages: Arc<Mutex<Vec<StoredGroupMessage>>> = Arc::new(Mutex::new(Vec::new()));\n+        let messages_clone = messages.clone();\n+\n+        let blocked = Arc::new(AtomicU64::new(55));\n+\n+        let blocked_pointer = blocked.clone();\n+        let mut handle =\n+            Client::<GrpcClient>::stream_all_messages_with_callback(caro.clone(), move |message| {\n+                (*messages_clone.lock().unwrap()).push(message);\n+                blocked_pointer.fetch_sub(1, Ordering::SeqCst);\n+            });\n+        handle.wait_for_ready().await;\n+\n+        let alix_group_pointer = alix_group.clone();\n+        let alix_pointer = alix.clone();\n+        tokio::spawn(async move {\n+            for _ in 0..50 {\n+                alix_group_pointer\n+                    .send_message(b\"spam\", &alix_pointer)\n+                    .await\n+                    .unwrap();\n+                tokio::time::sleep(std::time::Duration::from_micros(200)).await;\n+            }\n+        });\n+\n+        for _ in 0..5 {\n+            let new_group = alix\n+                .create_group(None, GroupMetadataOptions::default())\n+                .unwrap();\n+            new_group\n+                .add_members_by_inbox_id(&alix, vec![caro.inbox_id()])\n+                .await\n+                .unwrap();\n+            new_group\n+                .send_message(b\"spam from new group\", &alix)\n+                .await\n+                .unwrap();\n+        }\n+\n+        let _ = tokio::time::timeout(std::time::Duration::from_secs(120), async {\n+            while blocked.load(Ordering::SeqCst) > 0 {\n+                tokio::task::yield_now().await;\n+            }\n+        })\n+        .await;\n+\n+        let missed_messages = blocked.load(Ordering::SeqCst);\n+        if missed_messages > 0 {\n+            println!(\"Missed {} Messages\", missed_messages);\n+            panic!(\"Test failed due to missed messages\");\n+        }\n+    }\n+\n     #[tokio::test(flavor = \"multi_thread\")]\n     async fn test_self_group_creation() {\n         let alix = Arc::new(ClientBuilder::new_test_client(&generate_local_wallet()).await);\n         let bo = Arc::new(ClientBuilder::new_test_client(&generate_local_wallet()).await);\n \n         let groups = Arc::new(Mutex::new(Vec::new()));\n-        let notify = Arc::new(Notify::new());\n+        let notify = Delivery::new();\n         let (notify_pointer, groups_pointer) = (notify.clone(), groups.clone());\n \n-        let closer = Client::<GrpcClient>::stream_conversations_with_callback(\n-            alix.clone(),\n-            move |g| {\n+        let closer =\n+            Client::<GrpcClient>::stream_conversations_with_callback(alix.clone(), move |g| {\n                 let mut groups = groups_pointer.lock().unwrap();\n                 groups.push(g);\n                 notify_pointer.notify_one();\n-            },\n-            || {},\n-        )\n-        .unwrap();\n+            });\n \n         alix.create_group(None, GroupMetadataOptions::default())\n             .unwrap();\n \n-        tokio::time::timeout(std::time::Duration::from_secs(60), async {\n-            notify.notified().await\n-        })\n-        .await\n-        .expect(\"Stream never received group\");\n+        notify\n+            .wait_for_delivery()\n+            .await\n+            .expect(\"Stream never received group\");\n \n         {\n             let grps = groups.lock().unwrap();\n@@ -612,17 +671,13 @@ mod tests {\n             .await\n             .unwrap();\n \n-        tokio::time::timeout(std::time::Duration::from_secs(60), async {\n-            notify.notified().await\n-        })\n-        .await\n-        .expect(\"Stream never received group\");\n+        notify.wait_for_delivery().await.unwrap();\n \n         {\n             let grps = groups.lock().unwrap();\n             assert_eq!(grps.len(), 2);\n         }\n \n-        closer.end();\n+        closer.handle.abort();\n     }\n }\n\n@@ -4,6 +4,8 @@ use rand::{\n     distributions::{Alphanumeric, DistString},\n     Rng,\n };\n+use std::sync::Arc;\n+use tokio::{sync::Notify, time::error::Elapsed};\n use xmtp_api_grpc::grpc_api_helper::Client as GrpcClient;\n use xmtp_id::associations::{generate_inbox_id, RecoverableEcdsaSignature};\n \n@@ -111,6 +113,31 @@ impl ClientBuilder<GrpcClient> {\n     }\n }\n \n+/// wrapper over a `Notify` with a 60-scond timeout for waiting\n+#[derive(Clone, Default)]\n+pub struct Delivery {\n+    notify: Arc<Notify>,\n+}\n+\n+impl Delivery {\n+    pub fn new() -> Self {\n+        Self {\n+            notify: Arc::new(Notify::new()),\n+        }\n+    }\n+\n+    pub async fn wait_for_delivery(&self) -> Result<(), Elapsed> {\n+        tokio::time::timeout(std::time::Duration::from_secs(60), async {\n+            self.notify.notified().await\n+        })\n+        .await\n+    }\n+\n+    pub fn notify_one(&self) {\n+        self.notify.notify_one()\n+    }\n+}\n+\n impl Client<GrpcClient> {\n     pub async fn is_registered(&self, address: &String) -> bool {\n         let ids = self\n"}
{"id": 54, "ground_truth": "Async Wait", "predicted": "Logic", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n`test_process_certificate` (https://github.com/MystenLabs/sui/blob/main/crates/sui-core/src/unit_tests/authority_aggregator_tests.rs#L815) is flaky. The first run usually succeeds, but if you iterate the test it will quickly fail.\r\n\r\nTo reproduce, run: \r\n\r\n     $ MSIM_TEST_NUM=10 cargo simtest test_process_certificate\r\n\r\nThis will usually fail in the first few iterations. If we do nothing, this will likely eventually fail on the first try during an unlucky CI run.\r\n\r\n I took a look at the logs and it appears this may be a real bug, not just a fragile test - passing runs indicate that `sync_certificate_to_authority` is called more often when the test passes then when it fails: \r\n\r\n<img width=\"1713\" alt=\"image\" src=\"https://user-images.githubusercontent.com/103447440/194340895-4a1d0b79-38ee-4a47-a6c0-a30631af0c75.png\">\r\n\r\n(After https://github.com/MystenLabs/sui/pull/5020 merges, it will be possible to reproduce the failed iteration exactly by specifying the random seed - for now, that does not work).\n\n### Code Patch:\n@@ -1485,36 +1485,12 @@ where\n                                 name = ?name.concise(),\n                                 \"Validator handled certificate successfully\",\n                             );\n-                            // We got an ok answer, so returning the result of processing\n-                            // the transaction.\n-                            return res;\n                         }\n \n-                        // LockErrors indicate the authority may be out-of-date.\n-                        // We only attempt to update authority and retry if we are seeing LockErrors.\n-                        // For any other error, we stop here and return.\n-                        if !matches!(res, Err(SuiError::ObjectErrors { .. })) {\n-                            debug!(\n-                                tx_digest = ?tx_digest,\n-                                name = ?name.concise(),\n-                                \"Error from validator handle_confirmation_transaction: {:?}\",\n-                                res\n-                            );\n-                            return res;\n-                        }\n-\n-                        debug!(authority =? name, error =? res, ?timeout_after_quorum, \"Validator out of date - syncing certificates\");\n-                        // If we got LockErrors, we try to update the authority asynchronously\n-                        self.sync_certificate_to_authority(\n-                                cert_ref.clone(),\n-                                name,\n-                                DEFAULT_RETRIES,\n-                                self.timeouts.authority_request_timeout,\n-                                self.timeouts.pre_quorum_timeout,\n-                            )\n-                            .instrument(tracing::trace_span!(\"sync_cert\", authority =? name.concise()))\n-                            .await\n-                            .map_err(|e| { info!(err =? e, \"Error from sync_certificate\"); e})\n+                        // The authority may have failed to process the certificate if there were\n+                        // missing parents. In that case, the authority will attempt to perform causal\n+                        // completion and execute the cert later.\n+                        res\n                     })\n                 },\n                 |mut state, name, weight, result| {\n@@ -1599,15 +1575,6 @@ where\n         })\n     }\n \n-    /// Find the higgest sequence number that is known to a quorum of authorities.\n-    /// NOTE: This is only reliable in the synchronous model, with a sufficient timeout value.\n-    #[cfg(test)]\n-    async fn get_latest_sequence_number(&self, object_id: ObjectID) -> SequenceNumber {\n-        let (object_infos, _certificates) = self.get_object_by_id(object_id).await.unwrap(); // Not safe, but want to blow up if testing.\n-        let top_ref = object_infos.keys().last().unwrap().0;\n-        top_ref.1\n-    }\n-\n     pub async fn execute_transaction(\n         &self,\n         transaction: &Transaction,\n\n@@ -25,6 +25,7 @@ use sui_network::{\n };\n \n use sui_types::{error::*, messages::*};\n+use tap::TapFallible;\n use tokio::{\n     sync::mpsc::{channel, Receiver, Sender},\n     task::JoinHandle,\n@@ -34,7 +35,7 @@ use sui_types::messages_checkpoint::CheckpointRequest;\n use sui_types::messages_checkpoint::CheckpointResponse;\n \n use crate::authority::ConsensusHandler;\n-use tracing::{info, Instrument};\n+use tracing::{error, info, Instrument};\n \n #[cfg(test)]\n #[path = \"unit_tests/server_tests.rs\"]\n@@ -444,13 +445,23 @@ impl ValidatorService {\n             tx_kind = certificate.signed_data.data.kind_as_str()\n         );\n \n-        let response = state\n-            .handle_certificate(certificate)\n+        match state\n+            .handle_certificate(certificate.clone())\n             .instrument(span)\n             .await\n-            .map_err(|e| tonic::Status::internal(e.to_string()))?;\n-\n-        Ok(tonic::Response::new(response))\n+            .map_err(|e| tonic::Status::internal(e.to_string()))\n+        {\n+            Err(e) => {\n+                // Record the cert for later execution, including causal completion if necessary.\n+                let tx_digest = *tx_digest;\n+                let _ = state\n+                    .database\n+                    .add_pending_certificates(vec![(tx_digest, Some(certificate))])\n+                    .tap_err(|e| error!(?tx_digest, \"add_pending_certificates failed: {}\", e));\n+                Err(e)\n+            }\n+            Ok(response) => Ok(tonic::Response::new(response)),\n+        }\n     }\n }\n \n\n@@ -792,73 +792,6 @@ async fn test_sync_all_owned_objects() {\n     );\n }\n \n-async fn get_owned_objects(\n-    authorities: &AuthorityAggregator<LocalAuthorityClient>,\n-    addr: SuiAddress,\n-) -> BTreeMap<ObjectRef, Vec<AuthorityPublicKeyBytes>> {\n-    let (owned_objects, _) = authorities\n-        .get_all_owned_objects(addr, Duration::from_secs(10))\n-        .await\n-        .unwrap();\n-\n-    // As a result, we have 2 gas objects and 1 created object.\n-    dbg!(&owned_objects);\n-    owned_objects\n-}\n-\n-#[sim_test]\n-async fn test_process_certificate() {\n-    let (addr1, key1): (_, AccountKeyPair) = get_key_pair();\n-    let gas_object1 = Object::with_owner_for_testing(addr1);\n-    let gas_object2 = Object::with_owner_for_testing(addr1);\n-    let (authorities, _, pkg_ref) =\n-        init_local_authorities(4, vec![gas_object1.clone(), gas_object2.clone()]).await;\n-    let authority_clients: Vec<_> = authorities.authority_clients.values().collect();\n-\n-    // Make a schedule of transactions\n-    let gas_ref_1 = get_latest_ref(authority_clients[0], gas_object1.id()).await;\n-    let create1 = crate_object_move_transaction(addr1, &key1, addr1, 100, pkg_ref, gas_ref_1);\n-\n-    do_transaction(authority_clients[0], &create1).await;\n-    do_transaction(authority_clients[1], &create1).await;\n-    do_transaction(authority_clients[2], &create1).await;\n-\n-    // Get a cert\n-    let cert1 = extract_cert(&authority_clients, &authorities.committee, create1.digest()).await;\n-\n-    // Submit the cert to 1 authority.\n-    let new_ref_1 = do_cert(authority_clients[0], &cert1).await.created[0].0;\n-    do_cert(authority_clients[1], &cert1).await;\n-    do_cert(authority_clients[2], &cert1).await;\n-\n-    // Check the new object is at version 1\n-    let new_object_version = authorities.get_latest_sequence_number(new_ref_1.0).await;\n-    assert_eq!(SequenceNumber::from(1), new_object_version);\n-    get_owned_objects(&authorities, addr1).await;\n-\n-    // Make a schedule of transactions\n-    let gas_ref_set = get_latest_ref(authority_clients[0], gas_object1.id()).await;\n-    let create2 = set_object_move_transaction(addr1, &key1, new_ref_1, 100, pkg_ref, gas_ref_set);\n-\n-    do_transaction(authority_clients[0], &create2).await;\n-    do_transaction(authority_clients[1], &create2).await;\n-    do_transaction(authority_clients[2], &create2).await;\n-\n-    let cert2 = extract_cert(&authority_clients, &authorities.committee, create2.digest()).await;\n-    get_owned_objects(&authorities, addr1).await;\n-\n-    // Test: process the certificate, including bring up to date authority 3.\n-    //       which is 2 certs behind.\n-    authorities.process_certificate(cert2).await.unwrap();\n-    // Give authority 3 enough time to process the certificate\n-    tokio::time::sleep(tokio::time::Duration::from_secs(5)).await;\n-    let owned_object = get_owned_objects(&authorities, addr1).await;\n-    assert_eq!(3, owned_object.len());\n-    // Check this is the latest version.\n-    let new_object_version = authorities.get_latest_sequence_number(new_ref_1.0).await;\n-    assert_eq!(SequenceNumber::from(2), new_object_version);\n-}\n-\n #[sim_test]\n async fn test_execute_cert_to_true_effects() {\n     let (addr1, key1): (_, AccountKeyPair) = get_key_pair();\n"}
{"id": 40, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nhttps://github.com/get10101/10101/actions/runs/7969843098/job/21756644996?pr=2070#step:10:493\n\n### Code Patch:\n@@ -7,6 +7,7 @@ use native::trade::order::api::NewOrder;\n use native::trade::order::api::OrderType;\n use native::trade::order::OrderState;\n use native::trade::position::PositionState;\n+use std::time::Duration;\n use tests_e2e::app::submit_channel_opening_order;\n use tests_e2e::setup::TestSetup;\n use tests_e2e::wait_until;\n@@ -65,6 +66,9 @@ async fn reject_offer() {\n         stable: false,\n     };\n \n+    // give the coordinator some time to process the reject message, before submitting the next\n+    // order.\n+    tokio::time::sleep(Duration::from_secs(5)).await;\n     submit_channel_opening_order(order.clone(), 0, 0);\n \n     // Assert that the order was posted\n"}
{"id": 34, "ground_truth": "Unordered data", "predicted": "Unordered data", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n              This looks like a flaky test. I will fix it.\r\n```\r\n05_0001_set_var:                                                        [ FAIL ] - result differs with:\r\n--- /runner/_work/databend/databend/tests/suites/0_stateless/05_hints/05_0001_set_var.result\t2023-04-28 00:15:37.866486923 +0000\r\n+++ /runner/_work/databend/databend/tests/suites/0_stateless/05_hints/05_0001_set_var.stdout\t2023-04-28 00:19:13.660031675 +0000\r\n@@ -1,6 +1,6 @@\r\n-x\tx\r\n timezone\tAmerica/Los_Angeles\r\n x\tx\r\n+x\tx\r\n timezone\tAsia/Shanghai\r\n timezone\tAsia/Shanghai\tUTC\tSESSION\tSets the timezone.\tString\r\n storage_read_buffer_size\t1048576\r\n```\r\n\r\n_Originally posted by @TCeason in https://github.com/datafuselabs/databend/issues/11258#issuecomment-1526826926_\r\n            \n\n### Code Patch:\n@@ -6,8 +6,8 @@ CURDIR=$(cd \"$(dirname \"${BASH_SOURCE[0]}\")\" && pwd)\n # mariadb mysql client has some bug, please use mysql offical client\n # mysql --version\n # mysql  Ver 8.0.32-0ubuntu0.20.04.2 for Linux on x86_64 ((Ubuntu))\n-echo \"select /*+SET_VAR(timezone='Asia/Shanghai')*/ name, value value from system.settings where name in ('timezone') union all (select /*+SET_VAR(timezone='America/Los_Angeles')*/ 'x','x');\" |  $MYSQL_CLIENT_CONNECT\n-echo \"select /*+SET_VAR(timezone='America/Los_Angeles')*/ name, value value from system.settings where name in ('timezone') union all (select /*+SET_VAR(timezone='Asia/Shanghai')*/ 'x','x');\" |  $MYSQL_CLIENT_CONNECT\n+echo \"select * from (select /*+SET_VAR(timezone='Asia/Shanghai')*/ name, value value from system.settings where name in ('timezone') union all (select /*+SET_VAR(timezone='America/Los_Angeles')*/ 'x','x')) order by name desc;\" |  $MYSQL_CLIENT_CONNECT\n+echo \"select * from (select /*+SET_VAR(timezone='America/Los_Angeles')*/ name, value value from system.settings where name in ('timezone') union all (select /*+SET_VAR(timezone='Asia/Shanghai')*/ 'x','x')) order by name desc;\" |  $MYSQL_CLIENT_CONNECT\n echo \"select /*+SET_VAR(timezone='Asia/Shanghai') */ * from system.settings where name = 'timezone';\" |  $MYSQL_CLIENT_CONNECT\n echo \"select /*+SET_VAR(timezone='Asia') SET_VAR(storage_read_buffer_size=200)*/ name, value from system.settings where name in ('timezone', 'storage_read_buffer_size')\" |  $MYSQL_CLIENT_CONNECT\n echo \"select /*+SET_VAR(timezone='Asia/Shanghai') SET_VAR(storage_read_buffer_size=200)*/ /*+SET_VAR(storage_read_buffer_size=100)*/name, /*+xx*/ value from system.settings where name in ('timezone', 'storage_read_buffer_size')\" |  $MYSQL_CLIENT_CONNECT\n"}
{"id": 75, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nEncountered on CI of a PR modifying purely github metadata:\r\nhttps://github.com/MystenLabs/sui/runs/6200152939?check_suite_focus=true\r\n\r\nFlaky and non-reproducible.\r\n\r\n```\r\nrunning 2 tests\r\ntest shared_object_transaction ... ok\r\ntest call_shared_object_contract ... FAILED\r\n\r\nfailures:\r\n\r\n---- call_shared_object_contract stdout ----\r\nthread 'call_shared_object_contract' panicked at 'Error acquiring lock for object(s): [UnexpectedSequenceNumber { object_id: 6EE48C2E2A362479E1F5DD836BA27D48C5546E7A, expected_sequence: SequenceNumber(2), given_sequence: SequenceNumber(1) }]', sui/tests/shared_objects_tests.rs:75:30\r\nstack backtrace:\r\n   0: rust_begin_unwind\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/std/src/panicking.rs:584:5\r\n   1: core::panicking::panic_fmt\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/panicking.rs:143:14\r\n   2: shared_objects_tests::submit_shared_object_transaction::{{closure}}\r\n             at ./tests/shared_objects_tests.rs:75:30\r\n   3: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/future/mod.rs:91:19\r\n   4: shared_objects_tests::call_shared_object_contract::{{closure}}\r\n             at ./tests/shared_objects_tests.rs:168:72\r\n   5: <core::future::from_generator::GenFuture<T> as core::future::future::Future>::poll\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/future/mod.rs:91:19\r\n   6: <core::pin::Pin<P> as core::future::future::Future>::poll\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/future/future.rs:124:9\r\n   7: tokio::runtime::basic_scheduler::CoreGuard::block_on::{{closure}}::{{closure}}::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:498:48\r\n   8: tokio::coop::with_budget::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:102:9\r\n   9: std::thread::local::LocalKey<T>::try_with\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/std/src/thread/local.rs:442:16\r\n  10: std::thread::local::LocalKey<T>::with\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/std/src/thread/local.rs:418:9\r\n  11: tokio::coop::with_budget\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:95:5\r\n  12: tokio::coop::budget\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/coop.rs:72:5\r\n  13: tokio::runtime::basic_scheduler::CoreGuard::block_on::{{closure}}::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:498:25\r\n  14: tokio::runtime::basic_scheduler::Context::enter\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:356:19\r\n  15: tokio::runtime::basic_scheduler::CoreGuard::block_on::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:497:36\r\n  16: tokio::runtime::basic_scheduler::CoreGuard::enter::{{closure}}\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:555:57\r\n  17: tokio::macros::scoped_tls::ScopedKey<T>::set\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/macros/scoped_tls.rs:61:9\r\n  18: tokio::runtime::basic_scheduler::CoreGuard::enter\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:555:27\r\n  19: tokio::runtime::basic_scheduler::CoreGuard::block_on\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:488:9\r\n  20: tokio::runtime::basic_scheduler::BasicScheduler::block_on\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/basic_scheduler.rs:168:24\r\n  21: tokio::runtime::Runtime::block_on\r\n             at /home/ubuntu/.cargo/registry/src/github.com-1ecc6299db9ec823/tokio-1.17.0/src/runtime/mod.rs:475:46\r\n  22: shared_objects_tests::call_shared_object_contract\r\n             at ./tests/shared_objects_tests.rs:186:5\r\n  23: shared_objects_tests::call_shared_object_contract::{{closure}}\r\n             at ./tests/shared_objects_tests.rs:104:7\r\n  24: core::ops::function::FnOnce::call_once\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/ops/function.rs:[227](https://github.com/MystenLabs/sui/runs/6200152939?check_suite_focus=true#step:5:227):5\r\n  25: core::ops::function::FnOnce::call_once\r\n             at /rustc/69a6d12e9f0372e3c6d82bc7c7e410dab02d0500/library/core/src/ops/function.rs:227:5\r\nnote: Some details are omitted, run with `RUST_BACKTRACE=full` for a verbose backtrace.\r\n\r\n\r\nfailures:\r\n    call_shared_object_contract\r\n\r\ntest result: FAILED. 1 passed; 1 failed; 0 ignored; 0 measured; 0 filtered out; finished in 8.10s\r\n\r\nerror: test failed, to rerun pass '-p sui --test shared_objects_tests'\r\nError: Process completed with exit code 101.\r\n```\r\n\r\nFull log : https://gist.github.com/huitseeker/e5333b31a45b20fe6e56d1d6e425f78b\r\n\r\ncc @asonnino for shared objects, @velvia for locking service.\n\n### Code Patch:\n@@ -805,7 +805,7 @@ dependencies = [\n [[package]]\n name = \"config\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"crypto\",\n  \"serde 1.0.136\",\n@@ -833,20 +833,18 @@ dependencies = [\n [[package]]\n name = \"consensus\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"bincode\",\n  \"blake2\",\n  \"bytes\",\n  \"config 0.1.0\",\n  \"crypto\",\n- \"futures\",\n  \"rand 0.7.3\",\n  \"rocksdb\",\n  \"serde 1.0.136\",\n  \"serde_bytes\",\n  \"tokio\",\n- \"tokio-util\",\n  \"tracing\",\n  \"typed-store 0.1.0 (git+https://github.com/mystenlabs/mysten-infra.git?rev=808de09203d147b43d59114b8afd9e51cbcf5778)\",\n  \"types\",\n@@ -981,7 +979,7 @@ dependencies = [\n [[package]]\n name = \"crypto\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"ark-bls12-377\",\n  \"ark-ec\",\n@@ -1096,12 +1094,12 @@ dependencies = [\n \n [[package]]\n name = \"darling\"\n-version = \"0.14.0\"\n+version = \"0.14.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"f1a5d2e8b5a94b2261efb20e99a01255b9c5293797d69bbf04600567b2f9b8d7\"\n+checksum = \"4529658bdda7fd6769b8614be250cdcfc3aeb0ee72fe66f9e41e5e5eb73eac02\"\n dependencies = [\n- \"darling_core 0.14.0\",\n- \"darling_macro 0.14.0\",\n+ \"darling_core 0.14.1\",\n+ \"darling_macro 0.14.1\",\n ]\n \n [[package]]\n@@ -1120,9 +1118,9 @@ dependencies = [\n \n [[package]]\n name = \"darling_core\"\n-version = \"0.14.0\"\n+version = \"0.14.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"8f1c7d56716be82d9c6adb967cfe700955179ea88806e898483dad6987330a54\"\n+checksum = \"649c91bc01e8b1eac09fb91e8dbc7d517684ca6be8ebc75bb9cafc894f9fdb6f\"\n dependencies = [\n  \"fnv\",\n  \"ident_case\",\n@@ -1145,24 +1143,24 @@ dependencies = [\n \n [[package]]\n name = \"darling_macro\"\n-version = \"0.14.0\"\n+version = \"0.14.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"64dd7e5a75a00cb6799ae9fbbfc3bba0134def6579a9e27564e72c839c837bed\"\n+checksum = \"ddfc69c5bfcbd2fc09a0f38451d2daf0e372e367986a83906d1b0dbc88134fb5\"\n dependencies = [\n- \"darling_core 0.14.0\",\n+ \"darling_core 0.14.1\",\n  \"quote\",\n  \"syn\",\n ]\n \n [[package]]\n name = \"dashmap\"\n-version = \"5.2.0\"\n+version = \"5.3.1\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"4c8858831f7781322e539ea39e72449c46b059638250c14344fec8d0aa6e539c\"\n+checksum = \"084c61a88f94d7557441d01e89fc3800b0b80658462b05aa7b55dfae069da09b\"\n dependencies = [\n  \"cfg-if 1.0.0\",\n- \"num_cpus\",\n- \"parking_lot 0.12.0\",\n+ \"hashbrown 0.12.0\",\n+ \"lock_api 0.4.7\",\n ]\n \n [[package]]\n@@ -1202,7 +1200,7 @@ version = \"0.11.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n checksum = \"1f91d4cfa921f1c05904dc3c57b4a32c38aed3340cce209f3a6fd1478babafc4\"\n dependencies = [\n- \"darling 0.14.0\",\n+ \"darling 0.14.1\",\n  \"proc-macro2\",\n  \"quote\",\n  \"syn\",\n@@ -1433,7 +1431,7 @@ checksum = \"77f3309417938f28bf8228fcff79a4a37103981e3e186d2ccd19c74b38f4eb71\"\n [[package]]\n name = \"executor\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"async-trait\",\n  \"bincode\",\n@@ -1804,9 +1802,9 @@ dependencies = [\n \n [[package]]\n name = \"http\"\n-version = \"0.2.6\"\n+version = \"0.2.7\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"31f4c6746584866f0feabcc69893c5b51beef3831656a968ed7ae254cdc4fd03\"\n+checksum = \"ff8670570af52249509a86f5e3e18a08c60b177071826898fde8997cf5f6bfbb\"\n dependencies = [\n  \"bytes\",\n  \"fnv\",\n@@ -1920,12 +1918,12 @@ dependencies = [\n \n [[package]]\n name = \"im\"\n-version = \"15.0.0\"\n+version = \"15.1.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"111c1983f3c5bb72732df25cddacee9b546d08325fb584b5ebd38148be7b0246\"\n+checksum = \"d0acd33ff0285af998aaf9b57342af478078f53492322fafc47450e09397e0e9\"\n dependencies = [\n  \"bitmaps\",\n- \"rand_core 0.5.1\",\n+ \"rand_core 0.6.3\",\n  \"rand_xoshiro\",\n  \"sized-chunks\",\n  \"typenum\",\n@@ -2216,9 +2214,9 @@ dependencies = [\n \n [[package]]\n name = \"libc\"\n-version = \"0.2.124\"\n+version = \"0.2.125\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"21a41fed9d98f27ab1c6d161da622a4fa35e8a54a8adc24bbf3ddd0ef70b0e50\"\n+checksum = \"5916d2ae698f6de9bfb891ad7a8d65c09d232dc58cc4ac433c7da3b2fd84bc2b\"\n \n [[package]]\n name = \"libloading\"\n@@ -2247,9 +2245,9 @@ dependencies = [\n \n [[package]]\n name = \"libz-sys\"\n-version = \"1.1.5\"\n+version = \"1.1.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"6f35facd4a5673cb5a48822be2be1d4236c1c99cb4113cab7061ac720d5bf859\"\n+checksum = \"92e7e15d7610cce1d9752e137625f14e61a28cd45929b6e12e47b50fe154ee2e\"\n dependencies = [\n  \"cc\",\n  \"pkg-config\",\n@@ -2264,9 +2262,9 @@ checksum = \"7fb9b38af92608140b86b693604b9ffcc5824240a484d1ecd4795bacb2fe88f3\"\n \n [[package]]\n name = \"linux-raw-sys\"\n-version = \"0.0.42\"\n+version = \"0.0.46\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"5284f00d480e1c39af34e72f8ad60b94f47007e3481cd3b731c1d67190ddc7b7\"\n+checksum = \"d4d2456c373231a208ad294c33dc5bff30051eafd954cd4caae83a712b12854d\"\n \n [[package]]\n name = \"lock_api\"\n@@ -3069,7 +3067,7 @@ dependencies = [\n [[package]]\n name = \"network\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"async-trait\",\n  \"bytes\",\n@@ -3106,7 +3104,7 @@ dependencies = [\n [[package]]\n name = \"node\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"anyhow\",\n  \"async-trait\",\n@@ -3691,7 +3689,7 @@ dependencies = [\n [[package]]\n name = \"primary\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"async-recursion\",\n  \"async-trait\",\n@@ -3876,11 +3874,11 @@ dependencies = [\n \n [[package]]\n name = \"rand_xoshiro\"\n-version = \"0.4.0\"\n+version = \"0.6.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"a9fcdd2e881d02f1d9390ae47ad8e5696a9e4be7b547a1da2afbc61973217004\"\n+checksum = \"6f97cdb2a36ed4183de61b2f824cc45c9f1037f28afe0a322e9fff4c108b5aaa\"\n dependencies = [\n- \"rand_core 0.5.1\",\n+ \"rand_core 0.6.3\",\n ]\n \n [[package]]\n@@ -4080,9 +4078,9 @@ dependencies = [\n \n [[package]]\n name = \"rustix\"\n-version = \"0.34.4\"\n+version = \"0.34.6\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"3f5d1c6ed6d1c6915aa64749b809fc1bafff49d160f5d927463658093d7d62ab\"\n+checksum = \"f3e74b3f02f2b6eb33790923756784614f456de79d821d6b2670dc7d5fbea807\"\n dependencies = [\n  \"bitflags\",\n  \"errno\",\n@@ -4933,9 +4931,9 @@ dependencies = [\n \n [[package]]\n name = \"syn\"\n-version = \"1.0.91\"\n+version = \"1.0.92\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"b683b2b825c8eef438b77c36a06dc262294da3d5a5813fac20da149241dcd44d\"\n+checksum = \"7ff7c592601f11445996a06f8ad0c27f094a58857c2f89e97974ab9235b92c52\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -5149,9 +5147,9 @@ dependencies = [\n \n [[package]]\n name = \"tokio\"\n-version = \"1.17.0\"\n+version = \"1.18.0\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2af73ac49756f3f7c01172e34a23e5d0216f6c32333757c2c61feb2bbff5a5ee\"\n+checksum = \"0f48b6d60512a392e34dbf7fd456249fd2de3c83669ab642e021903f4015185b\"\n dependencies = [\n  \"bytes\",\n  \"libc\",\n@@ -5244,9 +5242,9 @@ dependencies = [\n \n [[package]]\n name = \"tower-http\"\n-version = \"0.3.0\"\n+version = \"0.3.2\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"79dd37121c38240c4b4fe6520332406218bbf876f2f690fe9e406020189366fd\"\n+checksum = \"e980386f06883cf4d0578d6c9178c81f68b45d77d00f2c2c1bc034b3439c2c56\"\n dependencies = [\n  \"bitflags\",\n  \"bytes\",\n@@ -5299,9 +5297,9 @@ dependencies = [\n \n [[package]]\n name = \"tracing-attributes\"\n-version = \"0.1.20\"\n+version = \"0.1.21\"\n source = \"registry+https://github.com/rust-lang/crates.io-index\"\n-checksum = \"2e65ce065b4b5c53e73bb28912318cb8c9e9ad3921f1d669eb0e68b4c8143a2b\"\n+checksum = \"cc6b8ad3567499f98a1db7a752b07a7c8c7c7c34c332ec00effb2b0027974b7c\"\n dependencies = [\n  \"proc-macro2\",\n  \"quote\",\n@@ -5455,7 +5453,7 @@ checksum = \"dcf81ac59edc17cc8697ff311e8f5ef2d99fcbd9817b34cec66f90b6c3dfd987\"\n [[package]]\n name = \"types\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"base64\",\n  \"bincode\",\n@@ -5465,13 +5463,10 @@ dependencies = [\n  \"crypto\",\n  \"derive_builder\",\n  \"ed25519-dalek\",\n- \"futures\",\n  \"rand 0.7.3\",\n  \"serde 1.0.136\",\n- \"tempfile\",\n  \"thiserror\",\n  \"tokio\",\n- \"tokio-util\",\n  \"typed-store 0.1.0 (git+https://github.com/mystenlabs/mysten-infra.git?rev=808de09203d147b43d59114b8afd9e51cbcf5778)\",\n ]\n \n@@ -5867,7 +5862,7 @@ checksum = \"d19538ccc21819d01deaf88d6a17eae6596a12e9aafdbb97916fb49896d89de9\"\n [[package]]\n name = \"worker\"\n version = \"0.1.0\"\n-source = \"git+https://github.com/MystenLabs/narwhal?rev=976355d2ff75aefc6b7ec7fd32742d3d7188c4e0#976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\"\n+source = \"git+https://github.com/MystenLabs/narwhal?rev=72efe71f0615f91f861cc658e031b763ba30fd5b#72efe71f0615f91f861cc658e031b763ba30fd5b\"\n dependencies = [\n  \"async-trait\",\n  \"bincode\",\n\n@@ -56,9 +56,9 @@ move-binary-format = { git = \"https://github.com/move-language/move\", rev = \"4e0\n move-bytecode-utils = { git = \"https://github.com/move-language/move\", rev = \"4e025186af502c931318884df53c11bf34a664bc\" }\n move-unit-test = { git = \"https://github.com/move-language/move\", rev = \"4e025186af502c931318884df53c11bf34a664bc\" }\n \n-narwhal-node = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\", package = \"node\" }\n-narwhal-config = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\", package = \"config\" }\n-narwhal-crypto = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\", package = \"crypto\" }\n+narwhal-node = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"72efe71f0615f91f861cc658e031b763ba30fd5b\", package = \"node\" }\n+narwhal-config = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"72efe71f0615f91f861cc658e031b763ba30fd5b\", package = \"config\" }\n+narwhal-crypto = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"72efe71f0615f91f861cc658e031b763ba30fd5b\", package = \"crypto\" }\n \n once_cell = \"1.10.0\"\n \n\n@@ -553,11 +553,7 @@ pub async fn make_authority(\n \n     // Spawn a consensus listener. It listen for consensus outputs and notifies the\n     // authority server when a sequenced transaction is ready for execution.\n-    ConsensusListener::spawn(\n-        authority_state.clone(),\n-        rx_sui_to_consensus,\n-        rx_consensus_to_sui,\n-    );\n+    ConsensusListener::spawn(rx_sui_to_consensus, rx_consensus_to_sui);\n \n     // If we have network information make authority clients\n     // to all authorities in the system.\n\n@@ -3,15 +3,17 @@\n use bytes::Bytes;\n use futures::{sink::SinkExt, stream::StreamExt};\n use sui::config::AuthorityPrivateInfo;\n-use sui_types::error::SuiError;\n+use sui_types::base_types::ObjectRef;\n+use sui_types::error::{SuiError, SuiResult};\n use sui_types::messages::CallArg;\n use sui_types::messages::Transaction;\n use sui_types::messages::TransactionInfoResponse;\n use sui_types::messages::{ConsensusTransaction, ExecutionStatus};\n+use sui_types::object::Object;\n use sui_types::serialize::{\n-    deserialize_message, deserialize_transaction_info, serialize_consensus_transaction,\n+    deserialize_message, deserialize_transaction_info, serialize_cert,\n+    serialize_consensus_transaction, SerializedMessage,\n };\n-use sui_types::serialize::{serialize_cert, SerializedMessage};\n use test_utils::authority::{spawn_test_authorities, test_authority_configs};\n use test_utils::messages::{make_certificates, move_transaction, publish_move_package_transaction};\n use test_utils::messages::{parse_package_ref, test_shared_object_transactions};\n@@ -48,37 +50,65 @@ async fn submit_single_owner_transaction(\n     responses\n }\n \n-// Keep submitting the certificate until it is sequenced by consensus. We use the loop\n-// since some consensus protocols (like Tusk) are not guaranteed to include the transaction\n-// (but it has high probability to do so, so it should virtually never be used).\n+/// Keep submitting the certificates of a shared-object transaction until it is sequenced by\n+/// at least one consensus node. We use the loop since some consensus protocols (like Tusk)\n+/// may drop transactions. The certificate is submitted to every Sui authority.\n async fn submit_shared_object_transaction(\n     transaction: Transaction,\n     configs: &[AuthorityPrivateInfo],\n-) -> TransactionInfoResponse {\n+) -> Vec<Option<SuiResult<TransactionInfoResponse>>> {\n     let certificate = make_certificates(vec![transaction]).pop().unwrap();\n     let message = ConsensusTransaction::UserTransaction(certificate);\n     let serialized = Bytes::from(serialize_consensus_transaction(&message));\n \n-    'main: loop {\n-        match transmit(serialized.clone(), &configs[0]).await {\n-            SerializedMessage::TransactionResp(reply) => {\n-                // We got a reply from the Sui authority.\n-                break 'main *reply;\n-            }\n-            SerializedMessage::Error(error) => match *error {\n-                SuiError::ConsensusConnectionBroken(_) => {\n-                    // This is the (confusing) error message returned by the consensus\n-                    // adapter. It means it didn't hear back from consensus and timed out.\n+    loop {\n+        let futures: Vec<_> = configs\n+            .iter()\n+            .map(|config| transmit(serialized.clone(), config))\n+            .collect();\n+\n+        let mut replies = Vec::new();\n+        for result in futures::future::join_all(futures).await {\n+            match result {\n+                SerializedMessage::TransactionResp(reply) => {\n+                    // We got a reply from the Sui authority.\n+                    replies.push(Some(Ok(*reply)));\n                 }\n-                error => panic!(\"{error}\"),\n-            },\n-            message => panic!(\"Unexpected protocol message: {message:?}\"),\n+                SerializedMessage::Error(error) => match *error {\n+                    SuiError::ConsensusConnectionBroken(_) => {\n+                        // This is the (confusing) error message returned by the consensus\n+                        // adapter. It means it didn't hear back from consensus and timed out.\n+                        replies.push(None);\n+                    }\n+                    error => replies.push(Some(Err(error))),\n+                },\n+                message => panic!(\"Unexpected protocol message: {message:?}\"),\n+            }\n+        }\n+        if replies.iter().any(|x| x.is_some()) {\n+            break replies;\n         }\n     }\n }\n \n+/// Helper function to publish the move package of a simple shared counter.\n+async fn publish_counter_package(\n+    gas_object: Object,\n+    configs: &[AuthorityPrivateInfo],\n+) -> ObjectRef {\n+    let transaction = publish_move_package_transaction(gas_object);\n+    let replies = submit_single_owner_transaction(transaction, configs).await;\n+    let mut package_refs = Vec::new();\n+    for reply in replies {\n+        let effects = reply.signed_effects.unwrap().effects;\n+        assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+        package_refs.push(parse_package_ref(&effects).unwrap());\n+    }\n+    package_refs.pop().unwrap()\n+}\n+\n+/// Send a simple shared object transaction to Sui and ensures the client gets back a response.\n #[tokio::test]\n-#[ignore = \"Flaky, see #1624\"]\n async fn shared_object_transaction() {\n     let mut objects = test_gas_objects();\n     objects.push(test_shared_object());\n@@ -94,12 +124,44 @@ async fn shared_object_transaction() {\n     // Submit the transaction. Note that this transaction is random and we do not expect\n     // it to be successfully executed by the Move execution engine.\n     tokio::task::yield_now().await;\n-    let reply = submit_shared_object_transaction(transaction, &configs).await;\n-    assert!(reply.signed_effects.is_some());\n+    let reply = submit_shared_object_transaction(transaction, &configs[0..1])\n+        .await\n+        .pop()\n+        .unwrap();\n+    let info = reply.unwrap().unwrap();\n+    assert!(info.signed_effects.is_some());\n }\n \n+/// Same as `shared_object_transaction` but every authorities submit the transaction.\n+#[tokio::test]\n+async fn many_shared_object_transactions() {\n+    let mut objects = test_gas_objects();\n+    objects.push(test_shared_object());\n+\n+    // Get the authority configs and spawn them. Note that it is important to not drop\n+    // the handles (or the authorities will stop).\n+    let configs = test_authority_configs();\n+    let _handles = spawn_test_authorities(objects, &configs).await;\n+\n+    // Make a test shared object certificate.\n+    let transaction = test_shared_object_transactions().pop().unwrap();\n+\n+    // Submit the transaction. Note that this transaction is random and we do not expect\n+    // it to be successfully executed by the Move execution engine.\n+    tokio::task::yield_now().await;\n+    let replies = submit_shared_object_transaction(transaction, &configs).await;\n+    for reply in replies {\n+        match reply {\n+            Some(Ok(_)) => (),\n+            Some(Err(error)) => panic!(\"{error}\"),\n+            None => (), // May happen rarely (see above comment on consensus)\n+        }\n+    }\n+}\n+\n+/// End-to-end shared transaction test for a Sui validator. It does not test the client, wallet,\n+/// or gateway but tests the end-to-end flow from Sui to consensus.\n #[tokio::test]\n-#[ignore = \"Flaky, see #1624\"]\n async fn call_shared_object_contract() {\n     let mut gas_objects = test_gas_objects();\n \n@@ -110,15 +172,99 @@ async fn call_shared_object_contract() {\n \n     // Publish the move package to all authorities and get the new package ref.\n     tokio::task::yield_now().await;\n-    let transaction = publish_move_package_transaction(gas_objects.pop().unwrap());\n+    let package_ref = publish_counter_package(gas_objects.pop().unwrap(), &configs).await;\n+\n+    // Make a transaction to create a counter.\n+    tokio::task::yield_now().await;\n+    let transaction = move_transaction(\n+        gas_objects.pop().unwrap(),\n+        \"Counter\",\n+        \"create\",\n+        package_ref,\n+        /* arguments */ Vec::default(),\n+    );\n     let replies = submit_single_owner_transaction(transaction, &configs).await;\n-    let mut package_refs = Vec::new();\n+    let mut counter_ids = Vec::new();\n     for reply in replies {\n         let effects = reply.signed_effects.unwrap().effects;\n         assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n-        package_refs.push(parse_package_ref(&effects).unwrap());\n+        let ((shared_object_id, _, _), _) = effects.created[0];\n+        counter_ids.push(shared_object_id);\n     }\n-    let package_ref = package_refs.pop().unwrap();\n+    let counter_id = counter_ids.pop().unwrap();\n+\n+    // Ensure the value of the counter is `0`.\n+    tokio::task::yield_now().await;\n+    let transaction = move_transaction(\n+        gas_objects.pop().unwrap(),\n+        \"Counter\",\n+        \"assert_value\",\n+        package_ref,\n+        vec![\n+            CallArg::SharedObject(counter_id),\n+            CallArg::Pure(0u64.to_le_bytes().to_vec()),\n+        ],\n+    );\n+    let reply = submit_shared_object_transaction(transaction, &configs[0..1])\n+        .await\n+        .pop()\n+        .unwrap();\n+    let info = reply.unwrap().unwrap();\n+    let effects = info.signed_effects.unwrap().effects;\n+    assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+\n+    // Make a transaction to increment the counter.\n+    tokio::task::yield_now().await;\n+    let transaction = move_transaction(\n+        gas_objects.pop().unwrap(),\n+        \"Counter\",\n+        \"increment\",\n+        package_ref,\n+        vec![CallArg::SharedObject(counter_id)],\n+    );\n+    let reply = submit_shared_object_transaction(transaction, &configs[0..1])\n+        .await\n+        .pop()\n+        .unwrap();\n+    let info = reply.unwrap().unwrap();\n+    let effects = info.signed_effects.unwrap().effects;\n+    assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+\n+    // Ensure the value of the counter is `1`.\n+    tokio::task::yield_now().await;\n+    let transaction = move_transaction(\n+        gas_objects.pop().unwrap(),\n+        \"Counter\",\n+        \"assert_value\",\n+        package_ref,\n+        vec![\n+            CallArg::SharedObject(counter_id),\n+            CallArg::Pure(1u64.to_le_bytes().to_vec()),\n+        ],\n+    );\n+    let reply = submit_shared_object_transaction(transaction, &configs[0..1])\n+        .await\n+        .pop()\n+        .unwrap();\n+    let info = reply.unwrap().unwrap();\n+    let effects = info.signed_effects.unwrap().effects;\n+    assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+}\n+\n+/// Same test as `call_shared_object_contract` but the clients submits many times the same\n+/// transaction (one copy per authority).\n+#[tokio::test]\n+async fn shared_object_flood() {\n+    let mut gas_objects = test_gas_objects();\n+\n+    // Get the authority configs and spawn them. Note that it is important to not drop\n+    // the handles (or the authorities will stop).\n+    let configs = test_authority_configs();\n+    let _handles = spawn_test_authorities(gas_objects.clone(), &configs).await;\n+\n+    // Publish the move package to all authorities and get the new pacakge ref.\n+    tokio::task::yield_now().await;\n+    let package_ref = publish_counter_package(gas_objects.pop().unwrap(), &configs).await;\n \n     // Make a transaction to create a counter.\n     tokio::task::yield_now().await;\n@@ -151,9 +297,17 @@ async fn call_shared_object_contract() {\n             CallArg::Pure(0u64.to_le_bytes().to_vec()),\n         ],\n     );\n-    let reply = submit_shared_object_transaction(transaction, &configs).await;\n-    let effects = reply.signed_effects.unwrap().effects;\n-    assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+    let replies = submit_shared_object_transaction(transaction, &configs).await;\n+    for reply in replies {\n+        match reply {\n+            Some(Ok(info)) => {\n+                let effects = info.signed_effects.unwrap().effects;\n+                assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+            }\n+            Some(Err(error)) => panic!(\"{error}\"),\n+            None => (), // May happen rarely (see above comment on consensus)\n+        }\n+    }\n \n     // Make a transaction to increment the counter.\n     tokio::task::yield_now().await;\n@@ -164,9 +318,17 @@ async fn call_shared_object_contract() {\n         package_ref,\n         vec![CallArg::SharedObject(counter_id)],\n     );\n-    let reply = submit_shared_object_transaction(transaction, &configs).await;\n-    let effects = reply.signed_effects.unwrap().effects;\n-    assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+    let replies = submit_shared_object_transaction(transaction, &configs).await;\n+    for reply in replies {\n+        match reply {\n+            Some(Ok(info)) => {\n+                let effects = info.signed_effects.unwrap().effects;\n+                assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+            }\n+            Some(Err(error)) => panic!(\"{error}\"),\n+            None => (), // May happen rarely (see above comment on consensus)\n+        }\n+    }\n \n     // Ensure the value of the counter is `1`.\n     tokio::task::yield_now().await;\n@@ -180,7 +342,101 @@ async fn call_shared_object_contract() {\n             CallArg::Pure(1u64.to_le_bytes().to_vec()),\n         ],\n     );\n-    let reply = submit_shared_object_transaction(transaction, &configs).await;\n+    let replies = submit_shared_object_transaction(transaction, &configs).await;\n+    for reply in replies {\n+        match reply {\n+            Some(Ok(info)) => {\n+                let effects = info.signed_effects.unwrap().effects;\n+                assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+            }\n+            Some(Err(error)) => panic!(\"{error}\"),\n+            None => (), // May happen rarely (see above comment on consensus)\n+        }\n+    }\n+}\n+\n+#[tokio::test]\n+async fn shared_object_sync() {\n+    let mut gas_objects = test_gas_objects();\n+\n+    // Get the authority configs and spawn them. Note that it is important to not drop\n+    // the handles (or the authorities will stop).\n+    let configs = test_authority_configs();\n+    let _handles = spawn_test_authorities(gas_objects.clone(), &configs).await;\n+\n+    // Publish the move package to all authorities and get the new pacakge ref.\n+    tokio::task::yield_now().await;\n+    let package_ref = publish_counter_package(gas_objects.pop().unwrap(), &configs).await;\n+\n+    // Send a transaction to create a counter, but only to one authority.\n+    tokio::task::yield_now().await;\n+    let create_counter_transaction = move_transaction(\n+        gas_objects.pop().unwrap(),\n+        \"Counter\",\n+        \"create\",\n+        package_ref,\n+        /* arguments */ Vec::default(),\n+    );\n+    let mut replies =\n+        submit_single_owner_transaction(create_counter_transaction.clone(), &configs[0..1]).await;\n+    let reply = replies.pop().unwrap();\n     let effects = reply.signed_effects.unwrap().effects;\n     assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+    let ((counter_id, _, _), _) = effects.created[0];\n+\n+    // Make a transaction to increment the counter.\n+    tokio::task::yield_now().await;\n+    let increment_counter_transaction = move_transaction(\n+        gas_objects.pop().unwrap(),\n+        \"Counter\",\n+        \"increment\",\n+        package_ref,\n+        vec![CallArg::SharedObject(counter_id)],\n+    );\n+\n+    // Let's submit the transaction to the first authority (the only one up-to-date).\n+    let reply =\n+        submit_shared_object_transaction(increment_counter_transaction.clone(), &configs[0..1])\n+            .await\n+            .pop()\n+            .unwrap();\n+    let info = reply.unwrap().unwrap();\n+    let effects = info.signed_effects.unwrap().effects;\n+    assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+\n+    // Let's submit the transaction to the out-of-date authorities.\n+    let replies =\n+        submit_shared_object_transaction(increment_counter_transaction.clone(), &configs[1..])\n+            .await;\n+    for reply in replies {\n+        match reply {\n+            Some(Err(SuiError::SharedObjectLockingFailure(_))) => (),\n+            Some(_) => panic!(\"Unexpected protocol message\"),\n+            None => (), // May happen rarely (see above comment on consensus)\n+        }\n+    }\n+\n+    // Now send the missing certificates to the outdated authorities. We also re-send\n+    // the transaction to the first authority who should simply ignore it.\n+    tokio::task::yield_now().await;\n+    let replies = submit_single_owner_transaction(create_counter_transaction, &configs).await;\n+    for reply in replies {\n+        let effects = reply.signed_effects.unwrap().effects;\n+        assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+    }\n+\n+    // Now we can try again with the shared-object transaction who failed before.\n+    tokio::task::yield_now().await;\n+    let replies =\n+        submit_shared_object_transaction(increment_counter_transaction, &configs[1..]).await;\n+    for reply in replies {\n+        match reply {\n+            Some(Ok(info)) => {\n+                let effects = info.signed_effects.unwrap().effects;\n+                assert!(matches!(effects.status, ExecutionStatus::Success { .. }));\n+            }\n+            Some(Err(error)) => panic!(\"{error}\"),\n+            None => (), // May happen rarely (see above comment on consensus)\n+        }\n+    }\n }\n\n@@ -42,7 +42,7 @@ move-vm-types = { git = \"https://github.com/move-language/move\", rev = \"4e025186\n \n typed-store = { git = \"https://github.com/MystenLabs/mysten-infra\", rev = \"d2976a45420147ad821baae96e6fe4b12215f743\"}\n \n-narwhal-executor = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\", package = \"executor\" }\n+narwhal-executor = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"72efe71f0615f91f861cc658e031b763ba30fd5b\", package = \"executor\" }\n \n [dev-dependencies]\n serde-reflection = \"0.3.5\"\n\n@@ -24,6 +24,7 @@ use std::{\n     sync::Arc,\n };\n use sui_adapter::adapter;\n+use sui_types::serialize::serialize_transaction_info;\n use sui_types::{\n     base_types::*,\n     batch::UpdateItem,\n@@ -257,8 +258,9 @@ impl AuthorityState {\n \n         // Ensure an idempotent answer.\n         if self._database.effects_exists(&transaction_digest)? {\n-            let transaction_info = self.make_transaction_info(&transaction_digest).await?;\n-            return Ok(transaction_info);\n+            let info = self.make_transaction_info(&transaction_digest).await?;\n+            debug!(\"Transaction {transaction_digest:?} already executed\");\n+            return Ok(info);\n         }\n \n         // Check the certificate and retrieve the transfer data.\n@@ -302,7 +304,9 @@ impl AuthorityState {\n                 } else if shared_locks[object_id] != *version {\n                     Some(SuiError::UnexpectedSequenceNumber {\n                         object_id: *object_id,\n+                        // This sequence number is the one attributed by consensus.\n                         expected_sequence: shared_locks[object_id],\n+                        // This sequence number is the one we currently have in the database.\n                         given_sequence: *version,\n                     })\n                 } else {\n@@ -313,6 +317,8 @@ impl AuthorityState {\n \n         fp_ensure!(\n             lock_errors.is_empty(),\n+            // NOTE: the error message here will say 'Error acquiring lock' but what it means is\n+            // 'error checking lock'.\n             SuiError::LockErrors {\n                 errors: lock_errors\n             }\n@@ -720,7 +726,7 @@ impl AuthorityState {\n     }\n \n     /// Make an information response for a transaction\n-    async fn make_transaction_info(\n+    pub async fn make_transaction_info(\n         &self,\n         transaction_digest: &TransactionDigest,\n     ) -> Result<TransactionInfoResponse, SuiError> {\n@@ -838,10 +844,34 @@ impl ExecutionState for AuthorityState {\n         &self,\n         execution_indices: ExecutionIndices,\n         transaction: Self::Transaction,\n-    ) -> Result<(), Self::Error> {\n+    ) -> Result<Vec<u8>, Self::Error> {\n         let ConsensusTransaction::UserTransaction(certificate) = transaction;\n-        self.handle_consensus_certificate(certificate, execution_indices)\n-            .await\n+\n+        // Ensure an idempotent answer.\n+        let digest = certificate.digest();\n+        if self._database.effects_exists(digest)? {\n+            let info = self.make_transaction_info(digest).await?;\n+            debug!(\"Shared-object transaction {digest:?} already executed\");\n+            return Ok(serialize_transaction_info(&info));\n+        }\n+\n+        // Assign locks to shared objects.\n+        self.handle_consensus_certificate(certificate.clone(), execution_indices)\n+            .await?;\n+        debug!(\"Shared objects locks successfully attributed to transaction {digest:?}\");\n+\n+        // Attempt to execute the transaction. This will only succeed if the authority\n+        // already executed all its dependencies.\n+        let confirmation_transaction = ConfirmationTransaction {\n+            certificate: certificate.clone(),\n+        };\n+        let info = self\n+            .handle_confirmation_transaction(confirmation_transaction.clone())\n+            .await?;\n+        debug!(\"Executed transaction {digest:?}\");\n+\n+        // Return a serialized transaction info response. This will be sent back to the client.\n+        Ok(serialize_transaction_info(&info))\n     }\n \n     fn ask_consensus_write_lock(&self) -> bool {\n\n@@ -838,9 +838,7 @@ impl<const ALL_OBJ_VER: bool, S: Eq + Serialize + for<'de> Deserialize<'de>>\n                 // sequence number (`OBJECT_START_VERSION`). Otherwise use the `scheduled` map to\n                 // to assign the next sequence number.\n                 let version = v.unwrap_or_else(|| OBJECT_START_VERSION);\n-                let next_version = v\n-                    .map(|v| v.increment())\n-                    .unwrap_or_else(|| SequenceNumber::from(2));\n+                let next_version = version.increment();\n \n                 let sequenced = ((transaction_digest, *id), version);\n                 let scheduled = (id, next_version);\n\n@@ -256,11 +256,9 @@ impl AuthorityServer {\n                 .handle_batch_streaming(*message, channel)\n                 .await\n                 .map(|_| None),\n-            SerializedMessage::ConsensusTransaction(message) => self\n-                .consensus_adapter\n-                .submit(&message)\n-                .await\n-                .map(|info| Some(serialize_transaction_info(&info))),\n+            SerializedMessage::ConsensusTransaction(message) => {\n+                self.consensus_adapter.submit(&message).await.map(Some)\n+            }\n \n             _ => Err(SuiError::UnexpectedMessage),\n         };\n\n@@ -1,21 +1,17 @@\n // Copyright (c) 2022, Mysten Labs, Inc.\n // SPDX-License-Identifier: Apache-2.0\n-use crate::authority::AuthorityState;\n use bytes::Bytes;\n use futures::SinkExt;\n use narwhal_executor::SubscriberResult;\n use std::collections::hash_map::DefaultHasher;\n use std::collections::HashMap;\n use std::hash::{Hash, Hasher};\n use std::net::SocketAddr;\n-use std::sync::Arc;\n use sui_network::transport;\n use sui_network::transport::{RwChannel, TcpDataStream};\n use sui_types::committee::Committee;\n use sui_types::error::{SuiError, SuiResult};\n-use sui_types::messages::ConfirmationTransaction;\n use sui_types::messages::ConsensusTransaction;\n-use sui_types::messages::TransactionInfoResponse;\n use tokio::sync::mpsc::{Receiver, Sender};\n use tokio::sync::oneshot;\n use tokio::task::JoinHandle;\n@@ -29,15 +25,14 @@ pub mod consensus_tests;\n /// A serialized consensus transaction.\n type SerializedConsensusTransaction = Vec<u8>;\n \n-/// The digest of a consensus transactions\n+/// The digest of a consensus transactions.\n type ConsensusTransactionDigest = u64;\n \n-/// The message returned by the consensus to notify that a Sui certificate has been sequenced\n-/// and all its shared objects are locked.\n-type ConsensusOutput = (SubscriberResult<()>, SerializedConsensusTransaction);\n+/// Transaction info response serialized by Sui.\n+type SerializedTransactionInfoResponse = Vec<u8>;\n \n /// Channel to notify the called when the Sui certificate has been sequenced.\n-type Replier = oneshot::Sender<SuiResult<TransactionInfoResponse>>;\n+type Replier = oneshot::Sender<SuiResult<SerializedTransactionInfoResponse>>;\n \n /// Message to notify the consensus adapter of a new certificate sent to consensus.\n #[derive(Debug)]\n@@ -46,6 +41,13 @@ pub struct ConsensusInput {\n     replier: Replier,\n }\n \n+/// The message returned by the consensus to notify that a Sui certificate has been sequenced\n+/// and all its shared objects are locked.\n+type ConsensusOutput = (\n+    /* result */ SubscriberResult<SerializedTransactionInfoResponse>,\n+    /* transaction */ SerializedConsensusTransaction,\n+);\n+\n /// Submit Sui certificates to the consensus.\n pub struct ConsensusAdapter {\n     /// The network address of the consensus node.\n@@ -95,7 +97,7 @@ impl ConsensusAdapter {\n     pub async fn submit(\n         &self,\n         certificate: &ConsensusTransaction,\n-    ) -> SuiResult<TransactionInfoResponse> {\n+    ) -> SuiResult<SerializedTransactionInfoResponse> {\n         // Check the Sui certificate (submitted by the user).\n         certificate.check(&self.committee)?;\n \n@@ -139,8 +141,6 @@ impl ConsensusAdapter {\n /// This module interfaces the consensus with Sui. It receives certificates input to consensus and\n /// notify the called when they are sequenced.\n pub struct ConsensusListener {\n-    /// The authority state to execute shared-object transactions.\n-    state: Arc<AuthorityState>,\n     /// Receive messages input to the consensus.\n     rx_consensus_input: Receiver<ConsensusInput>,\n     /// Receive consensus outputs.\n@@ -152,13 +152,11 @@ pub struct ConsensusListener {\n impl ConsensusListener {\n     /// Spawn a new consensus adapter in a dedicated tokio task.\n     pub fn spawn(\n-        state: Arc<AuthorityState>,\n         rx_consensus_input: Receiver<ConsensusInput>,\n         rx_consensus_output: Receiver<ConsensusOutput>,\n     ) -> JoinHandle<()> {\n         tokio::spawn(async move {\n             Self {\n-                state,\n                 rx_consensus_input,\n                 rx_consensus_output,\n                 pending: HashMap::new(),\n@@ -168,35 +166,6 @@ impl ConsensusListener {\n         })\n     }\n \n-    /// Execute sequenced transactions and notify the end-user (if any).\n-    async fn process_consensus_output(&mut self, output: ConsensusOutput) {\n-        let (result, serialized) = output;\n-\n-        // Execute the transaction (if the consensus successfully sequenced it).\n-        let outcome = match result {\n-            Ok(()) => {\n-                let message =\n-                    bincode::deserialize(&serialized).expect(\"Failed to deserialize consensus tx\");\n-                let ConsensusTransaction::UserTransaction(certificate) = message;\n-                let confirmation_transaction = ConfirmationTransaction { certificate };\n-                self.state\n-                    .handle_confirmation_transaction(confirmation_transaction)\n-                    .await\n-            }\n-            Err(e) => Err(SuiError::from(e)),\n-        };\n-\n-        // Notify the caller that the transaction has been sequenced (if there is a caller).\n-        let digest = Self::hash(&serialized);\n-        if let Some(repliers) = self.pending.remove(&digest) {\n-            for replier in repliers {\n-                if replier.send(outcome.clone()).is_err() {\n-                    debug!(\"No replier to listen to consensus output {digest}\");\n-                }\n-            }\n-        }\n-    }\n-\n     /// Main loop receiving messages input to consensus and notifying the caller once the inputs\n     /// are sequenced (of if an error happened).\n     async fn run(&mut self) {\n@@ -210,10 +179,18 @@ impl ConsensusListener {\n                     self.pending.entry(digest).or_insert_with(Vec::new).push(replier);\n                 },\n \n-                // Execute sequenced transactions and notify the end-user (if any).\n-                Some(output) = self.rx_consensus_output.recv() => self\n-                    .process_consensus_output(output)\n-                    .await\n+                // Notify the caller that the transaction has been sequenced (if there is a caller).\n+                Some((result, serialized)) = self.rx_consensus_output.recv() => {\n+                    let outcome = result.map_err(SuiError::from);\n+                    let digest = Self::hash(&serialized);\n+                    if let Some(repliers) = self.pending.remove(&digest) {\n+                        for replier in repliers {\n+                            if replier.send(outcome.clone()).is_err() {\n+                                debug!(\"No replier to listen to consensus output {digest}\");\n+                            }\n+                        }\n+                    }\n+                }\n             }\n         }\n     }\n\n@@ -11,11 +11,13 @@ use sui_adapter::genesis;\n use sui_types::base_types::{ObjectID, TransactionDigest};\n use sui_types::crypto::Signature;\n use sui_types::gas_coin::GasCoin;\n+use sui_types::messages::ConfirmationTransaction;\n use sui_types::messages::{\n     CallArg, CertifiedTransaction, SignatureAggregator, Transaction, TransactionData,\n };\n use sui_types::object::OBJECT_START_VERSION;\n use sui_types::object::{MoveObject, Object, Owner};\n+use sui_types::serialize::serialize_transaction_info;\n use test_utils::network::test_listener;\n use test_utils::test_keys;\n use tokio::sync::mpsc::channel;\n@@ -114,7 +116,6 @@ async fn listen_to_sequenced_transaction() {\n \n     // Spawn a consensus listener.\n     ConsensusListener::spawn(\n-        Arc::new(state),\n         /* rx_consensus_input */ rx_sui_to_consensus,\n         /* rx_consensus_output */ rx_consensus_to_sui,\n     );\n@@ -129,7 +130,7 @@ async fn listen_to_sequenced_transaction() {\n \n     // Notify the consensus listener that the transaction has been sequenced.\n     tokio::task::yield_now().await;\n-    let output = (Ok(()), serialized);\n+    let output = (Ok(Vec::default()), serialized);\n     tx_consensus_to_sui.send(output).await.unwrap();\n \n     // Ensure the caller get notified from the consensus listener.\n@@ -181,7 +182,8 @@ async fn submit_transaction_to_consensus() {\n         let confirmation_transaction = ConfirmationTransaction { certificate };\n         let result = state\n             .handle_confirmation_transaction(confirmation_transaction)\n-            .await;\n+            .await\n+            .map(|info| serialize_transaction_info(&info));\n \n         // Reply to the submitter.\n         replier.send(result).unwrap();\n\n@@ -43,5 +43,5 @@ move-disassembler = { git = \"https://github.com/move-language/move\", rev = \"4e02\n move-ir-types = { git = \"https://github.com/move-language/move\", rev = \"4e025186af502c931318884df53c11bf34a664bc\" }\n move-vm-types = { git = \"https://github.com/move-language/move\", rev = \"4e025186af502c931318884df53c11bf34a664bc\" }\n \n-narwhal-executor = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\", package = \"executor\" }\n-narwhal-crypto = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\", package = \"crypto\" }\n+narwhal-executor = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"72efe71f0615f91f861cc658e031b763ba30fd5b\", package = \"executor\" }\n+narwhal-crypto = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"72efe71f0615f91f861cc658e031b763ba30fd5b\", package = \"crypto\" }\n\n@@ -23,7 +23,7 @@ sui-framework = { path = \"../sui_programmability/framework\" }\n move-package = { git = \"https://github.com/move-language/move\", rev = \"4e025186af502c931318884df53c11bf34a664bc\" }\n move-core-types = { git = \"https://github.com/move-language/move\", rev = \"4e025186af502c931318884df53c11bf34a664bc\", features = [\"address20\"] }\n typed-store = { git = \"https://github.com/MystenLabs/mysten-infra\", rev =\"d2976a45420147ad821baae96e6fe4b12215f743\"}\n-narwhal-config = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"976355d2ff75aefc6b7ec7fd32742d3d7188c4e0\", package = \"config\" }\n+narwhal-config = { git = \"https://github.com/MystenLabs/narwhal\", rev = \"72efe71f0615f91f861cc658e031b763ba30fd5b\", package = \"config\" }\n \n sui-types = { path = \"../sui_types\" }\n sui_core = { path = \"../sui_core\" }\n"}
{"id": 63, "ground_truth": "I/O", "predicted": "Async Wait", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n**Describe the bug**\r\n<!--\r\nA clear and concise description of what the bug is.\r\n-->\r\n\r\nI have seen this test fail a couple of times in CI lately, most recently\r\n\r\nhttps://github.com/apache/arrow-rs/actions/runs/6324042685/job/17172783580\r\n\r\nThis test was added in apache/arrow-rs#3824 by @metesynnada \r\n\r\n**To Reproduce**\r\n<!--\r\nSteps to reproduce the behavior:\r\n-->\r\n\r\n**Expected behavior**\r\n<!--\r\nA clear and concise description of what you expected to happen.\r\n-->\r\n\r\n**Additional context**\r\n<!--\r\nAdd any other context about the problem here.\r\n-->\n\n### Code Patch:\n@@ -1545,11 +1545,13 @@ mod not_wasm_tests {\n         for d in &data {\n             writer.write_all(d).await.unwrap();\n         }\n+        writer.flush().await.unwrap();\n \n         let mut writer = integration.append(&location).await.unwrap();\n         for d in &data {\n             writer.write_all(d).await.unwrap();\n         }\n+        writer.flush().await.unwrap();\n \n         let read_data = integration\n             .get(&location)\n"}
{"id": 29, "ground_truth": "I/O", "predicted": "Async Wait", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n# 🐛 Bug\r\n\r\nI have seen a few PRs fail with no code changes in `storage::command_adapter::tests::test_save_list_metadata_files`. Two runs are:\r\n- https://circleci.com/gh/libra/libra/167227\r\n- https://circleci.com/gh/libra/libra/166268\r\n\n\n### Code Patch:\n@@ -50,6 +50,7 @@ proptest! {\n         let tmpdir = TempPath::new();\n         let mut rt = Runtime::new().unwrap();\n \n-        rt.block_on(test_save_and_list_metadata_files_impl(get_store(&tmpdir), input));\n+        rt.block_on(test_save_and_list_metadata_files_impl(get_store(&tmpdir), input, &tmpdir.path().to_path_buf()));\n+\n     }\n }\n\n@@ -34,6 +34,6 @@ proptest! {\n         let store = LocalFs::new(tmpdir.path().to_path_buf());\n \n         let mut rt = Runtime::new().unwrap();\n-        rt.block_on(test_save_and_list_metadata_files_impl(Box::new(store), input));\n+        rt.block_on(test_save_and_list_metadata_files_impl(Box::new(store), input, &tmpdir.path().to_path_buf()));\n     }\n }\n\n@@ -9,8 +9,11 @@ use proptest::{\n     collection::{hash_map, vec},\n     prelude::*,\n };\n-use std::{collections::HashMap, process::Stdio};\n-use tokio::io::{AsyncReadExt, AsyncWriteExt};\n+use std::{collections::HashMap, path::PathBuf, process::Stdio};\n+use tokio::{\n+    io::{AsyncReadExt, AsyncWriteExt},\n+    time::{delay_for, Duration},\n+};\n \n fn to_file_name(tmpdir: &TempPath, backup_name: &str, file_name: &str) -> String {\n     tmpdir\n@@ -65,14 +68,15 @@ pub fn arb_backups(\n pub async fn test_save_and_list_metadata_files_impl(\n     store: Box<dyn BackupStorage>,\n     input: Vec<(ShellSafeName, TextLine)>,\n+    path: &PathBuf,\n ) {\n     for (name, content) in &input {\n         store.save_metadata_line(name, &content).await.unwrap();\n     }\n \n-    // It takes a little time for the ls command to reflect newly created entries if not synced.\n+    // It takes a little time for the ls command to reflect newly created entries.\n     // it's not a problem in real world.\n-    sync_fs().await;\n+    wait_for_dentries(path, input.len()).await;\n \n     let mut read_back = Vec::new();\n     for file_handle in store.list_metadata_files().await.unwrap() {\n@@ -107,7 +111,8 @@ pub fn arb_metadata_files() -> impl Strategy<Value = Vec<(ShellSafeName, TextLin\n     vec(any::<(ShellSafeName, TextLine)>(), 0..10)\n }\n \n-async fn sync_fs() {\n+async fn wait_for_dentries(path: &PathBuf, num_of_files: usize) {\n+    // sync\n     tokio::process::Command::new(\"sync\")\n         .stdin(Stdio::null())\n         .stdout(Stdio::null())\n@@ -116,4 +121,33 @@ async fn sync_fs() {\n         .unwrap()\n         .await\n         .unwrap();\n+\n+    // try every 10ms, for 10 seconds at most\n+    for n in 1..=1000usize {\n+        let output = tokio::process::Command::new(\"sh\")\n+            .arg(\"-c\")\n+            .arg(&format!(\n+                \"ls -1 {} || exec\",\n+                path.join(\"metadata\")\n+                    .into_os_string()\n+                    .into_string()\n+                    .unwrap()\n+            ))\n+            .stdin(Stdio::null())\n+            .output()\n+            .await\n+            .unwrap();\n+        let got_files = String::from_utf8(output.stdout).unwrap().lines().count();\n+        if got_files >= num_of_files {\n+            return;\n+        } else {\n+            println!(\n+                \"Got {} files on {}-th try, expecting {}.\",\n+                got_files, n, num_of_files\n+            );\n+        }\n+        delay_for(Duration::from_millis(10)).await;\n+    }\n+\n+    panic!(\"ls result never contained {} entries\", num_of_files);\n }\n"}
{"id": 28, "ground_truth": "Async Wait", "predicted": "Time", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nWe have a bunch of flaky tests to fix:\r\n\r\n- [x] `test_indexing_service_does_not_shut_down_pipelines_on_indexing_pipeline_timeout` ([link](https://github.com/quickwit-oss/quickwit/actions/runs/3448688279/jobs/5755916183)) @fmassot \r\n- [x] `test_run_gc_expires_stale_staged_splits_after_grace_period` @guilload \r\n- [x] `test_ingest_api_garbage_collector` ([link](https://github.com/quickwit-oss/quickwit/actions/runs/3464851848/jobs/5786887789)) @evanxg852000 \r\n- [x] `grpc_metastore::common_tests::test_metastore_list_splits` @ChillFish8 \r\n- [x] `test_garbage_collect_index_cli` ([link](https://github.com/quickwit-oss/quickwit/actions/runs/3473230518/jobs/5805017278)) @boraarslan \r\n- [x] `test_search_dynamic_mode_expand_dots` ([link](https://github.com/quickwit-oss/quickwit/actions/runs/3516424778/jobs/5892968003)) @fulmicoton \n\n### Code Patch:\n@@ -2103,7 +2103,7 @@ pub mod test_suite {\n             assert_eq!(split_ids, to_hash_set(&[\"list-splits-five\"]));\n \n             // Artificially increase the create_timestamp\n-            sleep(Duration::from_secs(2)).await;\n+            sleep(Duration::from_secs(1)).await;\n             // add a split without tag\n             let split_metadata_6 = SplitMetadata {\n                 footer_offsets: 1000..2000,\n@@ -2182,18 +2182,17 @@ pub mod test_suite {\n                 ])\n             );\n \n-            let select_timestamp = OffsetDateTime::now_utc().unix_timestamp() - 1;\n-            let query =\n-                ListSplitsQuery::for_index(index_id).with_update_timestamp_gte(select_timestamp);\n+            let query = ListSplitsQuery::for_index(index_id)\n+                .with_update_timestamp_gte(split_metadata_6.create_timestamp);\n             let splits = metastore.list_splits(query).await.unwrap();\n             let split_ids: HashSet<String> = splits\n                 .into_iter()\n                 .map(|meta| meta.split_id().to_string())\n                 .collect();\n             assert_eq!(split_ids, to_hash_set(&[\"list-splits-six\"]));\n \n-            let query =\n-                ListSplitsQuery::for_index(index_id).with_create_timestamp_lte(select_timestamp);\n+            let query = ListSplitsQuery::for_index(index_id)\n+                .with_create_timestamp_lt(split_metadata_6.create_timestamp);\n             let splits = metastore.list_splits(query).await.unwrap();\n             let split_ids: HashSet<String> = splits\n                 .into_iter()\n"}
{"id": 13, "ground_truth": "Network", "predicted": "Logic", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n`src/smtp.rs::send_smtp_messages` logs and ignores SMTP errors when sending MDNs, then returns `Ok(())`. Then in `src/scheduler::smtp_loop` we reset `timeout` to `None`:\r\nhttps://github.com/deltachat/deltachat-core-rust/blob/21845ca5ea17acaad192989605b3688a46d908f5/src/scheduler.rs#L773\r\nand then log \"smtp has no messages to retry\" and wait for interrupt forever:\r\nhttps://github.com/deltachat/deltachat-core-rust/blob/21845ca5ea17acaad192989605b3688a46d908f5/src/scheduler.rs#L815-L816\r\nIt is not true that SMTP has no messages to retry in this case.\r\n\r\nThe worst part of this is that during high load postfix returns temporary error \"421 4.4.2 *** Error: timeout exceeded\": https://github.com/deltachat/chatmail/issues/315 Then tests time out in CI because they never retry sending.\n\n### Code Patch:\n@@ -830,20 +830,26 @@ async fn send_mdn(context: &Context, smtp: &mut Smtp) -> Result<bool> {\n         .await\n         .context(\"failed to update MDN retries count\")?;\n \n-    let res = send_mdn_msg_id(context, msg_id, contact_id, smtp).await;\n-    if let Err(ref err) = res {\n-        // If there is an error, for example there is no message corresponding to the msg_id in the\n-        // database, do not try to send this MDN again.\n-        warn!(\n-            context,\n-            \"Error sending MDN for {msg_id}, removing it: {err:#}.\"\n-        );\n-        context\n-            .sql\n-            .execute(\"DELETE FROM smtp_mdns WHERE msg_id = ?\", (msg_id,))\n-            .await?;\n+    match send_mdn_msg_id(context, msg_id, contact_id, smtp).await {\n+        Err(err) => {\n+            // If there is an error, for example there is no message corresponding to the msg_id in the\n+            // database, do not try to send this MDN again.\n+            warn!(\n+                context,\n+                \"Error sending MDN for {msg_id}, removing it: {err:#}.\"\n+            );\n+            context\n+                .sql\n+                .execute(\"DELETE FROM smtp_mdns WHERE msg_id = ?\", (msg_id,))\n+                .await?;\n+            Err(err)\n+        }\n+        Ok(false) => {\n+            bail!(\"Temporary error while sending an MDN\");\n+        }\n+        Ok(true) => {\n+            // Successfully sent MDN.\n+            Ok(true)\n+        }\n     }\n-    // If there's a temporary error, pretend there are no more MDNs to send. It's unlikely that\n-    // other MDNs could be sent successfully in case of connectivity problems.\n-    res\n }\n"}
{"id": 32, "ground_truth": "Async Wait", "predicted": "Async Wait", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThis test was added recently, and is failing ~3% of the time.\r\n\r\nAn example failure:\r\n- https://neon-github-public-dev.s3.amazonaws.com/reports/pr-7663/9022743601/index.html#testresult/45c20a5d0b81d6cd/retries\r\n\r\n```\r\ntest_runner/regress/test_storage_controller.py:1312: in test_lock_time_tracing\r\n    env.storage_controller.assert_log_contains(\r\ntest_runner/fixtures/neon_fixtures.py:2003: in assert_log_contains\r\n    assert res is not None\r\nE   assert None is not None\r\n```\n\n### Code Patch:\n@@ -1284,7 +1284,7 @@ def test_lock_time_tracing(neon_env_builder: NeonEnvBuilder):\n \n     # Apply failpoint\n     env.storage_controller.configure_failpoints(\n-        (\"tenant-update-policy-exclusive-lock\", \"return(31000)\")\n+        (\"tenant-update-policy-exclusive-lock\", \"return(35000)\")\n     )\n \n     # This will hold the exclusive for enough time to cause an warning\n@@ -1306,7 +1306,7 @@ def update_tenent_policy():\n     env.storage_controller.pageserver_api().timeline_create(\n         pg_version=PgVersion.NOT_SET, tenant_id=tenant_id, new_timeline_id=timeline_id\n     )\n-    thread_update_tenant_policy.join(timeout=10)\n+    thread_update_tenant_policy.join()\n \n     env.storage_controller.assert_log_contains(\"Lock on UpdatePolicy was held for\")\n     env.storage_controller.assert_log_contains(\n"}
{"id": 45, "ground_truth": "Concurrency", "predicted": "Logic", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nAttached is a log from CI:\r\n[flaky-test.txt](https://github.com/deltachat/deltachat-core-rust/files/14000065/flaky-test.txt)\r\n\r\n\r\nMaybe similar issue to #5191 but better understand what exactly happened.\n\n### Code Patch:\n@@ -184,7 +184,7 @@ impl Session {\n             bail!(\"Attempt to fetch UID 0\");\n         }\n \n-        self.select_folder(context, folder).await?;\n+        self.select_with_uidvalidity(context, folder).await?;\n \n         // we are connected, and the folder is selected\n         info!(context, \"Downloading message {}/{} fully...\", folder, uid);\n\n@@ -543,15 +543,16 @@ impl Imap {\n             return Ok(false);\n         }\n \n-        let new_emails = session\n+        session\n             .select_with_uidvalidity(context, folder)\n             .await\n             .with_context(|| format!(\"Failed to select folder {folder:?}\"))?;\n \n-        if !new_emails && !fetch_existing_msgs {\n+        if !session.new_mail && !fetch_existing_msgs {\n             info!(context, \"No new emails in folder {folder:?}.\");\n             return Ok(false);\n         }\n+        session.new_mail = false;\n \n         let uid_validity = get_uidvalidity(context, folder).await?;\n         let old_uid_next = get_uid_next(context, folder).await?;\n@@ -838,7 +839,7 @@ impl Session {\n         // Collect pairs of UID and Message-ID.\n         let mut msgs = BTreeMap::new();\n \n-        self.select_folder(context, folder).await?;\n+        self.select_with_uidvalidity(context, folder).await?;\n \n         let mut list = self\n             .uid_fetch(\"1:*\", RFC724MID_UID)\n@@ -1039,7 +1040,7 @@ impl Session {\n             // MOVE/DELETE operations. This does not result in multiple SELECT commands\n             // being sent because `select_folder()` does nothing if the folder is already\n             // selected.\n-            self.select_folder(context, folder).await?;\n+            self.select_with_uidvalidity(context, folder).await?;\n \n             // Empty target folder name means messages should be deleted.\n             if target.is_empty() {\n@@ -1087,7 +1088,7 @@ impl Session {\n             .await?;\n \n         for (folder, rowid_set, uid_set) in UidGrouper::from(rows) {\n-            self.select_folder(context, &folder)\n+            self.select_with_uidvalidity(context, &folder)\n                 .await\n                 .context(\"failed to select folder\")?;\n \n@@ -1131,7 +1132,7 @@ impl Session {\n             return Ok(());\n         }\n \n-        self.select_folder(context, folder)\n+        self.select_with_uidvalidity(context, folder)\n             .await\n             .context(\"failed to select folder\")?;\n \n\n@@ -29,9 +29,13 @@ impl Session {\n     ) -> Result<Self> {\n         use futures::future::FutureExt;\n \n-        self.select_folder(context, folder).await?;\n+        self.select_with_uidvalidity(context, folder).await?;\n \n         if self.server_sent_unsolicited_exists(context)? {\n+            self.new_mail = true;\n+        }\n+\n+        if self.new_mail {\n             return Ok(self);\n         }\n \n@@ -92,6 +96,9 @@ impl Session {\n         session.as_mut().set_read_timeout(Some(IMAP_TIMEOUT));\n         self.inner = session;\n \n+        // Fetch mail once we exit IDLE.\n+        self.new_mail = true;\n+\n         Ok(self)\n     }\n }\n\n@@ -10,12 +10,6 @@ type Result<T> = std::result::Result<T, Error>;\n \n #[derive(Debug, thiserror::Error)]\n pub enum Error {\n-    #[error(\"IMAP Connection Lost or no connection established\")]\n-    ConnectionLost,\n-\n-    #[error(\"IMAP Folder name invalid: {0}\")]\n-    BadFolderName(String),\n-\n     #[error(\"Got a NO response when trying to select {0}, usually this means that it doesn't exist: {1}\")]\n     NoFolder(String, String),\n \n@@ -33,7 +27,8 @@ impl ImapSession {\n     /// Issues a CLOSE command if selected folder needs expunge,\n     /// i.e. if Delta Chat marked a message there as deleted previously.\n     ///\n-    /// CLOSE is considerably faster than an EXPUNGE, see\n+    /// CLOSE is considerably faster than an EXPUNGE\n+    /// because no EXPUNGE responses are sent, see\n     /// <https://tools.ietf.org/html/rfc3501#section-6.4.2>\n     pub(super) async fn maybe_close_folder(&mut self, context: &Context) -> anyhow::Result<()> {\n         if let Some(folder) = &self.selected_folder {\n@@ -44,6 +39,7 @@ impl ImapSession {\n                 info!(context, \"close/expunge succeeded\");\n                 self.selected_folder = None;\n                 self.selected_folder_needs_expunge = false;\n+                self.new_mail = false;\n             }\n         }\n         Ok(())\n@@ -52,11 +48,7 @@ impl ImapSession {\n     /// Selects a folder, possibly updating uid_validity and, if needed,\n     /// expunging the folder to remove delete-marked messages.\n     /// Returns whether a new folder was selected.\n-    pub(crate) async fn select_folder(\n-        &mut self,\n-        context: &Context,\n-        folder: &str,\n-    ) -> Result<NewlySelected> {\n+    async fn select_folder(&mut self, context: &Context, folder: &str) -> Result<NewlySelected> {\n         // if there is a new folder and the new folder is equal to the selected one, there's nothing to do.\n         // if there is _no_ new folder, we continue as we might want to expunge below.\n         if let Some(selected_folder) = &self.selected_folder {\n@@ -85,10 +77,6 @@ impl ImapSession {\n                 self.selected_mailbox = Some(mailbox);\n                 Ok(NewlySelected::Yes)\n             }\n-            Err(async_imap::error::Error::ConnectionLost) => Err(Error::ConnectionLost),\n-            Err(async_imap::error::Error::Validate(_)) => {\n-                Err(Error::BadFolderName(folder.to_string()))\n-            }\n             Err(async_imap::error::Error::No(response)) => {\n                 Err(Error::NoFolder(folder.to_string(), response))\n             }\n@@ -128,13 +116,14 @@ impl ImapSession {\n     /// When selecting a folder for the first time, sets the uid_next to the current\n     /// mailbox.uid_next so that no old emails are fetched.\n     ///\n-    /// Returns Result<new_emails> (i.e. whether new emails arrived),\n-    /// if in doubt, returns new_emails=true so emails are fetched.\n+    /// Updates `self.new_mail` if folder was previously unselected\n+    /// and new mails are detected after selecting,\n+    /// i.e. UIDNEXT advanced while the folder was closed.\n     pub(crate) async fn select_with_uidvalidity(\n         &mut self,\n         context: &Context,\n         folder: &str,\n-    ) -> Result<bool> {\n+    ) -> Result<()> {\n         let newly_selected = self\n             .select_or_create_folder(context, folder)\n             .await\n@@ -191,28 +180,26 @@ impl ImapSession {\n         mailbox.uid_next = new_uid_next;\n \n         if new_uid_validity == old_uid_validity {\n-            let new_emails = if newly_selected == NewlySelected::No {\n-                // The folder was not newly selected i.e. no SELECT command was run. This means that mailbox.uid_next\n-                // was not updated and may contain an incorrect value. So, just return true so that\n-                // the caller tries to fetch new messages (we could of course run a SELECT command now, but trying to fetch\n-                // new messages is only one command, just as a SELECT command)\n-                true\n-            } else if let Some(new_uid_next) = new_uid_next {\n-                if new_uid_next < old_uid_next {\n-                    warn!(\n-                        context,\n-                        \"The server illegally decreased the uid_next of folder {folder:?} from {old_uid_next} to {new_uid_next} without changing validity ({new_uid_validity}), resyncing UIDs...\",\n-                    );\n-                    set_uid_next(context, folder, new_uid_next).await?;\n-                    context.schedule_resync().await?;\n+            if newly_selected == NewlySelected::Yes {\n+                if let Some(new_uid_next) = new_uid_next {\n+                    if new_uid_next < old_uid_next {\n+                        warn!(\n+                            context,\n+                            \"The server illegally decreased the uid_next of folder {folder:?} from {old_uid_next} to {new_uid_next} without changing validity ({new_uid_validity}), resyncing UIDs...\",\n+                        );\n+                        set_uid_next(context, folder, new_uid_next).await?;\n+                        context.schedule_resync().await?;\n+                    }\n+\n+                    // If UIDNEXT changed, there are new emails.\n+                    self.new_mail |= new_uid_next != old_uid_next;\n+                } else {\n+                    warn!(context, \"Folder {folder} was just selected but we failed to determine UIDNEXT, assume that it has new mail.\");\n+                    self.new_mail = true;\n                 }\n-                new_uid_next != old_uid_next // If UIDNEXT changed, there are new emails\n-            } else {\n-                // We have no UIDNEXT and if in doubt, return true.\n-                true\n-            };\n+            }\n \n-            return Ok(new_emails);\n+            return Ok(());\n         }\n \n         // UIDVALIDITY is modified, reset highest seen MODSEQ.\n@@ -223,6 +210,7 @@ impl ImapSession {\n         let new_uid_next = new_uid_next.unwrap_or_default();\n         set_uid_next(context, folder, new_uid_next).await?;\n         set_uidvalidity(context, folder, new_uid_validity).await?;\n+        self.new_mail = true;\n \n         // Collect garbage entries in `imap` table.\n         context\n@@ -245,7 +233,7 @@ impl ImapSession {\n             old_uid_next,\n             old_uid_validity,\n         );\n-        Ok(false)\n+        Ok(())\n     }\n }\n \n\n@@ -40,6 +40,11 @@ pub(crate) struct Session {\n     pub selected_mailbox: Option<Mailbox>,\n \n     pub selected_folder_needs_expunge: bool,\n+\n+    /// True if currently selected folder has new messages.\n+    ///\n+    /// Should be false if no folder is currently selected.\n+    pub new_mail: bool,\n }\n \n impl Deref for Session {\n@@ -67,6 +72,7 @@ impl Session {\n             selected_folder: None,\n             selected_mailbox: None,\n             selected_folder_needs_expunge: false,\n+            new_mail: false,\n         }\n     }\n \n"}
{"id": 12, "ground_truth": "Async Wait", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n#### Problem\r\n#5659\r\n\r\n#### Proposed Solution\r\nDebug the race condition exposed by test_banking_stage_entryfication, fix it, then revert #5659\r\n\n\n### Code Patch:\n@@ -155,7 +155,7 @@ fn main() {\n             Blocktree::open(&ledger_path).expect(\"Expected to be able to open database ledger\"),\n         );\n         let (exit, poh_recorder, poh_service, signal_receiver) =\n-            create_test_recorder(&bank, &blocktree);\n+            create_test_recorder(&bank, &blocktree, None);\n         let cluster_info = ClusterInfo::new_with_invalid_keypair(Node::new_localhost().info);\n         let cluster_info = Arc::new(RwLock::new(cluster_info));\n         let _banking_stage = BankingStage::new(\n\n@@ -66,7 +66,7 @@ fn bench_consume_buffered(bencher: &mut Bencher) {\n             Blocktree::open(&ledger_path).expect(\"Expected to be able to open database ledger\"),\n         );\n         let (exit, poh_recorder, poh_service, _signal_receiver) =\n-            create_test_recorder(&bank, &blocktree);\n+            create_test_recorder(&bank, &blocktree, None);\n \n         let tx = test_tx();\n         let len = 4096;\n@@ -198,7 +198,7 @@ fn bench_banking(bencher: &mut Bencher, tx_type: TransactionType) {\n             Blocktree::open(&ledger_path).expect(\"Expected to be able to open database ledger\"),\n         );\n         let (exit, poh_recorder, poh_service, signal_receiver) =\n-            create_test_recorder(&bank, &blocktree);\n+            create_test_recorder(&bank, &blocktree, None);\n         let cluster_info = ClusterInfo::new_with_invalid_keypair(Node::new_localhost().info);\n         let cluster_info = Arc::new(RwLock::new(cluster_info));\n         let _banking_stage = BankingStage::new(\n\n@@ -928,14 +928,15 @@ impl Service for BankingStage {\n pub fn create_test_recorder(\n     bank: &Arc<Bank>,\n     blocktree: &Arc<Blocktree>,\n+    poh_config: Option<PohConfig>,\n ) -> (\n     Arc<AtomicBool>,\n     Arc<Mutex<PohRecorder>>,\n     PohService,\n     Receiver<WorkingBankEntry>,\n ) {\n     let exit = Arc::new(AtomicBool::new(false));\n-    let poh_config = Arc::new(PohConfig::default());\n+    let poh_config = Arc::new(poh_config.unwrap_or_default());\n     let (mut poh_recorder, entry_receiver) = PohRecorder::new(\n         bank.tick_height(),\n         bank.last_blockhash(),\n@@ -986,7 +987,7 @@ mod tests {\n                 Blocktree::open(&ledger_path).expect(\"Expected to be able to open database ledger\"),\n             );\n             let (exit, poh_recorder, poh_service, _entry_receiever) =\n-                create_test_recorder(&bank, &blocktree);\n+                create_test_recorder(&bank, &blocktree, None);\n             let cluster_info = ClusterInfo::new_with_invalid_keypair(Node::new_localhost().info);\n             let cluster_info = Arc::new(RwLock::new(cluster_info));\n             let banking_stage = BankingStage::new(\n@@ -1011,6 +1012,7 @@ mod tests {\n             mut genesis_block, ..\n         } = create_genesis_block(2);\n         genesis_block.ticks_per_slot = 4;\n+        let num_extra_ticks = 2;\n         let bank = Arc::new(Bank::new(&genesis_block));\n         let start_hash = bank.last_blockhash();\n         let (verified_sender, verified_receiver) = unbounded();\n@@ -1020,8 +1022,10 @@ mod tests {\n             let blocktree = Arc::new(\n                 Blocktree::open(&ledger_path).expect(\"Expected to be able to open database ledger\"),\n             );\n+            let mut poh_config = PohConfig::default();\n+            poh_config.target_tick_count = Some(bank.max_tick_height() + num_extra_ticks);\n             let (exit, poh_recorder, poh_service, entry_receiver) =\n-                create_test_recorder(&bank, &blocktree);\n+                create_test_recorder(&bank, &blocktree, Some(poh_config));\n             let cluster_info = ClusterInfo::new_with_invalid_keypair(Node::new_localhost().info);\n             let cluster_info = Arc::new(RwLock::new(cluster_info));\n             let banking_stage = BankingStage::new(\n@@ -1031,7 +1035,6 @@ mod tests {\n                 vote_receiver,\n             );\n             trace!(\"sending bank\");\n-            sleep(Duration::from_millis(600));\n             drop(verified_sender);\n             drop(vote_sender);\n             exit.store(true, Ordering::Relaxed);\n@@ -1069,8 +1072,11 @@ mod tests {\n             let blocktree = Arc::new(\n                 Blocktree::open(&ledger_path).expect(\"Expected to be able to open database ledger\"),\n             );\n+            let mut poh_config = PohConfig::default();\n+            // limit tick count to avoid clearing working_bank at PohRecord then PohRecorderError(MaxHeightReached) at BankingStage\n+            poh_config.target_tick_count = Some(bank.max_tick_height() - 1);\n             let (exit, poh_recorder, poh_service, entry_receiver) =\n-                create_test_recorder(&bank, &blocktree);\n+                create_test_recorder(&bank, &blocktree, Some(poh_config));\n             let cluster_info = ClusterInfo::new_with_invalid_keypair(Node::new_localhost().info);\n             let cluster_info = Arc::new(RwLock::new(cluster_info));\n             let banking_stage = BankingStage::new(\n@@ -1120,6 +1126,9 @@ mod tests {\n \n             drop(verified_sender);\n             drop(vote_sender);\n+            // wait until banking_stage to finish up all packets\n+            banking_stage.join().unwrap();\n+\n             exit.store(true, Ordering::Relaxed);\n             poh_service.join().unwrap();\n             drop(poh_recorder);\n@@ -1128,18 +1137,20 @@ mod tests {\n             let bank = Bank::new(&genesis_block);\n             bank.process_transaction(&fund_tx).unwrap();\n             //receive entries + ticks\n-            for _ in 0..10 {\n+            loop {\n                 let entries: Vec<Entry> = entry_receiver\n                     .iter()\n                     .map(|(_bank, (entry, _tick_height))| entry)\n                     .collect();\n \n                 assert!(entries.verify(&blockhash));\n-                blockhash = entries.last().unwrap().hash;\n-                for entry in entries {\n-                    bank.process_transactions(&entry.transactions)\n-                        .iter()\n-                        .for_each(|x| assert_eq!(*x, Ok(())));\n+                if !entries.is_empty() {\n+                    blockhash = entries.last().unwrap().hash;\n+                    for entry in entries {\n+                        bank.process_transactions(&entry.transactions)\n+                            .iter()\n+                            .for_each(|x| assert_eq!(*x, Ok(())));\n+                    }\n                 }\n \n                 if bank.get_balance(&to) == 1 {\n@@ -1153,13 +1164,11 @@ mod tests {\n             assert_eq!(bank.get_balance(&to2), 0);\n \n             drop(entry_receiver);\n-            banking_stage.join().unwrap();\n         }\n         Blocktree::destroy(&ledger_path).unwrap();\n     }\n \n     #[test]\n-    #[ignore]\n     fn test_banking_stage_entryfication() {\n         solana_logger::setup();\n         // In this attack we'll demonstrate that a verifier can interpret the ledger\n@@ -1212,8 +1221,11 @@ mod tests {\n                     Blocktree::open(&ledger_path)\n                         .expect(\"Expected to be able to open database ledger\"),\n                 );\n+                let mut poh_config = PohConfig::default();\n+                // limit tick count to avoid clearing working_bank at PohRecord then PohRecorderError(MaxHeightReached) at BankingStage\n+                poh_config.target_tick_count = Some(bank.max_tick_height() - 1);\n                 let (exit, poh_recorder, poh_service, entry_receiver) =\n-                    create_test_recorder(&bank, &blocktree);\n+                    create_test_recorder(&bank, &blocktree, Some(poh_config));\n                 let cluster_info =\n                     ClusterInfo::new_with_invalid_keypair(Node::new_localhost().info);\n                 let cluster_info = Arc::new(RwLock::new(cluster_info));\n\n@@ -32,7 +32,15 @@ impl PohService {\n             .name(\"solana-poh-service-tick_producer\".to_string())\n             .spawn(move || {\n                 if poh_config.hashes_per_tick.is_none() {\n-                    Self::sleepy_tick_producer(poh_recorder, &poh_config, &poh_exit_);\n+                    if poh_config.target_tick_count.is_none() {\n+                        Self::sleepy_tick_producer(poh_recorder, &poh_config, &poh_exit_);\n+                    } else {\n+                        Self::short_lived_sleepy_tick_producer(\n+                            poh_recorder,\n+                            &poh_config,\n+                            &poh_exit_,\n+                        );\n+                    }\n                 } else {\n                     // PoH service runs in a tight loop, generating hashes as fast as possible.\n                     // Let's dedicate one of the CPU cores to this thread so that it can gain\n@@ -60,6 +68,22 @@ impl PohService {\n         }\n     }\n \n+    fn short_lived_sleepy_tick_producer(\n+        poh_recorder: Arc<Mutex<PohRecorder>>,\n+        poh_config: &PohConfig,\n+        poh_exit: &AtomicBool,\n+    ) {\n+        let mut warned = false;\n+        for _ in 0..poh_config.target_tick_count.unwrap() {\n+            sleep(poh_config.target_tick_duration);\n+            poh_recorder.lock().unwrap().tick();\n+            if poh_exit.load(Ordering::Relaxed) && !warned {\n+                warned = true;\n+                warn!(\"exit signal is ignored because PohService is scheduled to exit soon\");\n+            }\n+        }\n+    }\n+\n     fn tick_producer(poh_recorder: Arc<Mutex<PohRecorder>>, poh_exit: &AtomicBool) {\n         let poh = poh_recorder.lock().unwrap().poh.clone();\n         loop {\n@@ -108,6 +132,7 @@ mod tests {\n             let poh_config = Arc::new(PohConfig {\n                 hashes_per_tick: Some(2),\n                 target_tick_duration: Duration::from_millis(42),\n+                target_tick_count: None,\n             });\n             let (poh_recorder, entry_receiver) = PohRecorder::new(\n                 bank.tick_height(),\n\n@@ -257,7 +257,7 @@ pub mod tests {\n         let blocktree = Arc::new(blocktree);\n         let bank = bank_forks.working_bank();\n         let (exit, poh_recorder, poh_service, _entry_receiver) =\n-            create_test_recorder(&bank, &blocktree);\n+            create_test_recorder(&bank, &blocktree, None);\n         let voting_keypair = Keypair::new();\n         let storage_keypair = Arc::new(Keypair::new());\n         let leader_schedule_cache = Arc::new(LeaderScheduleCache::new_from_bank(&bank));\n\n@@ -1675,6 +1675,7 @@ mod tests {\n                         / DEFAULT_TICKS_PER_SLOT,\n                 ),\n                 hashes_per_tick: None,\n+                target_tick_count: None,\n             },\n \n             ..GenesisBlock::default()\n\n@@ -6,6 +6,9 @@ pub struct PohConfig {\n     /// The target tick rate of the cluster.\n     pub target_tick_duration: Duration,\n \n+    /// The target total tick count to be produced; used for testing only\n+    pub target_tick_count: Option<u64>,\n+\n     /// How many hashes to roll before emitting the next tick entry.\n     /// None enables \"Low power mode\", which implies:\n     /// * sleep for `target_tick_duration` instead of hashing\n@@ -18,6 +21,7 @@ impl PohConfig {\n         Self {\n             target_tick_duration,\n             hashes_per_tick: None,\n+            target_tick_count: None,\n         }\n     }\n }\n"}
{"id": 5, "ground_truth": "Concurrency", "predicted": "Concurrency", "match": true, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nThe fuzzer causes some ungraceful shutdown of the tokio tasks, which makes CI fail in a flaky way.\n\n### Code Patch:\n@@ -301,19 +301,18 @@ impl DocumentActor {\n                 self.maybe_process_crdt_file_deltas_in_ot(file_deltas).await;\n                 self.process_cursor_states(cursor_states).await;\n \n-                response_tx\n-                    .send(peer_state)\n-                    .expect(\"Failed to send peer state in response to ReceiveSyncMessage\");\n+                if response_tx.send(peer_state).is_err() {\n+                    warn!(\"Failed to send peer state in response to ReceiveSyncMessage.\");\n+                }\n             }\n             DocMessage::GenerateSyncMessage {\n                 state: mut peer_state,\n                 response_tx,\n             } => {\n                 let message = self.crdt_doc.generate_sync_message(&mut peer_state);\n-                let val = response_tx.send((peer_state, message));\n \n-                if let Err(val) = val {\n-                    warn!(\"Failed to send peer state and sync message in response to GenerateSyncMessage: {val:?}\");\n+                if response_tx.send((peer_state, message)).is_err() {\n+                    warn!(\"Failed to send peer state and sync message in response to GenerateSyncMessage.\");\n                 }\n             }\n             DocMessage::NewEditorConnection(editor_handle) => {\n@@ -489,9 +488,7 @@ impl DocumentActor {\n                 uri: format!(\"file://{}\", self.absolute_path_for_file_path(&file_path)),\n                 ranges,\n             };\n-            for handle in &mut self.editor_clients.values_mut() {\n-                handle.send(message.clone()).await;\n-            }\n+            self.send_to_editor_clients(message).await;\n         }\n     }\n \n@@ -500,9 +497,22 @@ impl DocumentActor {\n             uri: format!(\"file://{}\", self.absolute_path_for_file_path(file_path)),\n             delta: rev_delta,\n         };\n+        self.send_to_editor_clients(message).await;\n+    }\n \n-        for handle in &mut self.editor_clients.values_mut() {\n-            handle.send(message.clone()).await;\n+    async fn send_to_editor_clients(&mut self, message: EditorProtocolMessageToEditor) {\n+        let mut to_remove = Vec::new();\n+        for (id, handle) in &mut self.editor_clients.iter_mut() {\n+            if handle.send(message.clone()).await.is_err() {\n+                // Remove this client.\n+                to_remove.push(*id);\n+            }\n+        }\n+        for id in to_remove {\n+            // The destructor of EditorHandle will shut down the actors when\n+            // we remove it from the HashMap.\n+            info!(\"Removing EditorHandle from client list.\");\n+            self.editor_clients.remove(&id);\n         }\n     }\n \n@@ -585,7 +595,7 @@ impl DocumentActor {\n         while let Some(message) = self.doc_message_rx.recv().await {\n             self.handle_message(message).await;\n         }\n-        panic!(\"Channel towards document task has been closed\");\n+        debug!(\"Channel towards document handle has been closed (probably shutting down)\");\n     }\n }\n \n\n@@ -1,4 +1,5 @@\n /// This module is all about daemon to editor communication.\n+use std::io;\n use tokio::{\n     io::{AsyncBufReadExt, AsyncWriteExt, BufReader, ReadHalf, WriteHalf},\n     net::UnixStream,\n@@ -22,6 +23,7 @@ pub async fn spawn_editor_connection(stream: UnixStream, document_handle: Docume\n \n pub struct EditorHandle {\n     editor_message_tx: EditorMessageSender,\n+    shutdown_token: CancellationToken,\n }\n \n impl EditorHandle {\n@@ -35,18 +37,32 @@ impl EditorHandle {\n         let mut reader = SocketReadActor::new(stream_read, shutdown_token.clone(), document_handle);\n         tokio::spawn(async move { reader.run().await });\n \n-        let mut writer = SocketWriteActor::new(stream_write, socket_message_rx, shutdown_token);\n+        let mut writer =\n+            SocketWriteActor::new(stream_write, socket_message_rx, shutdown_token.clone());\n         tokio::spawn(async move { writer.run().await });\n         Self {\n             editor_message_tx: socket_message_tx,\n+            shutdown_token,\n         }\n     }\n \n-    pub async fn send(&self, message: EditorProtocolMessageToEditor) {\n-        self.editor_message_tx\n-            .send(message)\n-            .await\n-            .expect(\"Failed to send to editor.\");\n+    pub async fn send(&self, message: EditorProtocolMessageToEditor) -> Result<(), io::Error> {\n+        // Can fail during shutdown or editor disconnect, when Actors already have been killed/closed\n+        if self.editor_message_tx.send(message).await.is_err() {\n+            Err(io::Error::new(\n+                io::ErrorKind::BrokenPipe,\n+                \"Can't keep up or dead\",\n+            ))\n+        } else {\n+            Ok(())\n+        }\n+    }\n+}\n+\n+impl Drop for EditorHandle {\n+    fn drop(&mut self) {\n+        debug!(\"Editor Handle dropped, shutting down socket actors\");\n+        self.shutdown_token.cancel();\n     }\n }\n \n@@ -134,7 +150,7 @@ impl SocketWriteActor {\n         loop {\n             tokio::select! {\n                 () = self.shutdown_token.cancelled() => {\n-                    debug!(\"Shutting down JSON-RPC sender (due to socket disconnet)\");\n+                    debug!(\"Shutting down JSON-RPC sender (due to socket disconnect)\");\n                     break;\n                 }\n                 editor_message_maybe = self.editor_message_receiver.recv() => match editor_message_maybe {\n"}
{"id": 67, "ground_truth": "Network", "predicted": "Concurrency", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\n### Describe the bug\r\n\r\n`bodies::bodies::tests::streams_bodies_in_order_after_range_reset` occasionally fails on main. A failure can be seen in CI here:\r\nhttps://github.com/paradigmxyz/reth/actions/runs/4077743007/jobs/7027133041#step:8:203\r\n\r\n### Steps to reproduce\r\n\r\nSometimes this can be reproduced locally with `cargo nextest run --workspace --locked --all-features`\r\n\r\n### Node logs\r\n\r\n```text\r\n--- STDERR:              reth-downloaders bodies::bodies::tests::streams_bodies_in_order_after_range_reset ---\r\nthread 'bodies::bodies::tests::streams_bodies_in_order_after_range_reset' panicked at 'assertion failed: `(left == right)`\r\n  left: `8`,\r\n right: `7`', crates/net/downloaders/src/bodies/bodies.rs:616:9\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n\r\n   Canceling due to test failure: 0 tests still running\r\n```\r\n\r\n\r\n### Platform(s)\r\n\r\n_No response_\r\n\r\n### What version/commit are you on?\r\n\r\n66aa947\r\n\r\n### Code of Conduct\r\n\r\n- [X] I agree to follow the Code of Conduct\n\n### Code Patch:\n@@ -550,8 +550,10 @@ mod tests {\n     use assert_matches::assert_matches;\n     use futures_util::stream::StreamExt;\n     use reth_db::mdbx::{test_utils::create_test_db, EnvKind, WriteMap};\n-    use reth_interfaces::test_utils::TestConsensus;\n-    use std::sync::Arc;\n+    use reth_eth_wire::BlockBody;\n+    use reth_interfaces::test_utils::{generators::random_block_range, TestConsensus};\n+    use reth_primitives::H256;\n+    use std::{collections::HashMap, sync::Arc};\n \n     // Check that the blocks are emitted in order of block number, not in order of\n     // first-downloaded\n@@ -580,6 +582,41 @@ mod tests {\n         assert_eq!(client.times_requested(), 1);\n     }\n \n+    // Check that the number of times requested equals to the number of headers divided by request\n+    // limit.\n+    #[tokio::test]\n+    async fn requests_correct_number_of_times() {\n+        // Generate some random blocks\n+        let db = create_test_db::<WriteMap>(EnvKind::RW);\n+        let blocks = random_block_range(0..200, H256::zero(), 1..2);\n+\n+        let headers = blocks.iter().map(|block| block.header.clone()).collect::<Vec<_>>();\n+        let bodies = blocks\n+            .into_iter()\n+            .map(|block| {\n+                (\n+                    block.hash(),\n+                    BlockBody {\n+                        transactions: block.body,\n+                        ommers: block.ommers.into_iter().map(|header| header.unseal()).collect(),\n+                    },\n+                )\n+            })\n+            .collect::<HashMap<_, _>>();\n+\n+        insert_headers(&db, &headers);\n+\n+        let request_limit = 10;\n+        let client = Arc::new(TestBodiesClient::default().with_bodies(bodies.clone()));\n+        let mut downloader = BodiesDownloaderBuilder::default()\n+            .with_request_limit(request_limit)\n+            .build(client.clone(), Arc::new(TestConsensus::default()), db);\n+        downloader.set_download_range(0..200).expect(\"failed to set download range\");\n+\n+        let _ = downloader.collect::<Vec<_>>().await;\n+        assert_eq!(client.times_requested(), 20);\n+    }\n+\n     // Check that bodies are returned in correct order\n     // after resetting the download range multiple times.\n     #[tokio::test]\n@@ -612,12 +649,6 @@ mod tests {\n             assert!(downloader.latest_queued_block_number >= Some(range_start));\n             range_start += stream_batch_size as u64;\n         }\n-\n-        assert_eq!(\n-            client.times_requested(),\n-            // div_ceil equivalent\n-            ((headers.iter().filter(|x| !x.is_empty()).count() + 9) / 10) as u64,\n-        );\n     }\n \n     // Check that the downloader picks up the new range and downloads bodies after previous range\n"}
{"id": 62, "ground_truth": "Concurrency", "predicted": "Async Wait", "match": false, "prompt": "You are an expert in Rust flaky tests. Given the issue description and code patch below, classify the root cause of flaky tests into one of the following categories:\n\nLogic, Randomness, Network, Async Wait, Concurrency, Time, I/O, Unordered data, Environment\n\nRespond only with the exact category name.\n\n### Issue Description:\nDetected by https://github.com/paritytech/subxt/actions/runs/6771722241/job/18402732995?pr=1238.\r\n\r\n\r\n```\r\n--- STDOUT:              integration-tests full_client::frame::balances::tx_basic_transfer ---\r\nfailures:\r\n    full_client::frame::balances::tx_basic_transfer\r\n\r\ntest result: FAILED. 0 passed; 1 failed; 0 ignored; 0 measured; 77 filtered out; finished in 50.34s\r\n\r\n\r\n--- STDERR:              integration-tests full_client::frame::balances::tx_basic_transfer ---\r\nthread 'full_client::frame::balances::tx_basic_transfer' panicked at testing/integration-tests/src/full_client/frame/balances.rs:83:5:\r\nassertion failed: alice_pre.data.free - 10_000 >= alice_post.data.free\r\nnote: run with `RUST_BACKTRACE=1` environment variable to display a backtrace\r\n```\n\n### Code Patch:\n@@ -10,6 +10,7 @@ use crate::utils::strip_compact_prefix;\n use crate::{\n     backend::{BlockRef, StreamOfResults, TransactionStatus as BackendTxStatus},\n     client::OnlineClientT,\n+    config::Header,\n     error::{DispatchError, Error, RpcError, TransactionError},\n     events::EventsClient,\n     Config,\n@@ -82,6 +83,20 @@ where\n     /// that this is true. In those cases the stream is closed however, so you currently have no way to find\n     /// out if they finally made it into a block or not.\n     pub async fn wait_for_finalized(mut self) -> Result<TxInBlock<T, C>, Error> {\n+        // Wait for the tx to be reported in a block by the transaction status.\n+        let tx_in_block = self.wait_for_tx_finalized().await?;\n+\n+        // Tx reported as finalized must be synchronized with the chain head:\n+        // - tx finalized number must be lower than the chain head finalized number.\n+        // - tx finalized hash must be equal to the chain head finalized hash.\n+        self.wait_for_chain_head_finalized(tx_in_block).await\n+    }\n+\n+    /// Wait for the transaction to be reported as finalized.\n+    ///\n+    /// This relies entirely by the transaction status, that could be out of sync with\n+    /// the chain head.\n+    async fn wait_for_tx_finalized(&mut self) -> Result<TxInBlock<T, C>, Error> {\n         while let Some(status) = self.next().await {\n             match status? {\n                 // Finalized! Return.\n@@ -101,6 +116,80 @@ where\n         Err(RpcError::SubscriptionDropped.into())\n     }\n \n+    /// Wait for the block hash of the transaction to be reported as finalized\n+    /// by the chain head.\n+    async fn wait_for_chain_head_finalized(\n+        &mut self,\n+        tx_in_block: TxInBlock<T, C>,\n+    ) -> Result<TxInBlock<T, C>, Error> {\n+        // Correlate the tx status with the head of the chain.\n+        let head = self.client.backend().latest_finalized_block_ref().await?;\n+\n+        // This optimizes the most common case, where the tx status is in sync with the head of the chain.\n+        if tx_in_block.block_hash() == head.hash() {\n+            return Ok(tx_in_block);\n+        }\n+\n+        // Must determine if the hash is older or newer than the chain head.\n+        let Some(tx_num) = self\n+            .client\n+            .backend()\n+            .block_header(tx_in_block.block_hash())\n+            .await?\n+            .map(|header: <T as Config>::Header| header.number())\n+        else {\n+            return Err(TransactionError::Error(\n+                \"Transaction block header not found. This is a bug, please open an issue\".into(),\n+            )\n+            .into());\n+        };\n+\n+        let Some(head_num) = self\n+            .client\n+            .backend()\n+            .block_header(head.hash())\n+            .await?\n+            .map(|header| header.number())\n+        else {\n+            return Err(TransactionError::Error(\n+                \"Chain finalized block header not found. This is a bug, please open an issue\"\n+                    .into(),\n+            )\n+            .into());\n+        };\n+\n+        let tx_num: u64 = tx_num.into();\n+        let head_num: u64 = head_num.into();\n+        // Rely on substrate to report a valid TX status included in a prior finalized block.\n+        if tx_num < head_num {\n+            return Ok(tx_in_block);\n+        }\n+\n+        // Wait for the chain head to report the tx block hash as finalized, or to report\n+        // a newer finalized block hash (in terms of block number).\n+        let mut finalized_stream = self\n+            .client\n+            .backend()\n+            .stream_finalized_block_headers()\n+            .await?;\n+\n+        while let Some(finalized) = finalized_stream.next().await {\n+            let (finalized, _block_ref) = finalized?;\n+\n+            // By the time we open this subscription stream, tx finalized number may (very unlikely) lag behind.\n+            let finalized_num = finalized.number().into();\n+            if tx_num < finalized_num {\n+                return Ok(tx_in_block);\n+            }\n+\n+            if finalized.hash() == tx_in_block.block_hash() {\n+                return Ok(tx_in_block);\n+            }\n+        }\n+\n+        Ok(tx_in_block)\n+    }\n+\n     /// Wait for the transaction to be finalized, and for the transaction events to indicate\n     /// that the transaction was successful. Returns the events associated with the transaction,\n     /// as well as a couple of other details (block hash and extrinsic hash).\n"}
